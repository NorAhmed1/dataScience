{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formalism of linear regression\n",
    "\n",
    "## SGD derived from cost function\n",
    "The hypothesis function for linear regression is \n",
    "$$h_{\\theta}(x) = \\sum_{i=0}^n \\theta_ix_i = \\theta^Tx,$$  \n",
    "where $x,\\theta \\in \\mathbb{R}^{n+1}$. \n",
    "The cost function (not loss function) is \n",
    "$$ J(\\theta) = \\frac{1}{2}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})^2$$\n",
    "Note the conventional notation of $J(\\theta), L(\\theta), l(\\theta)$, which describe cost, likelihood, log-likelihood functions respectively.  \n",
    "Minimizing the cost function (note the minus sign before $\\alpha$ for minimization) gives the gradient descent update rule for linear regression\n",
    "$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\theta_j + \\alpha (y^{(i)}-h_{\\theta}(x^{(i)})x_j^{(i)} $$\n",
    "\n",
    "## SGD derived from Gaussian probabilistic model  with maximum likelihood estimation (MLE)\n",
    "Assuming the probability of $y^{(i)}$ given $x^{(i)}$ is Gaussian and then writing the likelihood as \n",
    "\n",
    "$$L(\\theta) = \\prod_{i=1}^m p(y^{(i)}\\mid x^{(i)};\\theta),$$\n",
    "\n",
    "we find that maximizing the log likelihood \n",
    "$$l(\\theta) = \\mathrm{log}L(\\theta)$$ is equivalent to minimizing the same cost function introduced earlier. Therefore, we obtain the same updating rule for SGD. \n",
    "\n",
    "\n",
    "## Closed-form solutions \n",
    "See notes elsewhere about linear regression, and what insights can be derived from closed form solutions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formalism of logistic regression\n",
    "\n",
    "## SGD derived from Bernoulli probabilistic model with MLE\n",
    "The hypothesis function for logistic regression is sigmoid function. \n",
    "$$h_{\\theta}(x) = \\frac{1}{1+e^{-\\theta^Tx}}$$  \n",
    "For such a function, we cannot simply define a cost function as in linear regression and then minimize it to find the solution. The reason is that such a cost function is not convex and thus will introduce complexity in solving the problem. Thus a probabilistic model Bernoulli distribution (not like the Gaussian in linear regression) will be used to derive the SGD update rule by maximizing log likelihood. \n",
    "$$ p(y\\mid x;\\theta) = (h_{\\theta}(x))^y(1-h_{\\theta}(x))^{1-y}$$\n",
    "Further assuming there are $m$ independently generated samples, then the likelihood can be written as \n",
    "$$L(\\theta) = \\prod_{i=1}^m p(y^{(i)}\\mid x^{(i)};\\theta) = \\prod_{i=1}^m (h_{\\theta}(x^{(i)}))^{y^{(i)}}(1-h_{\\theta}(x^{(i)}))^{1-y^{(i)}}$$\n",
    "It is easier to maximize the log likelihood\n",
    "$$l(\\theta) = \\mathrm{log}L(\\theta) = \\sum_{i=1}^m y^{(i)}\\mathrm{log}h_{\\theta}(x^{(i)}) + (1-y^{(i)})\\mathrm{log}(1-h_{\\theta}(x^{(i)}))$$\n",
    "In many cases we can obtain the closed form of parameter $\\theta$ by maximizing the likelihood. We first calculate the gradient of likelihood w.r.t. the $\\theta$ and set it to be zero, and then solve the equation to obtain the solution to the parameter. We can do this in linear regression, mixture of Gaussian, mixture of Naive Bayes, etc. In the general expectation maximization (EM) approach, although it is impossible to obtain the general closed form solution, it is possible to obtain the closed form of parameter for each iterative step.  \n",
    "In the above logistic regression case, it is impossible to obtain an analytic solution. The parameter will still be calculated by maximizing the likelihood but with an iterative parameter updating process. Note the plus sign before $\\alpha$ for gradient ascent. We employ an gradient ascent rule to maximize the likelihood, i.e.,\n",
    "$$\\theta_j := \\theta_j + \\alpha \\frac{\\partial l(\\theta)}{\\partial \\theta_j} = \\theta_j + \\alpha (y^{(i)}-h_{\\theta}(x^{(i)})x_j^{(i)} $$\n",
    "\n",
    "## SGD derived from cost function\n",
    "As mentioned earlier, the form of squared difference cost function for linear regression is not suitable for logistic regression. However, one can define a cost function for logistic function with the concept of cross entropy. https://en.wikipedia.org/wiki/Cross_entropy. It turns out such as cost function has the form\n",
    "$$ J(\\theta) = -\\frac{1}{m}l(\\theta)$$\n",
    "Therefore, minimizing this cost function is equivalent to maximizing the log likelihood $l(\\theta)$ introduced before (due to the minus sign). We can thus obtain the same SGD updating rule. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.ndfromtxt('images.csv', delimiter=',')\n",
    "y = np.ndfromtxt(\"labels.csv\", delimiter=',', dtype=np.int8)\n",
    "img_size = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784) (10000,)\n",
      "(10000, 1) (10000,)\n",
      "(10000, 1) (10000, 1)\n",
      "True\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20196303240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Use another way wo read in the data\n",
    "XX = pd.read_csv(\"images.csv\",header = None).values\n",
    "yy = pd.read_csv(\"labels.csv\",header = None, dtype= np.int8).values\n",
    "print(X.shape,y.shape)\n",
    "print(yy.shape, y.shape)\n",
    "\n",
    "y=y.reshape(10000,1) #Original its dim is (10000,) and thus cannot compare with array yy.\n",
    "print(yy.shape, y.shape)\n",
    "print(np.array_equal(yy,y)) #Better than print(yy == y) which print out an boolean array.\n",
    "\n",
    "#After comparison, reshape it back for later use. \n",
    "y = y.reshape(10000) #y is reshaped to (10000,1) for comparison purpose earlier.\n",
    "\n",
    "#Using the reshape to flip between 1D and 2D images. It is better than other functions. \n",
    "X = df.values\n",
    "plt.imshow(X[300].reshape(28,28), cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2115, 784) (2115,)\n"
     ]
    }
   ],
   "source": [
    "# To filter out two classes for binary logistic regression\n",
    "# If the the dimension of y:(10000,) becomes (10000,1), then the following code will  have problem.\n",
    "ind = np.logical_or(y == 1, y == 0)\n",
    "ind.shape\n",
    "X = X[ind, :]\n",
    "y = y[ind]\n",
    "print(X.shape,y.shape)\n",
    "\n",
    "num_train = int(len(y) * 0.8)\n",
    "X_train = X[0:num_train, :] #This will not include the X[num_train,:]\n",
    "X_test = X[num_train:-1,:]  #This will not include the final row index. \n",
    "y_train = y[0:num_train]\n",
    "y_test = y[num_train:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n",
      "313 µs ± 10.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "3.93 µs ± 149 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def h1(theta, x):\n",
    "    sum = 0.0\n",
    "    for i in range(len(x)):\n",
    "        sum -= theta[i] * x[i]\n",
    "    return 1 / (1 + math.exp(sum))\n",
    "\n",
    "def h2(theta, x):\n",
    "    return 1 / (1 + np.exp(np.dot(theta, x)))\n",
    "\n",
    "theta = np.zeros([img_size])\n",
    "x = X[0,:]\n",
    "print(x.shape)\n",
    "%timeit h1(theta, x)\n",
    "%timeit h2(theta, x)\n",
    "#The vectorized version is much faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed: 0.0 seconds\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (784,) and (422,784) not aligned: 784 (dim 0) != 422 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-e5e2f841db25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"time elapsed: {0} seconds\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"percentage correct: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-83-e5e2f841db25>\u001b[0m in \u001b[0;36mh\u001b[1;34m(theta, x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# doing everything element-wise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mGD_elementwise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (784,) and (422,784) not aligned: 784 (dim 0) != 422 (dim 0)"
     ]
    }
   ],
   "source": [
    "# doing everything element-wise\n",
    "def h(theta, x):\n",
    "    return 1 / (1 + np.exp(-np.dot(theta, x)))\n",
    "\n",
    "def GD_elementwise(theta, X_train, y_train, alpha):\n",
    "    diff_arr = np.zeros([len(y_train)])\n",
    "    for m in range(len(y_train)):\n",
    "            diff_arr[m] = h(theta, X_train[m, :]) - y_train[m]\n",
    "    for j in range(len(theta)):\n",
    "        s = 0.0\n",
    "        for m in range(len(y_train)):\n",
    "            s += diff_arr[m] * X_train[m, j]\n",
    "        theta[j] = theta[j] - alpha * s\n",
    "        \n",
    "def train_elementwise(X_train, y_train, max_iter, alpha):\n",
    "    theta = np.zeros([img_size]) #In neural network, we cannot set parameters to zero. \n",
    "    for i in range(max_iter):\n",
    "        GD_elementwise(theta, X_train, y_train, alpha)       \n",
    "    return theta\n",
    "    \n",
    "max_iter = 10\n",
    "alpha = 0.01\n",
    "start = time.time()\n",
    "theta = train_elementwise(X_train, y_train, max_iter, alpha)\n",
    "end = time.time()\n",
    "print(\"time elapsed: {0} seconds\".format(end - start))\n",
    "pred = (np.sign(h(theta, X_test) - 0.5) + 1) / 2\n",
    "print(\"percentage correct: {0}\".format(np.sum(pred == y_test) / len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ljyan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed: 0.2528204917907715 seconds\n",
      "percentage correct: 1.0\n",
      "422\n"
     ]
    }
   ],
   "source": [
    "#some vectorization\n",
    "def h_vec(theta, X):\n",
    "    return 1 / (1 + np.exp(-np.matmul(X, theta)))\n",
    "\n",
    "def GD_better(theta,  X_train, y_train, alpha):\n",
    "    diff_arr = h_vec(theta, X_train) - y_train\n",
    "    for j in range(len(theta)):\n",
    "        theta[j] = theta[j] - alpha * np.dot(diff_arr, X_train[:, j])\n",
    "        \n",
    "def train_better(X_train, y_train, max_iter, alpha):\n",
    "    theta = np.zeros([img_size])\n",
    "    for i in range(max_iter):\n",
    "        GD_better(theta, X_train, y_train, alpha)       \n",
    "    return theta\n",
    "    \n",
    "max_iter = 10\n",
    "alpha = 0.01\n",
    "start = time.time()\n",
    "theta = train_better(X_train, y_train, max_iter, alpha)\n",
    "end = time.time()\n",
    "print(\"time elapsed: {0} seconds\".format(end - start))\n",
    "pred = (np.sign(h_vec(theta, X_test) - 0.5) + 1) / 2\n",
    "print(\"percentage correct: {0}\".format(np.sum(pred == y_test) / len(y_test)))\n",
    "print(sum(pred==y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elapsed: 0.015630245208740234 seconds\n",
      "percentage correct: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ljyan\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#fully vectorized\n",
    "def GD (theta, X_train, y_train, alpha):\n",
    "    theta -= alpha * np.squeeze(np.matmul(np.reshape(h_vec(theta, X_train) - y_train, [1, -1]), X_train))\n",
    "    \n",
    "def train_vec(X_train, y_train, max_iter, alpha):\n",
    "    theta = np.zeros([img_size])\n",
    "    for i in range(max_iter):\n",
    "        GD(theta, X_train, y_train, alpha)       \n",
    "    return theta\n",
    "\n",
    "max_iter = 10\n",
    "alpha = 0.01\n",
    "start = time.time()\n",
    "theta = train_vec(X_train, y_train, max_iter, alpha)\n",
    "end = time.time()\n",
    "print(\"time elapsed: {0} seconds\".format(end - start))\n",
    "pred = (np.sign(h_vec(theta, X_test) - 0.5) + 1) / 2\n",
    "print(\"percentage correct: {0}\".format(np.sum(pred == y_test) / len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify the code for linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
