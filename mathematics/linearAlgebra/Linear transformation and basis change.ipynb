{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "* Linear algebra is the art of choosing right basis. For example, in eigenbasis, matrix corresponding to linear transformation can become diagonalized and this will significantly simplify the related problems. \n",
    "\n",
    "* Linear transformation preserves orthogonality. Orthogonal (unitary transformation ) preserves norm. \n",
    "\n",
    "https://en.wikipedia.org/wiki/Change_of_basis  \n",
    "https://www.khanacademy.org/math/linear-algebra/alternate-bases/change-of-basis/v/lin-alg-transformation-matrix-with-respect-to-a-basis  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector in different bases\n",
    "\n",
    "* One vector $x$ are expanded in two bases (denoted by $S$ and $N$) as $Sx_s$ and $Nx_n$ respectively. This can be understood with the column picture of matrix multiplication introduced earlier. That is, $x_s$ and $x_n$ are the expansion coefficients of the vector $x$ in the two bases.  \n",
    "\n",
    "* Because we are representing the same vector in two bases, we have $Sx_s = Nx_n$. If we further assume that bases $S,N$ are invertible (this is not the general case where $S,N$ can be singular or even non square matrix), then we have $x_n = N^{-1}Sx_s$ and $x_s = S^{-1}Nx_n$. \n",
    "\n",
    "* If $S$ denotes standard basis, then $S$ is just the identity matrix $I$. The above two equations become $x_n = N^{-1}x_s$ and $x_s = Nx_n$ respectively. Here, we see that when changing from standard basis to a new or non-standard basis $N$, we will have a change of basis matrix $N$ and $x_n = N^{-1}x_s$. If $S$ is not standard basis, then the change of basis matrix is $S^{-1}N$. Only when we change from standard basis to new basis $N$, then the change of basis matrix is $N$ only. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear transformation matrix in different bases\n",
    "\n",
    "* Linear transformation $x \\rightarrow T(x)$ (1) can be regarded as a special (linear) mapping, or a special function. (2) Can be imagined as the vector $x$ is linearly transformed (e.g. rotated, reflected,etc.) into another vector $T(x)$ in the same vector space. \n",
    "\n",
    "* Matrix representation of linear transformation. Assuming the linear transformation $T: \\mathbb{R^n} \\rightarrow \\mathbb{R^n}$. In other words, the input basis where $x$ located and the output basis where $T(x)$ located are same. In this case, $x \\rightarrow T(x)$ can be represented with a relation $Ax$, where $A = \\{T(e_1), T(e_2),...T(e_n)\\}$. Check how matrix $A$ is calculated when the input and output basis are different in linear algebra text books. \n",
    "\n",
    "* Relation of linear transformation matrices in different bases. This can be obtained with almost the EXACT way used in deriving the vector in different basis. (1) In the vector case, the save vector expanded in different bases are same and thus we have $Sx_s = Nx_n$. (2). Here, after linear transformation, we have $x_s \\rightarrow T(x_s) = A_sx_s $ and $x_n \\rightarrow T(x_n) = A_nx_n$. Because we perform the same linear transformation $T$ in different basis $S, N$, $A_sx_s$ and $A_nx_n$ are the coordinates of the same transformed vectors in different bases. Therefore, we have $SA_sx_s = NA_nx_n$.  \n",
    "\n",
    "* Further assuming $S$ and $N$ are invertible and using the relation between $x_s$ and $x_n$ obtained earlier, we then have $$NA_nx_n = SA_sx_s \\Rightarrow NA_nN^{-1}Sx_s = SA_sx_s \\Rightarrow NA_nN^{-1}S = SA_s \\Rightarrow A_s = S^{-1}NA_nN^{-1}S \\Rightarrow A_n = N^{-1}S A_s S^{-1}N$$ \n",
    "\n",
    "* If $S$ is the standard basis $I$, then the above relations become \n",
    "$$A_s = NA_nN^{-1} \\Rightarrow A_n = N^{-1} A_s N$$\n",
    "\n",
    "* For easy memory, we always write $x_n, A_n$ in non-standard basis in terms standard basis counterparts. In this case, $N^{-1}$ always comes first as in the following relations: $$x_n = N^{-1}x_s \\\\ A_n = N^{-1} A_s N$$\n",
    "\n",
    "* The relation of change of basis matrix $N$ can be regarded as the special case of matrix of linear transformation when the linear transformation is identity operation. This can be easily seen in Dirac notation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirac Notation and basis change \n",
    "$\\newcommand{\\braket}[2]{\\left\\langle{#1}\\middle|{#2}\\right\\rangle}$ \n",
    "$\\newcommand{\\ket}[1]{\\left|{#1}\\right\\rangle}$\n",
    "$\\newcommand{\\bra}[1]{\\left\\langle{#1}\\right|}$  \n",
    "\n",
    "Dirac notation can help easily write basis change for either vector or operator (linear transformation). \n",
    "\n",
    "### Vector representation in basis\n",
    "$$\n",
    "\\ket{\\psi} = \\sum_{s} \\ket{s}\\braket{s}{\\psi} = \\sum_{s} c_s\\ket{s}  \n",
    "$$\n",
    "\n",
    "### Basis change for a vector\n",
    "$$\n",
    "\\ket{\\psi} = \\sum_{n} c_n\\ket{n} = \\sum_{s} c_s\\ket{s} = \\sum_{sn}c_s\\ket{n}\\braket{n}{s} = \\sum_{n}\\left( \\sum_{s}c_s\\braket{n}{s}\\right)\\ket{n} \n",
    "$$\n",
    "Thus we have \n",
    "$$\n",
    "c_n = \\sum_{s}c_s\\braket{n}{s}\n",
    "$$\n",
    "\n",
    "* Note the $c_n, c_s$ are the $nth$ and $sth$ coordinates on two bases $N,S$ respectively. They are different from the $x_n,x_s$ introduced earlier, which are the vectors coordinates. In other words, $x_n = \\{c_n\\}$ and $x_s = \\{c_s\\}$\n",
    "\n",
    "* The inner product $\\braket{n}{s}$ can be taken as the $(n,s)$ matrix element for the matrix achieving the basis change. If we assume $n$ indicates a non-standard basis and $s$ indicates the standard basis, then $\\braket{s}{n}$ can be matrix element of a matrix whose columns are the vectors of basis described by $n$. However, in practical calculation, we can just calculate the matrix indexed by $(n,s)$.  \n",
    "\n",
    "* Normally the matrix indexed by $(s,n)$ is called the change of basis matrix. If both $s$ and $n$ are the indices of the same basis, then the change of basis matrix will be an identity matrix.  \n",
    "\n",
    "* The basis change relation $ c_n = \\sum_{s}c_s\\braket{n}{s} $ is equivalent to the relation $x_n = N^{-1}x_s$ obtained earlier if we consider the special case, where basis described by $n$ is an orthogonal basis ($N^T = N^{-1}$). In quantum mechanics, basis is usually from the eigenvectors of a Hermitian operator and thus basis $N$ is always orthogonal. Here are some details: \n",
    "\n",
    "$$c_n = \\sum_s c_s\\braket{n}{s} \\equiv \\sum_s A_{ns}c_s \\equiv \\sum_s B^{T}_{ns}c_s \\Rightarrow x_n = Ax_s \\quad \\text{and} \\quad x_n = B^Tx_s$$\n",
    "From the above equation, we have \n",
    "$$ B^T_{ns} = A_{ns} = \\braket{n}{s} \\Rightarrow B_{sn} = \\braket{n}{s} \\Rightarrow B_{ns} = \\braket{s}{n}$$\n",
    "The $B$ here is same as the definition of $N$. \n",
    "#### Operator representation in a basis\n",
    "\n",
    "$$\n",
    "A= \\sum_{s_1s_2}\\ket{s_1}\\bra{s_1}A\\ket{s_2}\\bra{s_2} \\equiv \\sum_{s_1s_2}A_{s_1s_2}\\ket{s_1}\\bra{s_2} \n",
    "$$\n",
    "\n",
    "* The matrix representation above is for the linear transformation $A$ from the same input and output basis described by $s_1, s_2$. The input basis is the ket to the right of $A$. With this in mind, we can easily write the general case where the input and output bases are different in dimension. \n",
    "\n",
    "#### Basis change for an operator\n",
    "\n",
    "$$\n",
    "A= \\sum_{s_1 s_2 n_1 n_2}\\ket{n_1}\\braket{n_1}{s_1}\\bra{s_1}A\\ket{s_2}\\braket{s_2}{n_2}\\bra{n_2}=\\sum_{n_1n_2}\\ket{n_1}B_{n_1n_2}\\bra{n_2},\n",
    "$$\n",
    "where $$B_{n_1n_2} = \\sum_{s_1s_2}\\braket{n_1}{s_1}\\bra{s_1}A\\ket{s_2}\\braket{s_2}{n_2}$$\n",
    "\n",
    "* The above is equivalent to the relation $A_n = N^{-1} A_s N$ if the basis described by $(s_1, s_2)$ is standard basis, and the basis described by $(n_1,n_2)$ is orthogonal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric interpretation of linear transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a special 2D rotation matrix for example.  \n",
    "$$\n",
    "\\begin{pmatrix} \n",
    "\\text{cos}\\theta & -\\text{sin}\\theta \\\\\n",
    "\\text{sin}\\theta & \\text{cos}\\theta \n",
    "\\end{pmatrix}\n",
    "%\n",
    "\\underrightarrow{\\theta = 90^{\\circ}}\n",
    "%\n",
    "\\begin{pmatrix} \n",
    "0 & -1\\\\\n",
    "1 & 0 \n",
    "\\end{pmatrix}\n",
    "%\n",
    "\\underrightarrow{\\text{rotate vector}}\n",
    "\\begin{pmatrix} \n",
    "0 & -1\\\\\n",
    "1 & 0 \n",
    "\\end{pmatrix}\n",
    "%\n",
    "\\begin{pmatrix} \n",
    "1\\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix} \n",
    "-1\\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "When writing out the 2D rotation matrix, use the special rotation of two basis axes $(1,0)$ and $(0,1)$ to $(0,1)$ and $(-1,0)$ to figure out the signs of $\\text{sin}\\theta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First understanding: $Ax = y$ can be understood as linear operator $A$ acting (rotation or other linear transformation) on vector $x$ transforming it to another vector $y$. Note $x$ and $y$ are both in standard basis. In the example above, matrix $\\begin{pmatrix} 0 & -1\\\\\n",
    "1 & 0 \\end{pmatrix}$ transforms $\\begin{pmatrix} 1\\\\ 1\\end{pmatrix}$ to $\\begin{pmatrix} -1\\\\1\\end{pmatrix}$. The coordinates of the two vectors are all in the standard basis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Second understanding: From $Ax = y$, the vector $y$ can be understood as the linear combination of column vectors of matrix $A$ with coefficients in vector $x$. Thus, in the above example, the vector $\\begin{pmatrix} -1\\\\1\\end{pmatrix}$ in standard basis can be undertood as a vector $\\begin{pmatrix} 1\\\\ 1\\end{pmatrix}$ in the column space of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The equivalent statements above are not only true for the rotating, but also for general linear transformation. As an arbitrary matrix can be performed a singular value decomposition $A=U\\Sigma V^T$ where $U$ and $V$ are unitary matrices and $\\Sigma$ is a diagonal matrix, the action of a general matrix $A$ (can be non-square) can be understood as rotating by $V^T$, then stretching by $\\Sigma$, and rotating again by $U$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Transforming a basis by matrix $A$  \n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} \n",
    "0 & -1\\\\\n",
    "1 & 0 \n",
    "\\end{pmatrix}\n",
    "%\n",
    "\\begin{pmatrix} \n",
    "1 & 0\\\\\n",
    "0 & 1 \n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix} \n",
    "0 & -1\\\\\n",
    "1 & 0 \n",
    "\\end{pmatrix}\n",
    "$$\n",
    "This is obvious as we always have $AI = I$. Because $A$ above is a special rotating matrix (rotating $90^{\\circ}$), the geometric significance of the transformation above is also obvious. The basis vector $\\begin{pmatrix} 1\\\\0\\end{pmatrix}$ is transformed into $\\begin{pmatrix} 0\\\\1\\end{pmatrix}$, and $\\begin{pmatrix} 0\\\\1\\end{pmatrix}$ is transformed into $\\begin{pmatrix} -1\\\\0\\end{pmatrix}$. So $A$ always transform a standard basis to its column space. In this transformation, $A$ can be non-square as long as the column space of $A$ and standard basis have the same dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric interpretation of finding solution \n",
    "This part need be verified in the future. \n",
    "\n",
    "* Consider a vector $v_1 = \\begin{pmatrix} 2\\\\1\\end{pmatrix}$ vector in standard basis. Then rotate the vector and the two standard basis vectors $30^{\\circ}$ counterclockwise.  \n",
    "\n",
    "* First we can understand the vector in standard basis is rotated to $v_2 = \\begin{pmatrix} \\sqrt{3}-\\frac{1}{2}\\\\1+\\frac{\\sqrt{3}}{2}\\end{pmatrix}$. Here if we assume that $v_1$ is unknown, then finding $v_1$ from $A$ and $v_2$ can be understood as rotating $v_2$ back to $v_1$ by the inverse (if available) or pseudo inverse of matrix $A$. \n",
    "\n",
    "* Second we can understand the elements in $v_1$ as the coordinates in column space of $A$, $C(A)$. Although the same numbers, now $v_1$ is a different vector from that of in standard basis. Instead, it is same as the $v_2$ in standard basis. Again, if we assume that this $v_1 \\in C(A)$ is unknown, then to find $v_1$ from $Av_1 = v_2$, we write the original equation as $Av_1^{A} = Iv_2^{I}$, where superscripts $A$ and $I$ describe two spaces $C(A)$ and standard basis. Thus finding $v_1^{A}$ is just doing a basis change.  \n",
    "\n",
    "* From above, we know that the same tasking of finding $v_1$ can be understood as either undoing a linear transformation, or doing a basis change, depending from what perspective we are looking. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
