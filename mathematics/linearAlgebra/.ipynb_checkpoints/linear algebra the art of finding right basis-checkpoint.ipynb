{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question\n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do list\n",
    "* Basis change must be clearly understood in linear algebra and write up notes. (1) In ICA X = As, this corresponds to a basis change. In PCA we also have a basis change, compare with ICA in terms of basis change. FA also has a basis change? How many algorithms correspond to basis change? Be careful, in basis change, sometimes people may use row vector, and sometimes uses column vector. Right? (2) In ICA we have a latent random variable which produce the speaker's signals. Then in PCA what is the latent variable. In FA, we also have a latent variable z. All supervised learning have latent variable(s)?\n",
    "* classify component analysis with (1) whether new features necessary (new basis). or with (2) whether new latent variable is there? (3) Whether it is a probabilistic model. (4) Whether trying to use the benefit of lower-dimensional subspace. For example, if no correlations among features, then PCA is not so useful. So PCA mainly focus on dimension reduction, while ICA not.  **Note it seems not we can always think unsupervised learning is related to latent variables?** For example, Mixture of Gaussian, Naive Bayes, even FA are clearly related latent variable, even the formats are different, How about PCA? How to related PCA to a latent variable. ICA can be thought have latent variables. For example in the cocktail party problem, we can think of speakers are described by a latent variable.  \n",
    "* PCA do dimension reduction in eigenbasis, while FA do it in non-eigenbasis? Note the tumor 1d example for GDA, and gaussian mixture model for 2D are not good FA example, as they are already full rank, imagining the data in space, the data is not in a subspace. However, in FA, in 3D space, the data is mainly is from 2D, a lower dimension. Because the latent variable is d dimensiolnal which is lower or much lower then the data dimensional n. \n",
    "* All the factor analysis (in general sense) techniques are related to matrix factorizations? \n",
    "* All the factor analysis techniques are related to dimensional reduction? Otherwise no point to do so. \n",
    "* Add diagonal terms to diagonal is equivalent to do regularization. What is the  reasoning to do this in Factor analysis. \n",
    "* SVD related to ICA, FA, PCA, etc. and other 12 dimensional reduction techniques? How many of them related to eigenvalues? \n",
    "* first study wikipedia and be very familiar with the sentence why principal components are XV, and why this means extracting linear combination of the old variables. \n",
    "* Study many downloaded pdfs in the folder. \n",
    "* Study the 12 dimension reduction techniques and study their fundamentals. It is downloaded in the folder as PDF. \n",
    "* Also be very clear this is an orthogonal basis change, and related this to the above statement.\n",
    "* Linear transformation and basis change might need a very good understanding. \n",
    "* The biplot when people studying PCA is related to my work of doing effective rank finding? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "Because linear algebra is such a crucial building block for data science, a deep understanding of the most fundamental point of linear algebra would thus be very helpful in applying data science. Linear algebra is usually described as the art of finding right basis. Here we will try to connect many techniques in machine learning under the frame work of finding right basis. Many seemingly unrelated concepts such as factor analysis (FA), independent component analysis (ICA), principal component analysis (PCA), linear discriminant analysis (LDA) (Fisher's linear discriminant), quadratic discriminant analysis (QDA), feature extraction, linear expansion, linear combinations, etc., can all be unified with finding a right basis. \n",
    "\n",
    "Matrix factorization, or matrix decompositions such as eigen decomposition, singular value decomposition (SVD), LU, QR decomposition etc. are essentially all involving basis change.  \n",
    "\n",
    "The feature mapping or the kernel trick in support vector machine (SVM) is essentially also a basis change except it is mapping from a finite to a much higher or even infinite dimensional space. Similarly, the kernel trick applied in generalized linear models (GLM) and generalized discriminant analysis (GDA) is also a basis change that makes either GDA or GLM to handle nonlinear problems in higher dimensional spaces.  \n",
    "\n",
    "The fundamental point of finding a right basis or basis change in solving a problem is: in an appropriate basis the problems can either be much simplified, or it is possible to resolve the problem in the particular basis while it is impossible in a different basis.  \n",
    "\n",
    "The basics of representing a vector and a linear transformation in different bases is introduced in the LinearAlgebraFundamentals.ipynb. In this write-up, we focus on how different machine learning algorithms techniques employ basis changes in their implementations \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "## Introduction \n",
    "* Principal component analysis involves **extracting** linear composites of observed variables. So PCA is actually a feature extraction technique. Along the principal axis of the new coordinate system, the variance is maximized. \n",
    "\n",
    "* PCA uses an orthogonal transformation to convert a set of observations of possibly correlated variables (entities each of which takes on various numerical values) into a set of values of linearly uncorrelated variables called principal components, which are uncorrelated. **Comments:** Note the no-correlation means no linearly uncorrelated. But beyond linear, it might still be correlated.\n",
    "\n",
    "* Typically PCA is to calculate the eigenvectors of covariance matrix $X^TX$ (this form requires that $X$ is centered and each row is data vector). In the basis formed by these eigenvectors, $X^TX$ (but not $X$) will be diagonalized. This can be thought as maximizing the variance (the diagonal terms of covariance matrix $X^TX$), as the covariance terms (the off-diagonal terms of covariance matrix) all disappeared. Because we are considering the special symmetric matrix called covariance matrix $X^TX$ here, we thus achieve maximizing the variance. In other places (like diagonalizing Hamiltonian in quantum mechanics), we don't have such issues as maximizing variance. \n",
    "\n",
    "* Although PCA is typically introduced by diagonalizing the covariance matrix $X^TX$, people usually avoid doing so due to the numerical instability when forming the covariance matrix (see notes about linear algebra). Instead, people often implement PCA by using SVD. See details later. \n",
    "\n",
    "* If we are mainly do dimension reduction, then we can use SVD to replace PCA, as below:  \n",
    "$$A=U\\varSigma V^T$$\n",
    "$$A = U_r\\Sigma_r V_r^T = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuitive picture of PCA \n",
    "https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579\n",
    "\n",
    "* Check the above link and focus on the following two points: (A) First look for some wine properties (characteristics) that strongly differ across wines. PCA looks for properties that show as much variation across wines as possible. (B) Second look for properties that allow to reconstruct the original characteristics as well as possible. Surprisingly, it turns out that these two aims are equivalent, as shown in the animation of website: If the line is along the direction with most variance, then the total projection error will be minimized. \n",
    "\n",
    "* The above animation picture can also be understood with Pythagoras theorem, as shown in the animation.\n",
    "\n",
    "* The looking for maximum variance is also related to and eigenvectors and eigenvalues. In a diagonalized matrix, all the former off-diagonal covariances go to the diagonal elements, which are variances. Thus maximum variance is obtained when the matrix is diagonalized. \n",
    "\n",
    "* For formal proof of the equivalence of maximization of variance and finding eigenvectors, see notes eigenvalue_maxVariance.pdf and cs229-linalgbra.pdf.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA and SVD\n",
    "\n",
    "### Is there any advantage of SVD over PCA?\n",
    "SVD is a numerical method and PCA is an analysis approach (like least squares). You can do PCA using SVD, or you can do PCA doing the eigen-decomposition of $X^TX$ (or $XX^T$), or you can do PCA using many other methods, just like you can solve least squares with a dozen different algorithms like Newton's method or gradient descent or SVD etc. So there is no \"advantage\" to SVD over PCA because the two aren't comparable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion naming issues about PCA\n",
    "* Due to the usual definition of $n\\times p$ matrix $X$, where $n$ is the number of samples and $p$ is the number of variables, the $p\\times p$ covariance matrix $C = \\frac{X^TX}{n-1}$, but not $\\frac{XX^T}{n-1}$. Also this form is only true **when the data is centered**.  The covariance matrix $C$ is symmetric and thus can always be diagonalizable as $C=VLV^T$. Note the eigenvalues $\\lambda_i$ are in the decreasing order on the diagonal matrix $L$. \n",
    "\n",
    "* The eigenvectors in $V$ are called **principal axes** or **principal directions** of the data. \n",
    "\n",
    "* Projections of the data on the principal axes are called **principal components** or **PC scores**. Note however, people sometimes use these concepts for slightly different meanings. Note principal components are not principal axes. \n",
    "\n",
    "* The $jth$ principal component is given by $jth$ column of $XV$. The coordinates of the $ith$ data point in the new PC space are given by the $ith$ row of $XV$. This form is correct for the case that the row of $X$ is for a data point (a vector). The first row of $X$ dot multiply each column of $V$ (projections) gives the first row of $XV$ matrix. This row gives the projections of first data point (vector) (the first row of $X$) on different $V$ vectors(eigenvectors). Similarly, the first column of $XV$ gives the projections of all the data points (all the rows of $X$) on the first principal axis (the first column of $V$). \n",
    "\n",
    "* Some notation confusion clarified in the link: Even though $v_1$ (one of the eigenvectors of the covariance matrix, e.g. the first one) and $Xv_1$ (projection of the data onto the 1-dimensional subspace spanned by $v_1$) are two different things, both of them are often called \"principal component\", sometimes even in the same text. Below is a summary of the two conventions (the $u$ in the table is the $v$ mentioned above."
   ]
  },
  {
   "attachments": {
    "PCA_conventions.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PCA_conventions.png](attachment:PCA_conventions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping PCA terms to SVD \n",
    " \n",
    "https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca  \n",
    "  \n",
    "* Assuming SVD of $X = USV^T$, then it is easy to show $C = V\\frac{S^2}{n-1}V^T$. This means that right singular vectors $V$ are principal directions and that singular values are related to the eigenvalues of covariance matrix via $\\lambda_i = \\frac{s_i^2}{n-1}$. Note eigenvalues $\\lambda_i$ show variances of the respective PCs. Principal components (scores) are given by $XV = USV^TV = US$.  \n",
    "\n",
    "* Standardized score are given by columns of $\\sqrt{n-1}U$ and loadings are given by columns of $\\frac{VS}{n-1}$. See https://stats.stackexchange.com/questions/125684/how-does-fundamental-theorem-of-factor-analysis-apply-to-pca-or-how-are-pca-l and https://stats.stackexchange.com/questions/143905/loadings-vs-eigenvectors-in-pca-when-to-use-one-or-another for why 'loadings' should not be confused with principal directions. \n",
    "* **The above is correct only** if $X$ is centered and only the covariance matrix is equal to $X^TX/(n-1)$. \n",
    "* **The above is correct only** for $X$ having samples in rows and variables in columns. Otherwise, $U$ and $V$ exchange interpretations.\n",
    "* If one wants to perform PCA on a correlation matrix (instead of a covariance matrix), then columns of $X$ should not only be centered, but standardized as well, i.e., divided by their standard deviations. \n",
    "* To reduce the dimensionality of the data from $p$ to $k \\lt p$, select $k$ first columns of $U$, and $k\\times k$ upper-left part of $S$. Their product $U_kS_k$ is the required $n\\times k$ matrix containing first $k$ PCs. \n",
    "* Further multiplying the first $k$ PCs by the corresponding principal axes $V_k^T$ yields $X_k = U_kS_kV_k^T$ matrix that has the original $n\\times p$ size but is of lower rank (of rank $k$). This matrix $X_k$ provides a reconstruction of the original data from the first $k$ PCs. It has the lowest possible reconstruction error, as shown in https://stats.stackexchange.com/questions/130721/what-norm-of-the-reconstruction-error-is-minimized-by-the-low-rank-approximation. \n",
    "* Strictly speaking, $US$ is of $n\\times n$ size and $V$ is of $p\\times p$ size. However, if $n\\gt p$ then the last $n-p$ columns of $U$ are arbitrary (and corresponding rows of $S$ are zero); one should therefore use an economy size (or thin) SVD that returns $U$ of $n\\times p$ size, dropping the useless columns. For large $n>>p$ the matrix $U$ would otherwise be unnecessarily huge. The same applies for an opposite situation of $n<<p$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement PCA by diagonalizing covariance matrix and by SVD  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://math.stackexchange.com/questions/2359992/how-to-resolve-the-sign-issue-in-a-svd-problem  \n",
    "https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = \n",
      " [[ 0.09338628 -0.11086559 -0.02943783]\n",
      " [-0.11086559  0.18770817  0.0336127 ]\n",
      " [-0.02943783  0.0336127   0.12511719]]\n",
      "l = \n",
      " [0.27418905 0.11232653 0.01969604]\n",
      "V = \n",
      " [[ 0.53435576  0.10510519 -0.83869948]\n",
      " [-0.79577968 -0.27194755 -0.54109078]\n",
      " [-0.28495372  0.95655498 -0.06167616]]\n",
      "Y = \n",
      " [[-0.5382821   0.04170504 -0.17101639]\n",
      " [ 0.37801268 -0.26959854  0.10654358]\n",
      " [-0.60281427 -0.09375913  0.14821045]\n",
      " [ 0.31232627  0.5572872   0.03786103]\n",
      " [ 0.45075742 -0.23563458 -0.12159868]]\n",
      "(5, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "np.random.seed(42)\n",
    "\n",
    "def flip_signs(A, B):\n",
    "    \"\"\"\n",
    "    utility function for resolving the sign ambiguity in SVD\n",
    "    http://stats.stackexchange.com/q/34396/115202\n",
    "    \"\"\"\n",
    "    signs = np.sign(A) * np.sign(B)\n",
    "    #print(B*signs)\n",
    "    return A, B * signs\n",
    "    #Since the normalized eigenvectors are unique only up to a sign,eigenvectors calcuated \n",
    "    #with different approaches might have a sign difference. \n",
    "    #The function here will flip the sign of one vector if the signs of two vectors are different. \n",
    "\n",
    "\n",
    "# Let the data matrix X be of n x p size, where n is the number of samples and p is \n",
    "# the number of variables\n",
    "n, p = 5, 3\n",
    "X = np.random.rand(n, p) #generate a nxp matrix with random numbers. \n",
    "\n",
    "# Centerizing the data\n",
    "X -= np.mean(X, axis=0) # X = X - np.mean(X, axis=0)\n",
    "# A simple way to center the data\n",
    "\n",
    "# the p x p covariance matrix\n",
    "C = np.cov(X, rowvar=False) \n",
    "# Be careful the use of rowvar = False. Here each row is not a variable, but many variables.\n",
    "print (\"C = \\n\", C)\n",
    "\n",
    "# C is a symmetric matrix and so it can be diagonalized:\n",
    "l, principal_axes = la.eig(C)\n",
    "\n",
    "# sort results wrt. eigenvalues. This indicates that the 'raw' results are not ordered? Always? \n",
    "#Returns the indices that would sort this array.\n",
    "idx = l.argsort()[::-1]\n",
    "\n",
    "l, principal_axes = l[idx], principal_axes[:, idx]\n",
    "\n",
    "# the eigenvalues in decreasing order\n",
    "print( \"l = \\n\", l)\n",
    "\n",
    "# a matrix of eigenvectors (each column is an eigenvector)\n",
    "print( \"V = \\n\", principal_axes)\n",
    "\n",
    "# projections of X on the principal axes are called principal components.\n",
    "# Figure this out by checking .dot() elsewhere. \n",
    "principal_components = X.dot(principal_axes)\n",
    "print (\"Y = \\n\", principal_components)\n",
    "\n",
    "# we now perform singular value decomposition of X\n",
    "# \"economy size\" (or \"thin\") SVD. \n",
    "U, s, Vt = la.svd(X, full_matrices=False) # Shift+Tab see the meaning of full_matrices = False.\n",
    "V = Vt.T\n",
    "S = np.diag(s)\n",
    "\n",
    "\n",
    "\n",
    "# 1) then columns of V are principal directions/axes.\n",
    "assert np.allclose(*flip_signs(V, principal_axes))\n",
    "\n",
    "# 2) columns of US are principal components\n",
    "assert np.allclose(*flip_signs(U.dot(S), principal_components))\n",
    "\n",
    "# 3) singular values are related to the eigenvalues of covariance matrix\n",
    "assert np.allclose((s ** 2) / (n - 1), l)\n",
    "\n",
    "# 8) dimensionality reduction\n",
    "k = 2\n",
    "PC_k = principal_components[:, 0:k]\n",
    "US_k = U[:, 0:k].dot(S[0:k, 0:k])\n",
    "assert np.allclose(*flip_signs(PC_k, US_k))\n",
    "\n",
    "# 10) we used \"economy size\" (or \"thin\") SVD\n",
    "assert U.shape == (n, p)\n",
    "assert S.shape == (p, p)\n",
    "assert V.shape == (p, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of PCA \n",
    "* Represent the original data within this low-dimensional subspace: data compression.\n",
    "\n",
    "* Perform data visualization if dimensions of the subspace is low enough.\n",
    "\n",
    "* Image recognition: We can also efficiently calculate the distance between the projections of the original data on a few major principal axes. By comparing the distances, we can do image recognition. The key should be: in the reduced dimension, the distance is much easier to calculate. See https://en.wikipedia.org/wiki/Eigenface#Use_in_facial_recognition for an introduction. Be familiar with the geometric picture of the subtraction of two vectors. The different vector goes from the end of one vector to the other. \n",
    "\n",
    "* Do anomaly detection by calculating whether the distance of a data to the principal axis is very different from others. This is different from the image or pattern recognition. However, here we still make use of the advantage of using PCA, which is to calculate the distance very efficiently in a reduced dimension. Always use the 2D correlated data for intuitive picture. In 2D case, we may calculate distance just from 1D. \n",
    "\n",
    "* Do dimensional reduction in supervised learning. \n",
    "\n",
    "* Latent semantic indexing (LSI). We find similar articles by the similar approach as image recognition: comparing distances projected to principal axes. In LSI, usually skip the normalization because this will amplify the infrequent words. The similarity between articles is define as the $\\mathrm{cos}\\theta = \\frac{x^{(i)^T}x^{(j)}}{\\left\\|x^{(i)}\\right\\|^2\\left\\|x^{(j)}\\right\\|^2}$. Question: If we can compare distance to find similar articles, what is the purpose of using similarity defined above?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factor Analysis\n",
    "Study EM, etc. first, and then study cs229 docx notes.\n",
    "\n",
    "Not a single technique but a family of methods for analyzing a set of observed variables. Two basic branches in family tree: defined factors (aka principal components) and inferred factors (aka common factor analysis or classical factor analysis). \n",
    "* In principal components, we define new variables (factors), which are linear combinations of our observed variables. The focus is on expressing the new variables (the principal components) as weighted averages of the observed variables. The factors (properly called factor scores) have the same order (number of values) as the original variables. \n",
    "\n",
    "* In common factor analysis, we infer the existence of latent variables that explain the pattern of correlations among our observed variables the focus is on expressing the observed variables as a linear combination of underlying factors. \n",
    "\n",
    "* In both approaches, the factors are defined as linear combinations of the variables, and the variables are decomposed as linear combinations of the factors. Weird but true. \n",
    "\n",
    "* Two basic outputs from factor analysis: a set of column (variable) scores called factor loadings (each factor loading has as many values as there are variables in the data matrix), and a set of row (case) scores called factor scores (each factor score has as many values as there are cases in the data matrix).\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common Factor Analysis\n",
    "Given as input a rectangular, 2-mode matrix X whose columns are seen as variables, the objective of common factor analysis is to decompose (\"factor\") the variables in terms of a set of underlying \"latent\" variables called factors that are inferred from the pattern of correlations among the variables. The underlying factors are the \"reason for\" the observed correlations among the variables. That is, we assume that correlations among the variables are due to the fact that each variable is correlated with the underlying factors. So, if we simplify and assume there is just one underlying factor for a given set of variables, the idea is to see whether we can explain the observed correlation r(Y,Z) between two variables Y and Z as a function of the extent to which each is correlated with an unseen third variable, the factor F. That is, r(Y,Z) = r(Y,F)*r(Z,F). This known as Spearman's fundamental theorem of factor analysis. If there are two underlying factors, then the correlation between two variables is due to their correlations with each of the latent factors, like this:  r(Y,Z) = r(Y,F1)*r(Z,F1) + r(Y,F2)*r(Z,F2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis looks for linear combinations of the data matrix X that are uncorrelated and of high variance. We can write the data columns as linear combinations of the PCs.  \n",
    "Independent component analysis seeks to explain the data as linear combinations of independent factors.  \n",
    "Factor analysis seeks linear combinations of variables,called factors,that represent underlying fundamental quantities of which the observed variables are expressions. More precisely,the manifest variables are linear combinations of the factors,plus unique (or specific)factors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity and difference to PCA\n",
    "Check cs229, there is a problem set handling this. \n",
    "\n",
    "### Part I\n",
    "Both are data reduction techniques—they allow you to capture the variance in variables in a smaller set.  \n",
    "Both are usually run in stat software using the same procedure, and the output looks pretty much the same.  \n",
    "The steps you take to run them are the same—extraction, interpretation, rotation, choosing the number of factors or components.  \n",
    "Despite all these similarities, there is a fundamental difference between them: PCA is a linear combination of variables; Factor Analysis is a measurement model of a latent variable. See details in https://www.theanalysisfactor.com/the-fundamental-difference-between-principal-component-analysis-and-factor-analysis/.\n",
    "\n",
    "### Part II\n",
    "https://stats.stackexchange.com/questions/1576/what-are-the-differences-between-factor-analysis-and-principal-component-analysi\n",
    "Principal component analysis involves **extracting** linear composites of observed variables. (So PCA is actually is a feature extraction technique)  \n",
    "Factor analysis is based on a formal model predicting observed variables from theoretical latent factors.  \n",
    "In psychology these two techniques are often applied in the construction of multi-scale tests to determine which items load on which scales. They typically yield similar substantive conclusions (for a discussion see Comrey (1988) Factor-Analytic Methods of Scale Development in Personality and Clinical Psychology). This helps to explain why some statistics packages seem to bundle them together. I have also seen situations where \"principal component analysis\" is incorrectly labeled \"factor analysis\".  \n",
    "In terms of a simple rule of thumb, I'd suggest that you:  \n",
    "* Run factor analysis if you assume or wish to test a theoretical model of latent factors causing observed variables.\n",
    "* Run principal component analysis If you want to simply reduce your correlated observed variables to a smaller set of important independent composite variables.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
