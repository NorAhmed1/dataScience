{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiplication among matrices, vectors\n",
    "\n",
    "## Four ways of understanding matrix multiplication\n",
    "$A, B$ and $C$ are $m\\times n$, $n\\times p$ and $m \\times p$ respectively. The following four ways of treating matrix multiplication are useful different scenarios. \n",
    "\n",
    "### Inner product \n",
    "$$C = AB \\Longleftrightarrow C = \\{ c_{ij} \\} = \\{ a_i^T b_j \\} = \\{ \\sum_{k=1}^n a_{ik}b_{kj} \\} $$\n",
    "\n",
    "### Outer product \n",
    "$$C = AB \\Longleftrightarrow C = \\sum_{i=1}^n a_i b_i^T,$$\n",
    "where $a_i$ is column vector of $A$, and $b_i^T$ is row vector of $B$. The outer product of two vectors has rank one. **Note $b_i^T$ is not the transpose of column vector $b_i$ of $B$.**\n",
    "\n",
    "### Column picture\n",
    "$$Ax = x_1 A_{:,1} + x_2 A_{:,2} + x_3 A_{:,3} + ...$$\n",
    "$$AB = \\begin{bmatrix}AB_{:,1}&AB_{:,2}&AB_{:,3} + ...\\end{bmatrix},$$\n",
    "where $A_{:,i}, B_{:,i}$ are column vectors of matrices $A, B$. \n",
    "\n",
    "### Row picture\n",
    "$$x^{T}A = x_1A_{1,:} + x_2A_{2,:} + x_3A_{3,:} + ...$$\n",
    "$$AB = \\begin{bmatrix}A_{1,:}B \\\\ A_{2,:}B \\\\ A_{3,:}B \\\\...\\end{bmatrix},$$\n",
    "where $A_{i,:}$ is row vector.  \n",
    "\n",
    "### Examples\n",
    "* $Ax = b$. Linear combinations of columns of $A$ with coefficients $x$ gives $b$.\n",
    "* $x^TA = b^T$. Linear combinations of rows of $A$ with coefficients $x$ gives a row $b^T$; \n",
    "* $AB = C$ can be taken as right multiplying $B$ to $A$ to linear combine the columns of $A$. Each column of $B$ is a set of coefficients and will combine columns of $A$ to give a column in $C$.   \n",
    "* $AB = C$ can also be taken as left multiplying $A$ to $B$ to linear combine the rows of $B$. Each row of $A$ is a set of coefficients and will combine rows of $B$ to give a row in $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Element-wise Multiplications\n",
    "$A$ is a $m\\times n$ matrix.  \n",
    "$$z = Ax \\Longleftrightarrow z_i = \\sum_{k=1}^n a_{ik}y_k$$\n",
    "$$z^T = y^T A \\Longleftrightarrow z_i = \\sum_{k=1}^m y_k A_{ki}$$\n",
    "$$\\alpha = y^TAx \\Longleftrightarrow \\alpha = \\sum_{i=1}^m\\sum_{j=1}^n y_iA_{ij}x_j$$\n",
    "$$C = AB \\Longleftrightarrow c_{ij} = \\sum_{k=1}^n a_{ik}b_{kj}$$  \n",
    "* From left to right, keep the order of symbols in the LHS and let the summing indices adjacent. Use the dimension of either matrix or vector to determine the limit of summing index. \n",
    "* From right to left, rearrange the identical indices adjacent if they are not. If the vector index is in the left, need transpose sign. Another way is not rearranging, but using transpose. For example, \n",
    "$$\\sum_{i=1}^n a_j^{(i)}b_k^{(i)} = \\sum_{i=1}^n a_{ij}b_{ik}= \\sum_{i=1}^n (a^T)_{ji}b_{ik}= a^Tb$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Calculus\n",
    "* See matrix calculus entry of Wikipedia. \n",
    "* See the notes about matrix calculus in the cs229 linear algebra notes. Very nice complement to the notes below. \n",
    " \n",
    "## Basic relations\n",
    "It is in fact unnecessary to memorize these relations as they can be obtained by two simple ways.  \n",
    "* Dimensional analysis. Most relation can be obtained this way without element-wise examination. See the analysis for a relatively complicated example below (No. 7).  \n",
    "* Examining typical element. \n",
    "\n",
    "(1) Two vectors $y \\in \\mathbb{R}^m$, $x \\in \\mathbb{R}^n$. Let $y = \\psi (x)$. Then conventionally we have the definition  \n",
    "$$\\frac{\\partial y}{\\partial x}=\\begin{bmatrix}\\frac{\\partial y_{1}}{\\partial x_{1}} & \\frac{\\partial y_{1}}{\\partial x_{2}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "\\frac{\\partial y_{2}}{\\partial x_{1}} & \\frac{\\partial y_{2}}{\\partial x_{2}} & \\cdots & \\frac{\\partial y_{2}}{\\partial x_{n}}\\\\\n",
    "\\vdots & \\vdots &  & \\vdots\\\\\n",
    "\\frac{\\partial y_{m}}{\\partial x_{1}} & \\frac{\\partial y_{m}}{\\partial x_{2}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "\\end{bmatrix},$$\n",
    "which is called the Jacobian matrix of the transformation $\\psi()$.  Depending the dimension of $y$ and $x$, $\\frac{\\partial y}{\\partial x}$ might be a matrix, a column or a row vector. \n",
    "\n",
    "(2)  \n",
    "$$ y = Ax \\Rightarrow \\frac{\\partial y}{\\partial x} = A,$$\n",
    "where matrix $A$ does not depend on $x$. \n",
    "\n",
    "(3)  \n",
    "$$ y = Ax \\Rightarrow \\frac{\\partial y}{\\partial z} = \\frac{\\partial y}{\\partial x}\\frac{\\partial x}{\\partial z} =A\\frac{\\partial x}{\\partial z},$$\n",
    "where $x$ is function of $z$ and $A$ does not depend on both $x$ and $z$. \n",
    "\n",
    "(4)  \n",
    "$$\\alpha = y^TAx \\Rightarrow \\frac{\\partial \\alpha}{\\partial x} = y^TA \\quad \\mathrm{and} \\quad \\frac{\\partial \\alpha}{\\partial y} = x^TA^T,$$ \n",
    "\n",
    "(5)  \n",
    "$$\\alpha = x^TAx \\Rightarrow \\frac{\\partial \\alpha}{\\partial x} = x^T(A^T+A) = 2x^TA \\quad (A = A^T)$$\n",
    "This relation can be obtained by simple dimension analysis. It is just a special case of (7). However, here provide another way of examining typical elements.  \n",
    "$$\\alpha = \\sum_{i=1}^n\\sum_{j=1}^nx_ia_{i,j}x_j$$ Take derivative w.r.t $x_k$ gives\n",
    "$$\\frac{\\partial\\alpha}{\\partial x_k} = \\sum_{i=1}^n\\sum_{j=1}^n \\left(\\frac{\\partial x_i}{\\partial x_k}a_{i,j}x_j + x_ia_{i,j}\\frac{\\partial x_j}{\\partial x_k}\\right) = \\sum_{j=1}^n a_{kj}x_j + \\sum_{i=1}^na_{ik}x_i= \\sum_{j=1}^n x_ja_{jk}^* + \\sum_{i=1}^n x_ia_{ik} = x^T(A^T+A)$$\n",
    "\n",
    "(6) \n",
    "$$\\alpha = y^Tx \\Rightarrow  \\frac{\\partial \\alpha}{\\partial z} = \\frac{\\partial \\alpha}{\\partial y}\\frac{\\partial y}{\\partial z} + \\frac{\\partial \\alpha}{\\partial x}\\frac{\\partial x}{\\partial z} = x^T\\frac{\\partial y}{\\partial z} + y^T\\frac{\\partial x}{\\partial z}=2x^T\\frac{\\partial x}{\\partial z}\\quad (x=y),$$ \n",
    "where $x,y$ are functions of vector $z$. Here is the proof.\n",
    "$$ \\alpha = \\sum_{j=1}^n x_j y_j \\Rightarrow \\frac{\\partial \\alpha}{\\partial z_k} = \\sum_{j=1}^n \\left(x_j \\frac{\\partial y_j}{z_k}+y_j \\frac{\\partial x_j}{z_k}\\right)$$  \n",
    "The relation can also be obtained by dimensional analysis.  \n",
    "(7) \n",
    "$$\\alpha = y^TAx \\Rightarrow \\frac{\\partial \\alpha}{\\partial z} = x^TA^T\\frac{\\partial y}{\\partial z} + y^TA\\frac{\\partial x}{\\partial z}=2x^TA\\frac{\\partial x}{\\partial z}\\quad (x=y, A = A^T),$$\n",
    "where $x, y$ depend on $z$. The equation can be proved using the result in (6) using $\\alpha = y^TAx \\equiv w^Tx$.  \n",
    "\n",
    "A simple way to apply this relation. Assuming the dimensions of $x,y,z,A$ are respectively $n\\times 1, m\\times 1, k\\times 1, m\\times n$. The dimension of $\\alpha$ is $1\\times 1$. We first have \n",
    "$$ \\frac{\\partial \\alpha}{\\partial z} = \\frac{\\partial ( y^TAx)}{\\partial x}\\frac{\\partial x}{\\partial z} + \\frac{ \\partial (y^TAx)}{\\partial y}\\frac{\\partial y}{\\partial z} $$\n",
    "The dimension of the LHS is $1\\times k$. Thus we must arrange order or take transpose of the RHS expressions as proved earlier in order to have the same dimensions. In the analysis, be familiar with the conventional arrangement of Jacobian matrix. For example, from the dimensions of $x,y, z$, we know the dimension of $\\frac{\\partial y}{\\partial z}$ is $m\\times k$. \n",
    "\n",
    "(8) \n",
    "$$ \\frac{\\partial A^{-1}}{\\partial \\alpha} = -A^{-1}\\frac{\\partial A}{\\partial \\alpha} A^{-1},$$\n",
    "where $A$ is a $m\\times n$ matrix and $\\alpha$ is a scalar parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerations of using dimension analysis\n",
    "* Using dimension analysis can directly write many equations involving matrix calculus without element-wise calculations. However, although this trick is applicable for many cases, it is not for all. For example, it is suitable for $\\frac{\\partial y}{\\partial x}$ when $x,y$ are either scalar or vector,but not for the cases when $x, y$ are matrix. For example in the definition of gradient of $\\nabla_A f(A)$, its dimension is given by the dimension of $A$, rather than by the rule we used before.  \n",
    "* Different notations are used in literatures. For 1D vector, people usually do not distinguish $x$ and $x^T$. So the format of equations in different places such as here and cs229 notes is different.  \n",
    "* Note gradient and Hessian are defined only for scalar function. We cannot calculate $\\nabla_x(Ax)$ as $Ax$ is a vector. When calculating gradients, it is possible to use the dimension analysis to obtain equations. For example, for $f=x^TAx$, $\\nabla_x x^TAx = 2Ax$. For Hessian, however, we must use the original definition to calculate its element and then obtain the matrix-vector version. We **cannot take Hessian as taking gradient twice**, as the gradient of vector is not defined. See details in cs229.  \n",
    "* When taking gradient like $\\nabla f(Ax)$, we need make explicitly we differentiate relative to what. The above expression might have two interpretations (cs229 notes). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications \n",
    "\n",
    "(1) Gradients of the Determinant\n",
    "$$\\nabla_A \\left|A\\right| = \\left|A\\right|A^{-T}$$ \n",
    "$$\\nabla_A \\mathrm{log}\\left|A\\right| = \\frac{1}{\\left|A\\right|}\\nabla_A \\left|A\\right| = A^{-T}$$  \n",
    "See proof in the determinant section in this write up. The formula above has been used in the derivation of ICA.  \n",
    "\n",
    "(2) Gradient and Hessian \n",
    "$$\\nabla_x b^Tx = b$$\n",
    "$$\\nabla_x x^TAx = 2Ax \\quad (A = A^T)$$\n",
    "$$\\nabla_x^2 x^TAx = 2A (A = A^T)$$\n",
    "The hessian above is a symmetric matrix and positive semi-definite. **We can use dimension analysis to calculate the first two equations. But we need element-wise derivations for the Hessian.**. Also, when using dimension analysis, the format of above equations might be slightly different. For example, the first two equations will become $\\nabla_x b^Tx = b^T$ and, $\\nabla_x x^TAx = 2x^TA$.\n",
    "\n",
    "(3) Least square.  \n",
    "$$\\left \\|Ax -b \\right\\|^2 = (Ax - b)^T(Ax − b) = x^TA^TAx - 2b^TAx + b^Tb  $$  \n",
    "Taking the gradient with respect to $x$ we have, \n",
    "$$ \\nabla_x(x^TA^TAx - 2b^TAx + b^Tb) = 2A^TAx - 2A^Tb$$\n",
    "Setting this last expression equal to zero and solving for $x$ gives the normal equations\n",
    "$$x = (A^TA)^{−1}A^Tb$$\n",
    "With the dimension analysis approach, we can obtain the normal equation easily without another way using trace property, as in cs229 notes. To solve linear regression problems, we now have three ways: \n",
    "* Using projection operator. \n",
    "* Using the matrix calculus as above.\n",
    "* Using gradient descent. \n",
    "\n",
    "(4) Eigenvalues as Optimization\n",
    "$$\\text{max}_{x\\in \\mathbb{R}^n} x^TAx \\quad \\text{subject to }  \\left \\|x \\right \\|_2^2 = 1$$ for a symmetric matrix $A \\in \\mathbb{S}^n$. A standard way of solving optimization problems with equality constraints is by forming the Lagrangian,$L(x,\\lambda) = x^TAx − \\lambda x^Tx$, where $\\lambda$ is called the Lagrange multiplier associated with the equality constraint. It can be established that for $x^*$ to be a optimal point to the problem, the gradient of the Lagrangian has to be zero at $x^*$ (this is not the only condition, but it is required). That is,\n",
    "$$\\nabla_xL(x,\\lambda) = \\nabla_x(x^TAx - \\lambda x^Tx) = 2A^Tx − 2\\lambda x = 0$$\n",
    "Notice that this is just the linear equation $Ax = \\lambda x$. This shows that the only points which can possibly maximize (or minimize) $x^TAx$ assuming $\\left \\|x \\right \\|_2^2 = 1$ are the eigenvectors of $A$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Norms\n",
    "\n",
    "## Vector norms\n",
    "A norm is a FUNCTION that assigns a strictly positive length or size to each vector in a vector space except for the zero vector, which is assigned a length of zero. See the definition in the link https://en.wikipedia.org/wiki/Norm_(mathematics).    \n",
    "$l_p$ norms has the definition of $$\\left\\|x\\right\\|_p = \\left( \\sum_{i=1}^n |x_i|^p \\right )^{\\frac{1}{p}}$$\n",
    "When $p = 1, 2, \\infty $, we have the special $l_1, l_2, l_{\\infty}$ norms \n",
    "$$\\left\\|x\\right\\|_1 = \\left( \\sum_{i=1}^n |x_i| \\right ) \\\\ \\left\\|x\\right\\|_2 = \\sqrt{\\left( \\sum_{i=1}^n x_i^2 \\right )} = x^Tx \\\\ \\left\\|x\\right\\|_{\\infty} = \\text{max}_i |x_i|$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix norms\n",
    "http://mathworld.wolfram.com/MatrixNorm.html  \n",
    "https://en.wikipedia.org/wiki/Matrix_norm   \n",
    "Given a square complex or real matrix $A$, a matrix norm $\\left\\| A \\right\\|$ is a nonnegative number associated with $A$ having the properties. \n",
    "\n",
    "* $\\left\\| A \\right\\| > 0 $ when $\\left\\| A \\right\\| \\neq 0 $ and $\\left\\| A \\right\\|=0$ iff $A=0$,\n",
    "\n",
    "* $\\left\\| kA \\right\\|= |k|\\left\\| A \\right\\|$ for any scalar $k$,\n",
    "\n",
    "* $\\left\\| A+B \\right\\| \\leq \\left\\| A \\right\\| + \\left\\| B \\right\\|$,\n",
    "\n",
    "* $\\left\\| AB \\right\\| \\leq \\left\\| A \\right\\|\\left\\| B \\right\\|$  \n",
    "\n",
    "Let $\\lambda_1, \\lambda_2,...,\\lambda_n$ be the eigenvalues of $A$, then $$\\frac{1}{\\left\\| A^{-1} \\right\\|} \\leq |\\lambda| \\leq \\left\\| A \\right\\|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix norms induced by vector norms\n",
    "Suppose a vector norm is defined on $K^m$. Any $m\\times n$ matrix induces a linear operator from $K^n$ to $K^m$ with respect to the standard basis, and one defines the corresponding induced norm or operator norm on the space \n",
    "$$\\left\\| A \\right\\| = \\text{sup} \\left( \\frac{\\left \\| Ax \\right\\|}{\\left\\| x \\right\\|}:x \\in K^n \\text{  with  } x \\neq 0 \\right)$$\n",
    "In particular, if the $p$ norm for vectors is used for both spaces $K^n$ and $K^m$, then the induced operator norm (matrix norm) is: $$\\left \\|A\\right\\| = \\underset{x\\neq 0}{\\text{sup}} \\frac{\\left \\|Ax\\right \\|_p}{\\left \\|x\\right \\|_p}$$\n",
    "When $p$ takes special values of $1,2, \\infty$, we have the following special matrix norms.\n",
    "$$\\left \\|A\\right\\|_1 = \\underset{1\\leq j \\leq n}{\\text{max}} \\sum_{i = 1}^m |a_{ij}|,$$ which is simply the maximum absolute column sum of the matrix;\n",
    "$$ \\left \\|A\\right \\|_{\\infty} = \\underset{1\\leq i \\leq m}{\\text{max}} \\sum_{j = 1}^n |a_{ij}|,$$ which is simply the maximum absolute row of the matrix;\n",
    "$$ \\left \\|A\\right \\|_2 = \\sigma_{max}(A),$$ where $\\sigma_{max}(A)$ represents the largest singular value of matrix $A$. \n",
    "\n",
    "An important inequality for $p=2$ \n",
    "$$ \\left \\|A\\right \\|_2 = \\sigma_{max}(A) \\leq \\left \\|A \\right \\|_F = \\left(\\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^2 \\right)^{\\frac{1}{2}},$$ where $ \\left \\|A\\right \\|_F$ is the Frobenius norm. Equality holds if and only if the matrix $A$ is a rank-one matrix or a zero matrix. This inequality can be derived from the fact that the trace of a matrix is equal to the sum of its eigenvalues.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Entrywise\" and Schatten matrix norms\n",
    "\"Entrywise\" matrix norms treat an $m\\times n$ matrix as a vector of size $mn$, and use one of the familiar vector norms. For example, using the $p$-norm for vectors, $p\\geq 1$, we have \n",
    "$$ \\left \\|A\\right \\|_p = \\left \\|\\text{vec}(A)\\right \\|_p =\\left(\\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^p \\right)^{\\frac{1}{p}}$$\n",
    "This is a different norm from the induced $p$ norm and the Schatten $p$ norm (see Wiki), but the notation is the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric significance of matrix norms\n",
    "If we choose $p=2$ for the induced matrix norm, then $ \\left \\|A\\right \\|_2 = \\sigma_{max}(A),$ where $\\sigma_{max}(A)$ represents the largest singular value of matrix $A$. We know the geometric significance of singular value decomposition is 'rotate' -> 'stretch' ->  'rotate'. The stretching part is from the diagonal matrix $\\Sigma$, where each diagonal element represents the stretching or scaling factor to the axes. So $\\left \\|A\\right \\|_2$ is just the largest scaling factor of $A$ to a vector, or to a basis. This can also be seen in its original definition  \n",
    "$$\\left \\|A\\right\\| = \\underset{x\\neq 0}{\\text{sup}} \\frac{\\left \\|Ax\\right \\|_2}{\\left \\|x\\right \\|_2}$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
