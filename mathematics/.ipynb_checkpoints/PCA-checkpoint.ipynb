{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Principal_component_analysis\n",
    "\n",
    "## Introduction \n",
    "PCA has many names in various fields. For exmaple, it is called singular value decomposition (SVD) of X, eigenvalue decomposition (EVD) of $X^TX$ in linear algebra, factor analysis (for a discussion of the differences between PCA and factor analysis see Ch. 7 of Jolliffe's Principal Component Analysis. These statements suggest PCA is at least strongly related to SVD. \n",
    "\n",
    "PCA is mostly used as a tool in exploratory data analysis and for making predictive models. It is often used to visualize genetic distance and relatedness between populations. PCA can be done by eigenvalue decomposition of a data covariance (or correlation) matrix or singular value  decomposition of a data matrix, usually after a normalization step of the initial data. **Comments: covariance or correlation matrix must be square matrix and hence can be done with eigenvalue decomposion**. The normalization of each attribute consists of mean centering – subtracting each data value from its variable's measured mean so that its empirical mean (average) is zero – and, possibly, normalizing each variable's variance to make it equal to 1.\n",
    "\n",
    "PCA is the simplest of the true eigenvector-based multivariate analyses. Often, its operation can be thought of as revealing the internal structure of the data in a way that best explains the variance in the data. If a multivariate dataset is visualized as a set of coordinates in a high-dimensional data space (1 axis per variable), PCA can supply the user with a lower-dimensional picture, a projection of this object when viewed from its most informative viewpoint. This is done by using only the first few principal components so that the dimensionality of the transformed data is reduced.\n",
    "\n",
    "PCA is closely related to factor analysis. Factor analysis typically incorporates more domain specific assumptions about the underlying structure and solves eigenvectors of a slightly different matrix. **So FA is basically based on eigenvector analysis**. \n",
    "\n",
    "PCA is also related to canonical correlation analysis (CCA). CCA defines coordinate systems that optimally describe the cross-covariance between two datasets while PCA defines a new orthogonal coordinate system that optimally describes variance in a single dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation of PCA and SVD\n",
    "https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca\n",
    "Check the above link and several related links in the end. \n",
    "### Mapping PCA terms to SVD\n",
    "* Due to the usual definition of $n\\times p$ matrix $X$, where $n$ is the number of samples and $p$ is the number of variables, the $p\\times p$ covariance matrix $C = \\frac{X^TX}{n-1}$, but not $\\frac{XX^T}{n-1}$. Also this form is only true when the data is centered.  The covariance matrix $C$ is symmetric and thus can always be diagonalizable as $C=VLV^T$. Note the eigenvalues $\\lambda_i$ are in the decreasing order on the diagonal matrix $L$. \n",
    "* The eigenvectors in $V$ are called **principal axes** or **principal directions** of the data. \n",
    "* Projections of the data on the principal axes are called **principal components** or **PC scores**. \n",
    "* The $jth$ principal component is given by $jth$ column of $XV$. The coordinates of the $ith$ data point in the new PC space are given by the $ith$ row of $XV$. \n",
    "* Assuming SVD of $X = USV^T$, then it is easy to show $C = V\\frac{S^2}{n-1}V^T$. This means that right singular vectors $V$ are principal directions and that singular values are related to the eigenvalues of covariance matrix via $\\lambda_i = \\frac{s_i^2}{n-1}$. Note eigenvalues $\\lambda_i$ show variances of the respective PCs. Principal components (scores) are given by $XV = USV^TV = US$.\n",
    "* Standardized score are given by columns of $\\sqrt{n-1}U$ and loadings are given by columns of $\\frac{VS}{n-1}$. See https://stats.stackexchange.com/questions/125684/how-does-fundamental-theorem-of-factor-analysis-apply-to-pca-or-how-are-pca-l and https://stats.stackexchange.com/questions/143905/loadings-vs-eigenvectors-in-pca-when-to-use-one-or-another for why 'loadings' should not be confused with principal directions. \n",
    "* **The above is correct only** if $X$ is centered and only the covariance matrix is equal to $X^TX/(n-1)$. \n",
    "* **The above is correct only** for $X$ having samples in rows and variables in columns. Otherwise, $U$ and $V$ exchange interpretations.\n",
    "* If one wants to perform PCA on a correlation matrix (instead of a covariance matrix), then columns of $X$ should not only be centered, but standardized as well, i.e., divided by their standard deviations. \n",
    "* To reduce the dimensionality of the data from $p$ to $k \\lt p$, select $k$ first columns of $U$, and $k\\times k$ upper-left part of $S$. Their product $U_kS_k$ is the required $n\\times k$ matrix containing first $k$ PCs. \n",
    "* Further multiplying the first $k$ PCs by the corresponding principal axes $V_k^T$ yields $X_k = U_kS_kV_k^T$ matrix that has the original $n\\times p$ size but is of lower rank (of rank $k$). This matrix $X_k$ provides a reconstruction of the original data from the first $k$ PCs. It has the lowest possible reconstruction error, as shown in https://stats.stackexchange.com/questions/130721/what-norm-of-the-reconstruction-error-is-minimized-by-the-low-rank-approximation. \n",
    "* Strictly speaking, $US$ is of $n\\times n$ size and $V$ is of $p\\times p$ size. However, if $n\\gt p$ then the last $n-p$ columns of $U$ are arbitrary (and corresponding rows of $S$ are zero); one should therefore use an economy size (or thin) SVD that returns $U$ of $n\\times p$ size, dropping the useless columns. For large $n>>p$ the matrix $U$ would otherwise be unnecessarily huge. The same applies for an opposite situation of $n<<p$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is there any advantage of SVD over PCA?\n",
    "SVD is a numerical method and PCA is an analysis approach (like least squares). You can do PCA using SVD, or you can do PCA doing the eigen-decomposition of $X^TX$ (or $XX^T$), or you can do PCA using many other methods, just like you can solve least squares with a dozen different algorithms like Newton's method or gradient descent or SVD etc.\n",
    "\n",
    "So there is no \"advantage\" to SVD over PCA because it's like asking whether Newton's method is better than least squares: the two aren't comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuitive picture of PCA \n",
    "https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues/140579#140579\n",
    "\n",
    "* PCA checks what characteristics are redundant and discards them? No, PCA is not selecting some characteristics and discarding the others. Instead, it constructs some new characteristics that turn out to summarize our list of wines well. Of course these new characteristics are constructed using the old ones; for example, a new characteristic might be computed as wine age minus wine acidity level or some other combination like that (we call them linear combinations). In fact, PCA finds the best possible characteristics, the ones that summarize the list of wines as well as only possible (among all conceivable linear combinations). This is why it is so useful.\n",
    "\n",
    "* What do you actually mean when you say that these new PCA characteristics \"summarize\" the list of wines? (A) First answer is that you are looking for some wine properties (characteristics) that strongly differ across wines. PCA looks for properties that show as much variation across wines as possible. (B) The second answer is that you look for the properties that would allow you to predict, or \"reconstruct\", the original wine characteristics. So PCA looks for properties that allow to reconstruct the original characteristics as well as possible. (C) Surprisingly, it turns out that these two aims are equivalent and so PCA can kill two birds with one stone.\n",
    "\n",
    "* Why would be equivalent? See the excellent animation in the website. \n",
    "If the line is along the direction with most variance, then the total projection error will be minimized. This is because most of data are now almost on the line and thus with almost zero errors. \n",
    "\n",
    "* How does it related to Pythagoras theorem, and eigenvectors and eigenvalues? See the explanation on the website. Does it related to orthogonal projection? Also check the link https://stats.stackexchange.com/questions/217995/what-is-an-intuitive-explanation-for-how-pca-turns-from-a-geometric-problem-wit and see why maximize the variance is equivalent to finding eigenvectors. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C = \n",
      " [[ 0.09338628 -0.11086559 -0.02943783]\n",
      " [-0.11086559  0.18770817  0.0336127 ]\n",
      " [-0.02943783  0.0336127   0.12511719]]\n",
      "l = \n",
      " [0.27418905 0.11232653 0.01969604]\n",
      "V = \n",
      " [[ 0.53435576  0.10510519 -0.83869948]\n",
      " [-0.79577968 -0.27194755 -0.54109078]\n",
      " [-0.28495372  0.95655498 -0.06167616]]\n",
      "Y = \n",
      " [[-0.5382821   0.04170504 -0.17101639]\n",
      " [ 0.37801268 -0.26959854  0.10654358]\n",
      " [-0.60281427 -0.09375913  0.14821045]\n",
      " [ 0.31232627  0.5572872   0.03786103]\n",
      " [ 0.45075742 -0.23563458 -0.12159868]]\n"
     ]
    }
   ],
   "source": [
    "#The code is from the followin link\n",
    "#https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca\n",
    "import numpy as np\n",
    "from numpy import linalg as la\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def flip_signs(A, B):\n",
    "    \"\"\"\n",
    "    utility function for resolving the sign ambiguity in SVD\n",
    "    http://stats.stackexchange.com/q/34396/115202\n",
    "    \"\"\"\n",
    "    signs = np.sign(A) * np.sign(B)\n",
    "    return A, B * signs\n",
    "\n",
    "\n",
    "# Let the data matrix X be of n x p size,\n",
    "# where n is the number of samples and p is the number of variables\n",
    "n, p = 5, 3\n",
    "X = np.random.rand(n, p)\n",
    "# Let us assume that it is centered\n",
    "X -= np.mean(X, axis=0)\n",
    "\n",
    "# the p x p covariance matrix\n",
    "C = np.cov(X, rowvar=False)\n",
    "print (\"C = \\n\", C)\n",
    "# C is a symmetric matrix and so it can be diagonalized:\n",
    "l, principal_axes = la.eig(C)\n",
    "# sort results wrt. eigenvalues\n",
    "idx = l.argsort()[::-1]\n",
    "l, principal_axes = l[idx], principal_axes[:, idx]\n",
    "# the eigenvalues in decreasing order\n",
    "print( \"l = \\n\", l)\n",
    "# a matrix of eigenvectors (each column is an eigenvector)\n",
    "print( \"V = \\n\", principal_axes)\n",
    "# projections of X on the principal axes are called principal components\n",
    "principal_components = X.dot(principal_axes)\n",
    "print (\"Y = \\n\", principal_components)\n",
    "\n",
    "# we now perform singular value decomposition of X\n",
    "# \"economy size\" (or \"thin\") SVD\n",
    "U, s, Vt = la.svd(X, full_matrices=False)\n",
    "V = Vt.T\n",
    "S = np.diag(s)\n",
    "\n",
    "# 1) then columns of V are principal directions/axes.\n",
    "assert np.allclose(*flip_signs(V, principal_axes))\n",
    "\n",
    "# 2) columns of US are principal components\n",
    "assert np.allclose(*flip_signs(U.dot(S), principal_components))\n",
    "\n",
    "# 3) singular values are related to the eigenvalues of covariance matrix\n",
    "assert np.allclose((s ** 2) / (n - 1), l)\n",
    "\n",
    "# 8) dimensionality reduction\n",
    "k = 2\n",
    "PC_k = principal_components[:, 0:k]\n",
    "US_k = U[:, 0:k].dot(S[0:k, 0:k])\n",
    "assert np.allclose(*flip_signs(PC_k, US_k))\n",
    "\n",
    "# 10) we used \"economy size\" (or \"thin\") SVD\n",
    "assert U.shape == (n, p)\n",
    "assert S.shape == (p, p)\n",
    "assert V.shape == (p, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
