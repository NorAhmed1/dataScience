{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Preparing data\n",
    "\n",
    "### Reading DataFrames from multiple files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   NOC         Country   Total\n",
      "0  USA   United States  2088.0\n",
      "1  URS    Soviet Union   838.0\n",
      "2  GBR  United Kingdom   498.0\n",
      "3  FRA          France   378.0\n",
      "4  GER         Germany   407.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bronze = pd.read_csv('Bronze.csv')\n",
    "silver = pd.read_csv('Silver.csv')\n",
    "gold = pd.read_csv('Gold.csv')\n",
    "print(gold.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining DataFrames from multiple data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "medals = gold.copy()\n",
    "new_labels = ['NOC', 'Country', 'Gold']\n",
    "medals.columns = new_labels\n",
    "medals['Silver'] = silver['Total']\n",
    "medals['Bronze'] = bronze['Total']\n",
    "print(medals.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting DataFrame with the Index & columns\n",
    "The following is similar to 'order by' in SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "weatherDic = { 'Max TemperatureF':[68, 60, 68, 84, 88],'Month':['Jan','Feb','Mar','Apr','May']}\n",
    "weather1 = pd.DataFrame(weatherDic)\n",
    "weather1 = weather1.set_index('Month') \n",
    "weather2 = weather1.sort_index()\n",
    "weather3 = weather1.sort_index(ascending=False)\n",
    "weather4 = weather1.sort_values('Max TemperatureF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reindexing DataFrame from a list\n",
    "**Be familiar with .set_index() and .reindex()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Mean TemperatureF\n",
      "Month                   \n",
      "Jan            32.133333\n",
      "Feb                  NaN\n",
      "Mar                  NaN\n",
      "Apr            61.956044\n",
      "May                  NaN\n",
      "Jun                  NaN\n",
      "Jul            68.934783\n",
      "Aug                  NaN\n",
      "Sep                  NaN\n",
      "Oct            43.434783\n",
      "Nov                  NaN\n",
      "Dec                  NaN\n",
      "       Mean TemperatureF\n",
      "Month                   \n",
      "Jan            32.133333\n",
      "Feb            32.133333\n",
      "Mar            32.133333\n",
      "Apr            61.956044\n",
      "May            61.956044\n",
      "Jun            61.956044\n",
      "Jul            68.934783\n",
      "Aug            68.934783\n",
      "Sep            68.934783\n",
      "Oct            43.434783\n",
      "Nov            43.434783\n",
      "Dec            43.434783\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "year = ['Jan',\n",
    " 'Feb',\n",
    " 'Mar',\n",
    " 'Apr',\n",
    " 'May',\n",
    " 'Jun',\n",
    " 'Jul',\n",
    " 'Aug',\n",
    " 'Sep',\n",
    " 'Oct',\n",
    " 'Nov',\n",
    " 'Dec']\n",
    "\n",
    "weatherDic = { 'Mean TemperatureF':[61.956044, 32.133333, 68.934783, 43.434783],'Month':['Apr','Jan','Jul','Oct']}\n",
    "weather1 = pd.DataFrame(weatherDic)\n",
    "weather1 = weather1.set_index('Month')\n",
    "\n",
    "weather2 = weather1.reindex(year)\n",
    "\n",
    "print(weather2)\n",
    "\n",
    "weather3 = weather1.reindex(year).ffill()\n",
    "\n",
    "print(weather3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reindexing using another DataFrame Index\n",
    "**Note the multi-Index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  count\n",
      "name      gender       \n",
      "Anna      F        2698\n",
      "Emma      F        2034\n",
      "Elizabeth F        1852\n",
      "Margaret  F        1658\n",
      "Minnie    F        1653\n",
      "                count\n",
      "name    gender       \n",
      "Jessica F       42519\n",
      "Amanda  F       34370\n",
      "Sarah   F       28162\n",
      "Melissa F       28003\n",
      "Amy     F       20337\n",
      "(1934, 1)\n",
      "(19454, 1)\n",
      "(1934, 1)\n",
      "(1586, 1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "names_1981 = pd.read_csv(\"names1981.csv\")\n",
    "names_1881 = pd.read_csv(\"names1881.csv\")\n",
    "\n",
    "column_names = ['name','gender','count']\n",
    "names_1981.columns = column_names\n",
    "names_1881.columns = column_names\n",
    "\n",
    "names_1981 = names_1981.set_index(['name','gender'])\n",
    "names_1881 = names_1881.set_index(['name','gender'])\n",
    "print(names_1881.head())\n",
    "print(names_1981.head())\n",
    "\n",
    "print(names_1881.shape)\n",
    "print(names_1981.shape)\n",
    "\n",
    "common_names = names_1981.reindex(names_1881.index)\n",
    "print(common_names.shape)\n",
    "common_names = common_names.dropna()\n",
    "print(common_names.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding unaligned DataFrames\n",
    "If you were to add the following two DataFrames by executing the command total = january + february, how many rows would the resulting DataFrame have? \n",
    "\n",
    "january  \n",
    "                  Units  \n",
    "Company                 \n",
    "Acme Corporation     19  \n",
    "Hooli                17  \n",
    "Initech              20  \n",
    "Mediacore            10  \n",
    "Streeplex            13  \n",
    "\n",
    "february  \n",
    "                  Units  \n",
    "Company                \n",
    "Acme Corporation     15  \n",
    "Hooli                 3  \n",
    "Mediacore            13  \n",
    "Vandelay Inc         25  \n",
    "\n",
    "Answer:  \n",
    "january and february both consist of the sales of the Companies Acme Corporation, Hooli, and Mediacore. january has the additional two companies Initech and Streeplex, while february has the additional company Vandelay Inc. Together, they consist of the sales of 6 unique companies, and so total would have 6. \n",
    "**So this is like the full-join in SQL to some extent**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting in arithmetic formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Min TemperatureC  Mean TemperatureC  Max TemperatureC\n",
      "Date                                                           \n",
      "2013-1-1         -6.111111          -2.222222          0.000000\n",
      "2013-1-2         -8.333333          -6.111111         -3.888889\n",
      "2013-1-3         -8.888889          -4.444444          0.000000\n",
      "2013-1-4         -2.777778          -2.222222         -1.111111\n",
      "2013-1-5         -3.888889          -1.111111          1.111111\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "weather = pd.read_csv('pittsburgh2013.csv', index_col = 'Date')\n",
    "temps_f = weather[['Min TemperatureF','Mean TemperatureF','Max TemperatureF']]\n",
    "temps_c = (temps_f - 32) * 5/9\n",
    "temps_c.columns = temps_c.columns.str.replace('F', 'C')\n",
    "# Be aware of the .replace(). \n",
    "\n",
    "print(temps_c.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing percentage growth of GDP\n",
    "Compute the percentage growth of the resampled DataFrame yearly with .pct_change() * 100, which is defined on either Pandas Series or DataFrame.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              VALUE    growth\n",
      "DATE                         \n",
      "2008-12-31  14549.9       NaN\n",
      "2009-12-31  14566.5  0.114090\n",
      "2010-12-31  15230.2  4.556345\n",
      "2011-12-31  15785.3  3.644732\n",
      "2012-12-31  16297.3  3.243524\n",
      "2013-12-31  16999.9  4.311144\n",
      "2014-12-31  17692.2  4.072377\n",
      "2015-12-31  18222.8  2.999062\n",
      "2016-12-31  18436.5  1.172707\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "gdp = pd.read_csv('gdp_usa.csv', index_col='DATE', parse_dates=True)\n",
    "post2008 = gdp['2008':]\n",
    "yearly = post2008.resample('A').last() # The original is quartly data and thus resample by year.\n",
    "yearly['growth'] = yearly.pct_change() * 100\n",
    "print(yearly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting currency of stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Open        Close\n",
      "Date                                \n",
      "2015-01-02  1340.364425  1339.908750\n",
      "2015-01-05  1348.616555  1326.389506\n",
      "2015-01-06  1332.515980  1319.639876\n",
      "2015-01-07  1330.562125  1344.063112\n",
      "2015-01-08  1343.268811  1364.126161\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sp500 = pd.read_csv('sp500.csv',index_col='Date', parse_dates=True)\n",
    "exchange = pd.read_csv('exchange.csv',index_col='Date', parse_dates=True)\n",
    "dollars = sp500[['Open','Close']]\n",
    "pounds = dollars.multiply(exchange['GBP/USD'], axis=0)\n",
    "print(pounds.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating data\n",
    "\n",
    "Perform database-style operations to combine DataFrames: appending and concatenating DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending pandas Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20,)\n",
      "(20,)\n",
      "(20,)\n",
      "(60,)\n",
      "Date\n",
      "2015-01-27 07:11:55    18\n",
      "2015-02-02 08:33:01     3\n",
      "2015-02-02 20:54:49     9\n",
      "Name: Units, dtype: int64\n",
      "Date\n",
      "2015-02-26 08:57:45     4\n",
      "2015-02-26 08:58:51     1\n",
      "2015-03-06 10:11:45    17\n",
      "2015-03-06 02:03:56    17\n",
      "Name: Units, dtype: int64\n",
      "642\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "jan = pd.read_csv('sales-jan-2015.csv', index_col='Date', parse_dates=True)\n",
    "feb = pd.read_csv('sales-feb-2015.csv', index_col='Date', parse_dates=True)\n",
    "mar = pd.read_csv('sales-mar-2015.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "jan_units = jan['Units']\n",
    "feb_units = feb['Units']\n",
    "mar_units = mar['Units']\n",
    "\n",
    "quarter1 = jan_units.append(feb_units).append(mar_units)\n",
    "print(jan_units.shape)\n",
    "print(feb_units.shape)\n",
    "print(mar_units.shape)\n",
    "print(quarter1.shape)\n",
    "\n",
    "print(quarter1.loc['jan 27, 2015':'feb 2, 2015'])\n",
    "print(quarter1.loc['feb 26, 2015':'mar 7, 2015'])\n",
    "print(quarter1.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above appending DataFrame or Series is like union/union all of SQL set clause, which operate on rows?  \n",
    "\n",
    "### Concatenating pandas Series along row axis\n",
    "Having learned how to append Series, now learn how to achieve the same result by concatenating Series instead. Then **what is the difference between pd.concat() and pandas' .append() method.** One way to think of the difference is that .append() is a specific case of a concatenation, while pd.concat() gives you more flexibility, as you'll see in later exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2015-01-27 07:11:55    18\n",
      "2015-02-02 08:33:01     3\n",
      "2015-02-02 20:54:49     9\n",
      "Name: Units, dtype: int64\n",
      "Date\n",
      "2015-02-26 08:57:45     4\n",
      "2015-02-26 08:58:51     1\n",
      "2015-03-06 10:11:45    17\n",
      "2015-03-06 02:03:56    17\n",
      "Name: Units, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "units = []\n",
    "\n",
    "for month in [jan, feb, mar]:\n",
    "    units.append(month['Units'])\n",
    "\n",
    "quarter1 = pd.concat(units, axis='rows')\n",
    "\n",
    "print(quarter1.loc['jan 27, 2015':'feb 2, 2015'])\n",
    "print(quarter1.loc['feb 26, 2015':'mar 7, 2015'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending DataFrames with ignore_index\n",
    "DataFrames names_1981 and names_1881 are loaded without specifying an Index column (so the default Indexes for both are **RangeIndexes**). \n",
    "\n",
    "Use the DataFrame .append() method to make a DataFrame combined_names. To distinguish rows from the original two DataFrames, you'll add a 'year' column to each with the year (1881 or 1981 in this case). In addition, Specify ignore_index=True so that the index values are not used along the concatenation axis. The resulting axis will instead be labeled 0, 1, ..., n-1, which is useful if you are concatenating objects where the concatenation axis does not have meaningful indexing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=21388, step=1)\n",
      "Int64Index([    0,     1,     2,     3,     4,     5,     6,     7,     8,\n",
      "                9,\n",
      "            ...\n",
      "            19444, 19445, 19446, 19447, 19448, 19449, 19450, 19451, 19452,\n",
      "            19453],\n",
      "           dtype='int64', length=21388)\n",
      "(19454, 4)\n",
      "(1934, 4)\n",
      "(21388, 4)\n",
      "         name gender  count  year\n",
      "1282   Morgan      M     23  1881\n",
      "2094   Morgan      F   1769  1981\n",
      "14388  Morgan      M    766  1981\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "names_1981 = pd.read_csv(\"names1981.csv\")\n",
    "names_1881 = pd.read_csv(\"names1881.csv\")\n",
    "\n",
    "columnsList = ['name','gender','count']\n",
    "names_1981.columns = columnsList\n",
    "names_1881.columns = columnsList\n",
    "\n",
    "# Add 'year' column to names_1881 & names_1981\n",
    "names_1881['year'] = 1881\n",
    "names_1981['year'] = 1981 \n",
    "\n",
    "# Append names_1981 after names_1881 with ignore_index=True: combined_names\n",
    "combined_names = names_1881.append(names_1981, ignore_index=True) \n",
    "#This will use the default RangeIndex\n",
    "print(combined_names.index) \n",
    "# This will give a different index. May be compile the original together?\n",
    "combined_names1 = names_1881.append(names_1981)\n",
    "print(combined_names1.index)\n",
    "\n",
    "# Print shapes of names_1981, names_1881, and combined_names\n",
    "print(names_1981.shape)\n",
    "print(names_1881.shape)\n",
    "print(combined_names.shape)\n",
    "\n",
    "# Print all rows that contain the name 'Morgan'\n",
    "print(combined_names.loc[combined_names['name']=='Morgan'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating pandas DataFrames along column axis\n",
    "The function pd.concat() can concatenate DataFrames horizontally as well as vertically (vertical is the default). To make the DataFrames stack horizontally, you have to specify the keyword argument axis=1 or axis='columns'. **Here we know that unlike set clauses in SQL, pd.concat() can combine DataFrames both horizontally and vertically**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Max TemeratureF\n",
      "Month                 \n",
      "Jan                 68\n",
      "Apr                 89\n",
      "Jul                 91\n",
      "Oct                 84\n",
      "----------------------\n",
      "       Mean TemperatureF\n",
      "Month                   \n",
      "Apr            53.100000\n",
      "Aug            70.000000\n",
      "Dec            34.935484\n",
      "Feb            28.714286\n",
      "Jan            32.354839\n",
      "Jul            72.870968\n",
      "Jun            70.133333\n",
      "Mar            35.000000\n",
      "May            62.612903\n",
      "Nov            39.800000\n",
      "Oct            55.451613\n",
      "Sep            63.766667\n",
      "----------------------\n",
      "     Max TemeratureF  Mean TemperatureF\n",
      "Apr             89.0          53.100000\n",
      "Aug              NaN          70.000000\n",
      "Dec              NaN          34.935484\n",
      "Feb              NaN          28.714286\n",
      "Jan             68.0          32.354839\n",
      "Jul             91.0          72.870968\n",
      "Jun              NaN          70.133333\n",
      "Mar              NaN          35.000000\n",
      "May              NaN          62.612903\n",
      "Nov              NaN          39.800000\n",
      "Oct             84.0          55.451613\n",
      "Sep              NaN          63.766667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "weather_max = pd.DataFrame({'Month':['Jan','Apr','Jul','Oct'], 'Max TemeratureF':[68,89,91,84]})\n",
    "weather_max = weather_max.set_index('Month')\n",
    "print(weather_max)\n",
    "print('----------------------')\n",
    "weather_mean = pd.DataFrame({'Month':['Apr','Aug','Dec','Feb','Jan','Jul','Jun','Mar','May','Nov','Oct','Sep'],\n",
    "                            'Mean TemperatureF':[53.100000,70.000000,34.935484,28.714286,32.354839,72.870968,70.133333,\n",
    "                                35.000000,62.612903,39.800000,55.451613,63.766667]})\n",
    "weather_mean = weather_mean.set_index('Month') \n",
    "print(weather_mean)\n",
    "print('----------------------')\n",
    "\n",
    "weather = pd.concat([weather_max, weather_mean], axis=1)\n",
    "\n",
    "print(weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading multiple files to build a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                bronze  silver    gold\n",
      "France           475.0   461.0     NaN\n",
      "Germany          454.0     NaN   407.0\n",
      "Italy              NaN   394.0   460.0\n",
      "Soviet Union     584.0   627.0   838.0\n",
      "United Kingdom   505.0   591.0   498.0\n",
      "United States   1052.0  1195.0  2088.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "medal_types =  ['bronze', 'silver', 'gold']\n",
    "medals = []\n",
    "for medal in medal_types:\n",
    "    file_name = \"%s_top5.csv\" % medal\n",
    "    columns = ['Country', medal]\n",
    "    medal_df = pd.read_csv(file_name, header=0, index_col='Country', names=columns)\n",
    "    medals.append(medal_df) \n",
    "\n",
    "medals = pd.concat(medals, axis='columns')\n",
    "\n",
    "\n",
    "print(medals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating vertically to get MultiIndexed rows\n",
    "When stacking a sequence of DataFrames vertically, it is sometimes desirable to construct a MultiIndex to indicate the DataFrame from which each row originated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Total\n",
      "Country               \n",
      "United States   1052.0\n",
      "Soviet Union     584.0\n",
      "United Kingdom   505.0\n",
      "France           475.0\n",
      "Germany          454.0\n",
      "-----------\n",
      "                 Total\n",
      "Country               \n",
      "United States   1195.0\n",
      "Soviet Union     627.0\n",
      "United Kingdom   591.0\n",
      "France           461.0\n",
      "Italy            394.0\n",
      "-----------\n",
      "                 Total\n",
      "Country               \n",
      "United States   2088.0\n",
      "Soviet Union     838.0\n",
      "United Kingdom   498.0\n",
      "Italy            460.0\n",
      "Germany          407.0\n",
      "-----------\n",
      "                        Total\n",
      "       Country               \n",
      "bronze United States   1052.0\n",
      "       Soviet Union     584.0\n",
      "       United Kingdom   505.0\n",
      "       France           475.0\n",
      "       Germany          454.0\n",
      "silver United States   1195.0\n",
      "       Soviet Union     627.0\n",
      "       United Kingdom   591.0\n",
      "       France           461.0\n",
      "       Italy            394.0\n",
      "gold   United States   2088.0\n",
      "       Soviet Union     838.0\n",
      "       United Kingdom   498.0\n",
      "       Italy            460.0\n",
      "       Germany          407.0\n",
      "-----------\n",
      "MultiIndex(levels=[['bronze', 'silver', 'gold'], ['France', 'Germany', 'Italy', 'Soviet Union', 'United Kingdom', 'United States']],\n",
      "           labels=[[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2], [5, 3, 4, 0, 1, 5, 3, 4, 0, 2, 5, 3, 4, 2, 1]],\n",
      "           names=[None, 'Country'])\n"
     ]
    }
   ],
   "source": [
    "medal_types =  ['bronze', 'silver', 'gold']\n",
    "medals = []\n",
    "\n",
    "for medal in medal_types:\n",
    "\n",
    "    file_name = \"%s_top5.csv\" % medal\n",
    "\n",
    "    # Read file_name into a DataFrame: medal_df\n",
    "    medal_df = pd.read_csv(file_name, index_col='Country')\n",
    "    #print(medal_df.index)\n",
    "    # Append medal_df to medals\n",
    "    medals.append(medal_df)\n",
    "\n",
    "\n",
    "\n",
    "print(medals[0])\n",
    "print('-----------')\n",
    "print(medals[1])\n",
    "print('-----------')\n",
    "print(medals[2])\n",
    "print('-----------')\n",
    "    \n",
    "# Concatenate medals: medals\n",
    "\n",
    "# medals = pd.concat(medals) #Uncomment the following three and comment the later, and see what happens if no keys specified.\n",
    "# # Print medals\n",
    "# print(medals)\n",
    "# print('-----------')\n",
    "\n",
    "medals = pd.concat(medals, keys=['bronze', 'silver', 'gold'])\n",
    "print(medals)\n",
    "print('-----------')\n",
    "print(medals.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                 Total\n",
    "Country               \n",
    "United States   1052.0\n",
    "Soviet Union     584.0\n",
    "United Kingdom   505.0\n",
    "France           475.0\n",
    "Germany          454.0\n",
    "United States   1195.0\n",
    "Soviet Union     627.0\n",
    "United Kingdom   591.0\n",
    "France           461.0\n",
    "Italy            394.0\n",
    "United States   2088.0\n",
    "Soviet Union     838.0\n",
    "United Kingdom   498.0\n",
    "Italy            460.0\n",
    "Germany          407.0\n",
    "\n",
    "\n",
    "### Slicing MultiIndexed DataFrames\n",
    "This exercise picks up where the last ended (again using The Guardian's Olympic medal dataset).\n",
    "\n",
    "You are provided with the MultiIndexed DataFrame as produced at the end of the preceding exercise. Your task is to sort the DataFrame and to use the pd.IndexSlice to extract specific slices. Check out this exercise from Manipulating DataFrames with pandas to refresh your memory on how to deal with MultiIndexed DataFrames.\n",
    "\n",
    "pandas has been imported for you as pd and the DataFrame medals is already in your namespace.\n",
    "\n",
    "INSTRUCTIONS\n",
    "\n",
    "Create a new DataFrame medals_sorted with the entries of medals sorted. Use .sort_index(level=0) to ensure the Index is sorted suitably.\n",
    "Print the number of bronze medals won by Germany and all of the silver medal data. This has been done for you.\n",
    "Create an alias for pd.IndexSlice called idx. **A slicer pd.IndexSlice is required when slicing on the inner level of a MultiIndex.**\n",
    "Slice all the data on medals won by the United Kingdom. To do this, use the .loc[] accessor with idx[:,'United Kingdom'], :."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Total\n",
      "       Country               \n",
      "bronze United States   1052.0\n",
      "       Soviet Union     584.0\n",
      "       United Kingdom   505.0\n",
      "       France           475.0\n",
      "       Germany          454.0\n",
      "silver United States   1195.0\n",
      "       Soviet Union     627.0\n",
      "       United Kingdom   591.0\n",
      "       France           461.0\n",
      "       Italy            394.0\n",
      "gold   United States   2088.0\n",
      "       Soviet Union     838.0\n",
      "       United Kingdom   498.0\n",
      "       Italy            460.0\n",
      "       Germany          407.0\n",
      "------------------\n",
      "                        Total\n",
      "       Country               \n",
      "bronze France           475.0\n",
      "       Germany          454.0\n",
      "       Soviet Union     584.0\n",
      "       United Kingdom   505.0\n",
      "       United States   1052.0\n",
      "gold   Germany          407.0\n",
      "       Italy            460.0\n",
      "       Soviet Union     838.0\n",
      "       United Kingdom   498.0\n",
      "       United States   2088.0\n",
      "silver France           461.0\n",
      "       Italy            394.0\n",
      "       Soviet Union     627.0\n",
      "       United Kingdom   591.0\n",
      "       United States   1195.0\n",
      "------------------\n",
      "Total    454.0\n",
      "Name: (bronze, Germany), dtype: float64\n",
      "                 Total\n",
      "Country               \n",
      "France           461.0\n",
      "Italy            394.0\n",
      "Soviet Union     627.0\n",
      "United Kingdom   591.0\n",
      "United States   1195.0\n",
      "                       Total\n",
      "       Country              \n",
      "bronze United Kingdom  505.0\n",
      "gold   United Kingdom  498.0\n",
      "silver United Kingdom  591.0\n"
     ]
    }
   ],
   "source": [
    "print(medals)\n",
    "print('------------------')\n",
    "# Sort the entries of medals\n",
    "medals_sorted = medals.sort_index(level=0) #print(medals.index) can show what are level=0 index. \n",
    "\n",
    "print(medals_sorted)\n",
    "print('------------------')\n",
    "\n",
    "# Print the number of Bronze medals won by Germany\n",
    "print(medals_sorted.loc[('bronze','Germany')]) #Note how to locate multi-index with loc.\n",
    "print('------------------')\n",
    "\n",
    "# Print data about silver medals\n",
    "print(medals_sorted.loc['silver'])\n",
    "print('------------------')\n",
    "\n",
    "# Create alias for pd.IndexSlice: idx\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "# Print all the data on medals won by the United Kingdom\n",
    "print(medals_sorted.loc[idx[:,'United Kingdom'], :])\n",
    "print('------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating horizontally to get MultiIndexed columns\n",
    "It is also possible to construct a DataFrame with hierarchically indexed columns. For this exercise, you'll start with pandas imported and a list of three DataFrames called dataframes. All three DataFrames contain 'Company', 'Product', and 'Units' columns with a 'Date' column as the index pertaining to sales transactions during the month of February, 2015. The first DataFrame describes Hardware transactions, the second describes Software transactions, and the third, Service transactions.\n",
    "\n",
    "Your task is to concatenate the DataFrames horizontally and to create a MultiIndex on the columns. From there, you can summarize the resulting DataFrame and slice some information from it.\n",
    "\n",
    "INSTRUCTIONS\n",
    "\n",
    "Construct a new DataFrame february with MultiIndexed columns by concatenating the list dataframes.\n",
    "Use axis=1 to stack the DataFrames horizontally and the keyword argument keys=['Hardware', 'Software', 'Service'] to construct a hierarchical Index from each DataFrame.\n",
    "Print summary information from the new DataFrame february using the .info() method. This has been done for you.\n",
    "Create an alias called idx for pd.IndexSlice.\n",
    "Extract a slice called slice_2_8 from february (using .loc[] & idx) that comprises rows between Feb. 2, 2015 to Feb. 8, 2015 from columns under 'Company'.\n",
    "Print the slice_2_8. This has been done for you, so hit 'Submit Answer' to see the sliced data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             Company   Product  Units\n",
      "Date                                                 \n",
      "2015-02-04 21:52:45  Acme Coporation  Hardware     14\n",
      "2015-02-07 22:58:10  Acme Coporation  Hardware      1\n",
      "2015-02-19 10:59:33        Mediacore  Hardware     16\n",
      "2015-02-02 20:54:49        Mediacore  Hardware      9\n",
      "2015-02-21 20:41:47            Hooli  Hardware      3\n",
      "------------------\n",
      "                             Company   Product  Units\n",
      "Date                                                 \n",
      "2015-02-16 12:09:19            Hooli  Software     10\n",
      "2015-02-03 14:14:18          Initech  Software     13\n",
      "2015-02-02 08:33:01            Hooli  Software      3\n",
      "2015-02-05 01:53:06  Acme Coporation  Software     19\n",
      "2015-02-11 20:03:08          Initech  Software      7\n",
      "2015-02-09 13:09:55        Mediacore  Software      7\n",
      "2015-02-11 22:50:44            Hooli  Software      4\n",
      "2015-02-04 15:36:29        Streeplex  Software     13\n",
      "2015-02-21 05:01:26        Mediacore  Software      3\n",
      "------------------\n",
      "                       Company  Product  Units\n",
      "Date                                          \n",
      "2015-02-26 08:57:45  Streeplex  Service      4\n",
      "2015-02-25 00:29:00    Initech  Service     10\n",
      "2015-02-09 08:57:30  Streeplex  Service     19\n",
      "2015-02-26 08:58:51  Streeplex  Service      1\n",
      "2015-02-05 22:05:03      Hooli  Service     10\n",
      "2015-02-19 16:02:58  Mediacore  Service     10\n",
      "------------------\n",
      "                            Hardware                         Software  \\\n",
      "                             Company   Product Units          Company   \n",
      "Date                                                                    \n",
      "2015-02-02 08:33:01              NaN       NaN   NaN            Hooli   \n",
      "2015-02-02 20:54:49        Mediacore  Hardware   9.0              NaN   \n",
      "2015-02-03 14:14:18              NaN       NaN   NaN          Initech   \n",
      "2015-02-04 15:36:29              NaN       NaN   NaN        Streeplex   \n",
      "2015-02-04 21:52:45  Acme Coporation  Hardware  14.0              NaN   \n",
      "2015-02-05 01:53:06              NaN       NaN   NaN  Acme Coporation   \n",
      "2015-02-05 22:05:03              NaN       NaN   NaN              NaN   \n",
      "2015-02-07 22:58:10  Acme Coporation  Hardware   1.0              NaN   \n",
      "2015-02-09 08:57:30              NaN       NaN   NaN              NaN   \n",
      "2015-02-09 13:09:55              NaN       NaN   NaN        Mediacore   \n",
      "2015-02-11 20:03:08              NaN       NaN   NaN          Initech   \n",
      "2015-02-11 22:50:44              NaN       NaN   NaN            Hooli   \n",
      "2015-02-16 12:09:19              NaN       NaN   NaN            Hooli   \n",
      "2015-02-19 10:59:33        Mediacore  Hardware  16.0              NaN   \n",
      "2015-02-19 16:02:58              NaN       NaN   NaN              NaN   \n",
      "2015-02-21 05:01:26              NaN       NaN   NaN        Mediacore   \n",
      "2015-02-21 20:41:47            Hooli  Hardware   3.0              NaN   \n",
      "2015-02-25 00:29:00              NaN       NaN   NaN              NaN   \n",
      "2015-02-26 08:57:45              NaN       NaN   NaN              NaN   \n",
      "2015-02-26 08:58:51              NaN       NaN   NaN              NaN   \n",
      "\n",
      "                                       Service                 \n",
      "                      Product Units    Company  Product Units  \n",
      "Date                                                           \n",
      "2015-02-02 08:33:01  Software   3.0        NaN      NaN   NaN  \n",
      "2015-02-02 20:54:49       NaN   NaN        NaN      NaN   NaN  \n",
      "2015-02-03 14:14:18  Software  13.0        NaN      NaN   NaN  \n",
      "2015-02-04 15:36:29  Software  13.0        NaN      NaN   NaN  \n",
      "2015-02-04 21:52:45       NaN   NaN        NaN      NaN   NaN  \n",
      "2015-02-05 01:53:06  Software  19.0        NaN      NaN   NaN  \n",
      "2015-02-05 22:05:03       NaN   NaN      Hooli  Service  10.0  \n",
      "2015-02-07 22:58:10       NaN   NaN        NaN      NaN   NaN  \n",
      "2015-02-09 08:57:30       NaN   NaN  Streeplex  Service  19.0  \n",
      "2015-02-09 13:09:55  Software   7.0        NaN      NaN   NaN  \n",
      "2015-02-11 20:03:08  Software   7.0        NaN      NaN   NaN  \n",
      "2015-02-11 22:50:44  Software   4.0        NaN      NaN   NaN  \n",
      "2015-02-16 12:09:19  Software  10.0        NaN      NaN   NaN  \n",
      "2015-02-19 10:59:33       NaN   NaN        NaN      NaN   NaN  \n",
      "2015-02-19 16:02:58       NaN   NaN  Mediacore  Service  10.0  \n",
      "2015-02-21 05:01:26  Software   3.0        NaN      NaN   NaN  \n",
      "2015-02-21 20:41:47       NaN   NaN        NaN      NaN   NaN  \n",
      "2015-02-25 00:29:00       NaN   NaN    Initech  Service  10.0  \n",
      "2015-02-26 08:57:45       NaN   NaN  Streeplex  Service   4.0  \n",
      "2015-02-26 08:58:51       NaN   NaN  Streeplex  Service   1.0  \n",
      "------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 20 entries, 2015-02-02 08:33:01 to 2015-02-26 08:58:51\n",
      "Data columns (total 9 columns):\n",
      "(Hardware, Company)    5 non-null object\n",
      "(Hardware, Product)    5 non-null object\n",
      "(Hardware, Units)      5 non-null float64\n",
      "(Software, Company)    9 non-null object\n",
      "(Software, Product)    9 non-null object\n",
      "(Software, Units)      9 non-null float64\n",
      "(Service, Company)     6 non-null object\n",
      "(Service, Product)     6 non-null object\n",
      "(Service, Units)       6 non-null float64\n",
      "dtypes: float64(3), object(6)\n",
      "memory usage: 1.6+ KB\n",
      "None\n",
      "------------------\n",
      "                            Hardware         Software Service\n",
      "                             Company          Company Company\n",
      "Date                                                         \n",
      "2015-02-02 08:33:01              NaN            Hooli     NaN\n",
      "2015-02-02 20:54:49        Mediacore              NaN     NaN\n",
      "2015-02-03 14:14:18              NaN          Initech     NaN\n",
      "2015-02-04 15:36:29              NaN        Streeplex     NaN\n",
      "2015-02-04 21:52:45  Acme Coporation              NaN     NaN\n",
      "2015-02-05 01:53:06              NaN  Acme Coporation     NaN\n",
      "2015-02-05 22:05:03              NaN              NaN   Hooli\n",
      "2015-02-07 22:58:10  Acme Coporation              NaN     NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hardware = pd.read_csv('feb-sales-Hardware.csv',  index_col='Date', parse_dates=True)\n",
    "software = pd.read_csv('feb-sales-Software.csv',  index_col='Date', parse_dates=True)\n",
    "service  = pd.read_csv('feb-sales-Service.csv',  index_col='Date', parse_dates=True)\n",
    "\n",
    "dataframes = [hardware, software, service]\n",
    "print(dataframes[0])\n",
    "print('------------------')\n",
    "print(dataframes[1])\n",
    "print('------------------')\n",
    "\n",
    "print(dataframes[2])\n",
    "print('------------------')\n",
    "\n",
    "\n",
    "# my code above\n",
    "\n",
    "# # Concatenate dataframes: february\n",
    "# february = pd.concat(dataframes, axis=1) #Uncomment the following three and comment the three later and compare. They cannot compare simultaneously.\n",
    "# print(february)\n",
    "# print('------------------')\n",
    "\n",
    "february = pd.concat(dataframes, keys=['Hardware','Software','Service'], axis=1)\n",
    "print(february)\n",
    "print('------------------')\n",
    "\n",
    "# Print february.info()\n",
    "print(february.info())\n",
    "print('------------------')\n",
    "\n",
    "# Assign pd.IndexSlice: idx\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "# Create the slice: slice_2_8\n",
    "slice_2_8 = february.loc['2015-2-2':'2015-2-8', idx[:,'Company']]\n",
    "\n",
    "# Print slice_2_8\n",
    "print(slice_2_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating DataFrames from a dict\n",
    "You're now going to revisit the sales data you worked with earlier in the chapter. Three DataFrames jan, feb, and mar have been pre-loaded for you. Your task is to aggregate the sum of all sales over the 'Company' column into a single DataFrame. You'll do this by constructing a dictionary of these DataFrames and then concatenating them.\n",
    "\n",
    "INSTRUCTIONS\n",
    "\n",
    "Create a list called month_list consisting of the tuples ('january', jan), ('february', feb), and ('march', mar).\n",
    "Create an empty dictionary called month_dict.\n",
    "Inside the for loop:\n",
    "Group month_data by 'Company' and use .sum() to aggregate.\n",
    "Construct a new DataFrame called sales by concatenating the DataFrames stored in month_dict.\n",
    "Create an alias for pd.IndexSlice and print all sales by 'Mediacore'. This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "january\n",
      "-----------------\n",
      "                             Company   Product  Units\n",
      "Date                                                 \n",
      "2015-01-21 19:13:21        Streeplex  Hardware     11\n",
      "2015-01-09 05:23:51        Streeplex   Service      8\n",
      "2015-01-06 17:19:34          Initech  Hardware     17\n",
      "2015-01-02 09:51:06            Hooli  Hardware     16\n",
      "2015-01-11 14:51:02            Hooli  Hardware     11\n",
      "2015-01-01 07:31:20  Acme Coporation  Software     18\n",
      "2015-01-24 08:01:16          Initech  Software      1\n",
      "2015-01-25 15:40:07          Initech   Service      6\n",
      "2015-01-13 05:36:12            Hooli   Service      7\n",
      "2015-01-03 18:00:19            Hooli   Service     19\n",
      "2015-01-16 00:33:47            Hooli  Hardware     17\n",
      "2015-01-16 07:21:12          Initech   Service     13\n",
      "2015-01-20 19:49:24  Acme Coporation  Hardware     12\n",
      "2015-01-26 01:50:25  Acme Coporation  Software     14\n",
      "2015-01-15 02:38:25  Acme Coporation   Service     16\n",
      "2015-01-06 13:47:37  Acme Coporation  Software     16\n",
      "2015-01-15 15:33:40        Mediacore  Hardware      7\n",
      "2015-01-27 07:11:55        Streeplex   Service     18\n",
      "2015-01-20 11:28:02        Streeplex  Software     13\n",
      "2015-01-16 19:20:46        Mediacore   Service      8\n",
      "-----------------\n",
      "                 Units\n",
      "Company               \n",
      "Acme Coporation     76\n",
      "Hooli               70\n",
      "Initech             37\n",
      "Mediacore           15\n",
      "Streeplex           50\n",
      "-----------------\n",
      "february\n",
      "-----------------\n",
      "                             Company   Product  Units\n",
      "Date                                                 \n",
      "2015-02-26 08:57:45        Streeplex   Service      4\n",
      "2015-02-16 12:09:19            Hooli  Software     10\n",
      "2015-02-03 14:14:18          Initech  Software     13\n",
      "2015-02-02 08:33:01            Hooli  Software      3\n",
      "2015-02-25 00:29:00          Initech   Service     10\n",
      "2015-02-05 01:53:06  Acme Coporation  Software     19\n",
      "2015-02-09 08:57:30        Streeplex   Service     19\n",
      "2015-02-11 20:03:08          Initech  Software      7\n",
      "2015-02-04 21:52:45  Acme Coporation  Hardware     14\n",
      "2015-02-09 13:09:55        Mediacore  Software      7\n",
      "2015-02-07 22:58:10  Acme Coporation  Hardware      1\n",
      "2015-02-11 22:50:44            Hooli  Software      4\n",
      "2015-02-26 08:58:51        Streeplex   Service      1\n",
      "2015-02-05 22:05:03            Hooli   Service     10\n",
      "2015-02-04 15:36:29        Streeplex  Software     13\n",
      "2015-02-19 16:02:58        Mediacore   Service     10\n",
      "2015-02-19 10:59:33        Mediacore  Hardware     16\n",
      "2015-02-02 20:54:49        Mediacore  Hardware      9\n",
      "2015-02-21 05:01:26        Mediacore  Software      3\n",
      "2015-02-21 20:41:47            Hooli  Hardware      3\n",
      "-----------------\n",
      "                 Units\n",
      "Company               \n",
      "Acme Coporation     34\n",
      "Hooli               30\n",
      "Initech             30\n",
      "Mediacore           45\n",
      "Streeplex           37\n",
      "-----------------\n",
      "march\n",
      "-----------------\n",
      "                             Company   Product  Units\n",
      "Date                                                 \n",
      "2015-03-22 14:42:25        Mediacore  Software      6\n",
      "2015-03-12 18:33:06          Initech   Service     19\n",
      "2015-03-22 03:58:28        Streeplex  Software      8\n",
      "2015-03-15 00:53:12            Hooli  Hardware     19\n",
      "2015-03-17 19:25:37            Hooli  Hardware     10\n",
      "2015-03-16 05:54:06        Mediacore  Software      3\n",
      "2015-03-25 10:18:10          Initech  Hardware      9\n",
      "2015-03-25 16:42:42        Streeplex  Hardware     12\n",
      "2015-03-26 05:20:04        Streeplex  Software      3\n",
      "2015-03-06 10:11:45        Mediacore  Software     17\n",
      "2015-03-22 21:14:39          Initech  Hardware     11\n",
      "2015-03-17 19:38:12            Hooli  Hardware      8\n",
      "2015-03-28 19:20:38  Acme Coporation   Service      5\n",
      "2015-03-13 04:41:32        Streeplex  Hardware      8\n",
      "2015-03-06 02:03:56        Mediacore  Software     17\n",
      "2015-03-13 11:40:16          Initech  Software     11\n",
      "2015-03-27 08:29:45        Mediacore  Software      6\n",
      "2015-03-21 06:42:41        Mediacore  Hardware     19\n",
      "2015-03-15 08:50:45          Initech  Hardware     18\n",
      "2015-03-13 16:25:24        Streeplex  Software      9\n",
      "-----------------\n",
      "                 Units\n",
      "Company               \n",
      "Acme Coporation      5\n",
      "Hooli               37\n",
      "Initech             68\n",
      "Mediacore           68\n",
      "Streeplex           40\n",
      "-----------------\n",
      "                          Units\n",
      "         Company               \n",
      "february Acme Coporation     34\n",
      "         Hooli               30\n",
      "         Initech             30\n",
      "         Mediacore           45\n",
      "         Streeplex           37\n",
      "january  Acme Coporation     76\n",
      "         Hooli               70\n",
      "         Initech             37\n",
      "         Mediacore           15\n",
      "         Streeplex           50\n",
      "march    Acme Coporation      5\n",
      "         Hooli               37\n",
      "         Initech             68\n",
      "         Mediacore           68\n",
      "         Streeplex           40\n",
      "                    Units\n",
      "         Company         \n",
      "february Mediacore     45\n",
      "january  Mediacore     15\n",
      "march    Mediacore     68\n"
     ]
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load 'sales-jan-2015.csv' into a DataFrame: jan\n",
    "jan = pd.read_csv('sales-jan-2015.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Load 'sales-feb-2015.csv' into a DataFrame: feb\n",
    "feb = pd.read_csv('sales-feb-2015.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Load 'sales-mar-2015.csv' into a DataFrame: mar\n",
    "mar = pd.read_csv('sales-mar-2015.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "\n",
    "#my code above\n",
    "\n",
    "# Make the list of tuples: month_list\n",
    "month_list = [('january', jan), ('february', feb), ('march', mar)]\n",
    "\n",
    "# Create an empty dictionary: month_dict\n",
    "month_dict = {}\n",
    "\n",
    "for month_name, month_data in month_list:\n",
    "\n",
    "    # Group month_data: month_dict[month_name]\n",
    "    month_dict[month_name] = month_data.groupby('Company').sum()\n",
    "    print(month_name)\n",
    "    print('-----------------')\n",
    "    print(month_data)\n",
    "    print('-----------------')\n",
    "    print(month_dict[month_name])  ##Check why there are fewer columns after group by. Seems not as before.\n",
    "    print('-----------------')\n",
    "\n",
    "# Concatenate data in month_dict: sales\n",
    "sales = pd.concat(month_dict)\n",
    "\n",
    "# Print sales\n",
    "print(sales)\n",
    "\n",
    "# Print all sales by Mediacore\n",
    "idx = pd.IndexSlice\n",
    "print(sales.loc[idx[:, 'Mediacore'], :])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There is a question about group by above**  \n",
    "\n",
    "### Concatenating DataFrames with inner join\n",
    "Here, you'll continue working with DataFrames compiled from The Guardian's Olympic medal dataset.\n",
    "\n",
    "The DataFrames bronze, silver, and gold have been pre-loaded for you.\n",
    "\n",
    "Your task is to compute an inner join.\n",
    "\n",
    "INSTRUCTIONS\n",
    "\n",
    "Construct a list of DataFrames called medal_list with entries bronze, silver, and gold.\n",
    "Concatenate medal_list horizontally with an inner join to create medals.\n",
    "Use the keyword argument keys=['bronze', 'silver', 'gold'] to yield suitable hierarchical indexing.\n",
    "Use axis=1 to get horizontal concatenation.\n",
    "Use join='inner' to keep only rows that share common index labels.\n",
    "Print the new DataFrame medals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                bronze  silver    gold\n",
      "                 Total   Total   Total\n",
      "Country                               \n",
      "United States   1052.0  1195.0  2088.0\n",
      "Soviet Union     584.0   627.0   838.0\n",
      "United Kingdom   505.0   591.0   498.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bronze = pd.DataFrame({'Country':['United States','Soviet Union','United Kingdom','France','Germany'],\n",
    "                      'Total':[1052.0,584.0,505.0,475.0,454.0]})\n",
    "bronze = bronze.set_index('Country')\n",
    "\n",
    "\n",
    "silver = pd.DataFrame({'Country':['United States','Soviet Union','United Kingdom','France','Italy'],\n",
    "                      'Total':[1195.0,627.0,591.0,461.0,394.0]})\n",
    "silver = silver.set_index('Country')\n",
    "\n",
    "\n",
    "gold = pd.DataFrame({'Country':['United States','Soviet Union','United Kingdom','Italy','Germany'],\n",
    "                      'Total':[2088.0,838.0,498.0,460.0,407.0]})\n",
    "gold = gold.set_index('Country')\n",
    "\n",
    "#my code above\n",
    "\n",
    "# Create the list of DataFrames: medal_list\n",
    "medal_list = [bronze, silver, gold]\n",
    "\n",
    "# Concatenate medal_list horizontally using an inner join: medals\n",
    "medals = pd.concat(medal_list, keys=['bronze', 'silver', 'gold'], axis=1, join='inner')\n",
    "\n",
    "# Print medals\n",
    "print(medals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling & concatenating DataFrames with inner join\n",
    "In this exercise, you'll compare the historical 10-year GDP (Gross Domestic Product) growth in the US and in China. The data for the US starts in 1947 and is recorded quarterly; by contrast, the data for China starts in 1961 and is recorded annually.\n",
    "\n",
    "You'll need to use a combination of resampling and an inner join to align the index labels. You'll need an appropriate offset alias for resampling, and the method .resample() must be chained with some kind of aggregation method (.pct_change() and .last() in this case).\n",
    "\n",
    "pandas has been imported as pd, and the DataFrames china and us have been pre-loaded, with the output of china.head() and us.head() printed in the IPython Shell.\n",
    "\n",
    "INSTRUCTIONS\n",
    "\n",
    "Make a new DataFrame china_annual by resampling the DataFrame china with .resample('A') (i.e., with annual frequency) and chaining two method calls:\n",
    "Chain .pct_change(10) as an aggregation method to compute the percentage change with an offset of ten years.\n",
    "Chain .dropna() to eliminate rows containing null values.\n",
    "Make a new DataFrame us_annual by resampling the DataFrame us exactly as you resampled china.\n",
    "Concatenate china_annual and us_annual to construct a DataFrame called gdp. Use join='inner' to perform an inner join and use axis=1 to concatenate horizontally.\n",
    "Print the result of resampling gdp every decade (i.e., using .resample('10A')) and aggregating with the method .last(). This has been done for you, so hit 'Submit Answer' to see the result!\n",
    "\n",
    "**I need make it clear about the usage of 'A' and '10A' etc.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                China\n",
      "Year                 \n",
      "1960-01-01  59.184116\n",
      "1961-01-01  49.557050\n",
      "1962-01-01  46.685179\n",
      "1963-01-01  50.097303\n",
      "1964-01-01  59.062255\n",
      "               US\n",
      "Year             \n",
      "1947-01-01  243.1\n",
      "1947-04-01  246.3\n",
      "1947-07-01  250.1\n",
      "1947-10-01  260.3\n",
      "1948-01-01  266.2\n",
      "               China        US\n",
      "Year                          \n",
      "1970-12-31  0.546128  0.980397\n",
      "1971-12-31  0.988860  1.073188\n",
      "1972-12-31  1.402472  1.119273\n",
      "1973-12-31  1.730085  1.237090\n",
      "1974-12-31  1.408556  1.258503\n",
      "               China        US\n",
      "Year                          \n",
      "1970-12-31  0.546128  0.980397\n",
      "1980-12-31  1.072537  1.660540\n",
      "1990-12-31  0.892820  1.088953\n",
      "2000-12-31  2.357522  0.719980\n",
      "2010-12-31  4.011081  0.455009\n",
      "2020-12-31  3.789936  0.377506\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "china = pd.read_csv(\"gdp_china.csv\", index_col = \"Year\", parse_dates = True )\n",
    "china.columns = ['China'] #***# This needs []\n",
    "china.index.name = 'Year' #***# This does not need []\n",
    "print(china.head())\n",
    "us = pd.read_csv(\"gdp_usa.csv\", index_col = \"DATE\", parse_dates = True)\n",
    "us.columns =['US']\n",
    "us.index.name = 'Year'\n",
    "print(us.head())\n",
    "\n",
    "#my code above\n",
    "\n",
    "# Resample and tidy china: china_annual\n",
    "china_annual = china.resample('A').mean().pct_change(10).dropna()\n",
    "\n",
    "# Resample and tidy us: us_annual\n",
    "us_annual = us.resample('A').mean().pct_change(10).dropna()\n",
    "\n",
    "# Concatenate china_annual and us_annual: gdp\n",
    "gdp = pd.concat([china_annual, us_annual], axis=1, join='inner')\n",
    "\n",
    "print(gdp.head())\n",
    "\n",
    "# Resample gdp and print\n",
    "print(gdp.resample('10A').last())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the above example, if mean() is deleted, it still will call mean() by default and will give warnings**. \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging data\n",
    "\n",
    "Here, you'll learn all about merging pandas DataFrames. You'll explore different techniques for merging, and learn about left joins, right joins, inner joins, and outer joins, as well as when to use which. You'll also learn about ordered merging, which is useful when you want to merge DataFrames whose columns have natural orderings, like date-time columns.\n",
    "\n",
    "**First clarify the difference between last and this chapters: Concatenating and merging**.  \n",
    "\n",
    "### Merging company DataFrames\n",
    "Suppose your company has operations in several different cities under several different managers. The DataFrames revenue and managers contain partial information related to the company. That is, the rows of the city columns don't quite match in revenue and managers (the Mendocino branch has no revenue yet since it just opened and the manager of Springfield branch recently left the company).\n",
    "\n",
    "          city  revenue\n",
    "0       Austin      100\n",
    "1       Denver       83\n",
    "2  Springfield        4\n",
    "\n",
    "        city   manager\n",
    "0     Austin  Charlers\n",
    "1     Denver      Joel\n",
    "2  Mendocino     Brett\n",
    "\n",
    "The DataFrames have been printed in the IPython Shell. If you were to run the command combined = pd.merge(revenue, managers, on='city'), how many rows would combined have?\n",
    "\n",
    "INSTRUCTIONS\n",
    "\n",
    "Possible Answers\n",
    "0 rows.\n",
    "press 1\n",
    "2 rows.  Answer\n",
    "press 2\n",
    "3 rows.\n",
    "press 3\n",
    "4 rows.\n",
    "press 4\n",
    "\n",
    "**Remember, the default strategy for pd.merge() is an inner join. However, contrast here and other places where it gives full-outer-join like results although it is also called inner join. I must summarize this to distinguish**. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging on a specific column\n",
    "This exercise follows on the last one with the DataFrames revenue and managers for your company. You expect your company to grow and, eventually, to operate in cities with the same name on different states. As such, you decide that every branch should have a numerical branch identifier. Thus, you add a branch_id column to both DataFrames. Moreover, new cities have been added to both the revenue and managers DataFrames as well. pandas has been imported as pd and both DataFrames are available in your namespace.\n",
    "\n",
    "At present, there should be a 1-to-1 relationship between the city and branch_id fields. In that case, the result of a merge on the city columns ought to give you the same output as a merge on the branch_id columns. Do they? Can you spot an ambiguity in one of the DataFrames?\n",
    "\n",
    "INSTRUCTIONS\n",
    "\n",
    "Using pd.merge(), merge the DataFrames revenue and managers on the 'city' column of each. Store the result as merge_by_city.\n",
    "Print the DataFrame merge_by_city. This has been done for you.\n",
    "Merge the DataFrames revenue and managers on the 'branch_id' column of each. Store the result as merge_by_id.\n",
    "Print the DataFrame merge_by_id. This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "revenue\n",
      "   branch_id         city  revenue\n",
      "0         10       Austin      100\n",
      "1         20       Denver       83\n",
      "2         30  Springfield        4\n",
      "3         47    Mendocino      200\n",
      "managers\n",
      "   branch_id         city  manager\n",
      "0         10       Austin  Charles\n",
      "1         20       Denver     Joel\n",
      "2         47    Mendocino    Brett\n",
      "3         31  Springfield    Sally\n",
      "   branch_id_x         city  revenue  branch_id_y  manager\n",
      "0           10       Austin      100           10  Charles\n",
      "1           20       Denver       83           20     Joel\n",
      "2           30  Springfield        4           31    Sally\n",
      "3           47    Mendocino      200           47    Brett\n",
      "   branch_id     city_x  revenue     city_y  manager\n",
      "0         10     Austin      100     Austin  Charles\n",
      "1         20     Denver       83     Denver     Joel\n",
      "2         47  Mendocino      200  Mendocino    Brett\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "revenue = pd.DataFrame({'branch_id':[10,20,30,47],'city':['Austin','Denver','Springfield','Mendocino'],\n",
    "                       'revenue':[100,83,4,200]})\n",
    "print('revenue')\n",
    "print(revenue)\n",
    "\n",
    "managers = pd.DataFrame({'branch_id':[10,20,47,31],'city':['Austin','Denver','Mendocino','Springfield'],\n",
    "                       'manager':['Charles','Joel','Brett','Sally']})\n",
    "print('managers')\n",
    "print(managers)\n",
    "\n",
    "# Merge revenue with managers on 'city': merge_by_city\n",
    "merge_by_city = pd.merge(revenue, managers, on='city')\n",
    "\n",
    "# Print merge_by_city\n",
    "print(merge_by_city)\n",
    "\n",
    "# Merge revenue with managers on 'branch_id': merge_by_id\n",
    "merge_by_id = pd.merge(revenue, managers, on='branch_id')\n",
    "\n",
    "# Print merge_by_id\n",
    "print(merge_by_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pd.merge seems default on inner join. Note how it handles branch_id with branch_id_x/y in the first example, and how it handles city with city_x/y in the second example. Will this happen in SQL?**. \n",
    "\n",
    "**Also note**\n",
    "Notice that when you merge on 'city', the resulting DataFrame has a peculiar result: In row 2, the city Springfield has two different branch IDs. This is because there are actually two different cities named Springfield - one in the State of Illinois, and the other in Missouri. The revenue DataFrame has the one from Illinois, and the managers DataFrame has the one from Missouri. Consequently, when you merge on 'branch_id', both of these get dropped from the merged DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging on columns with non-matching labels\n",
    "You continue working with the revenue & managers DataFrames from before. This time, someone has changed the field name 'city' to 'branch' in the managers table. Now, when you attempt to merge DataFrames, an exception is thrown:\n",
    "\n",
    "pd.merge(revenue, managers, on='city')\n",
    "Traceback (most recent call last):\n",
    "    ... <text deleted> ...\n",
    "    pd.merge(revenue, managers, on='city')\n",
    "    ... <text deleted> ...\n",
    "KeyError: 'city'\n",
    "Given this, it will take a bit more work for you to join or merge on the city/branch name. You have to specify the left_on and right_on parameters in the call to pd.merge().\n",
    "\n",
    "As before, pandas has been pre-imported as pd and the revenue and managers DataFrames are in your namespace. They have been printed in the IPython Shell so you can examine the columns prior to merging.\n",
    "\n",
    "Are you able to merge better than in the last exercise? How should the rows with Springfield be handled?\n",
    "\n",
    "**This is actually the typical SQL case**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "revenue\n",
      "   branch_id         city  revenue state\n",
      "0         10       Austin      100    TX\n",
      "1         20       Denver       83    CO\n",
      "2         30  Springfield        4    IL\n",
      "3         47    Mendocino      200    CA\n",
      "managers\n",
      "        branch  branch_id  manager state\n",
      "0       Austin         10  Charles    TX\n",
      "1       Denver         20     Joel    CO\n",
      "2    Mendocino         47    Brett    CA\n",
      "3  Springfield         31    Sally    MO\n",
      "   branch_id_x         city  revenue state_x       branch  branch_id_y  \\\n",
      "0           10       Austin      100      TX       Austin           10   \n",
      "1           20       Denver       83      CO       Denver           20   \n",
      "2           30  Springfield        4      IL  Springfield           31   \n",
      "3           47    Mendocino      200      CA    Mendocino           47   \n",
      "\n",
      "   manager state_y  \n",
      "0  Charles      TX  \n",
      "1     Joel      CO  \n",
      "2    Sally      MO  \n",
      "3    Brett      CA  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "revenue = pd.DataFrame({'branch_id':[10,20,30,47],'city':['Austin','Denver','Springfield','Mendocino'],\n",
    "                       'revenue':[100,83,4,200], 'state':['TX','CO','IL','CA']})\n",
    "print('revenue')\n",
    "print(revenue)\n",
    "\n",
    "managers = pd.DataFrame({'branch':['Austin','Denver','Mendocino','Springfield'],'branch_id':[10,20,47,31],\n",
    "                       'manager':['Charles','Joel','Brett','Sally'], 'state':['TX','CO','CA','MO']})\n",
    "print('managers')\n",
    "print(managers)\n",
    "\n",
    "# Merge revenue & managers on 'city' & 'branch': combined\n",
    "combined = pd.merge(revenue, managers, left_on='city', right_on='branch')\n",
    "\n",
    "# Print combined\n",
    "print(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging on multiple columns\n",
    "Another strategy to disambiguate cities with identical names is to add information on the states in which the cities are located. To this end, you add a column called state to both DataFrames from the preceding exercises. Again, pandas has been pre-imported as pd and the revenue and managers DataFrames are in your namespace.\n",
    "\n",
    "Your goal in this exercise is to use pd.merge() to merge DataFrames using multiple columns (using 'branch_id', 'city', and 'state' in this case).\n",
    "\n",
    "Are you able to match all your company's branches correctly?\n",
    "\n",
    "INSTRUCTIONS\n",
    "\n",
    "Create a column called 'state' in the DataFrame revenue, consisting of the list ['TX','CO','IL','CA'].\n",
    "Create a column called 'state' in the DataFrame managers, consisting of the list ['TX','CO','CA','MO'].\n",
    "Merge the DataFrames revenue and managers using three columns :'branch_id', 'city', and 'state'. Pass them in as a list to the on paramater of pd.merge().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "revenue\n",
      "   branch_id         city  revenue state\n",
      "0         10       Austin      100    TX\n",
      "1         20       Denver       83    CO\n",
      "2         30  Springfield        4    IL\n",
      "3         47    Mendocino      200    CA\n",
      "managers\n",
      "   branch_id         city  manager state\n",
      "0         10       Austin  Charles    TX\n",
      "1         20       Denver     Joel    CO\n",
      "2         47    Mendocino    Brett    CA\n",
      "3         31  Springfield    Sally    MO\n",
      "   branch_id       city  revenue state  manager\n",
      "0         10     Austin      100    TX  Charles\n",
      "1         20     Denver       83    CO     Joel\n",
      "2         47  Mendocino      200    CA    Brett\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "revenue = pd.DataFrame({'branch_id':[10,20,30,47],'city':['Austin','Denver','Springfield','Mendocino'],\n",
    "                       'revenue':[100,83,4,200], 'state':['TX','CO','IL','CA']})\n",
    "print('revenue')\n",
    "print(revenue)\n",
    "\n",
    "managers = pd.DataFrame({'branch_id':[10,20,47,31],'city':['Austin','Denver','Mendocino','Springfield'],\n",
    "                       'manager':['Charles','Joel','Brett','Sally'], 'state':['TX','CO','CA','MO']})\n",
    "print('managers')\n",
    "print(managers)\n",
    "\n",
    "# Merge revenue & managers on 'branch_id', 'city', & 'state': combined\n",
    "combined = pd.merge(revenue, managers, on=['branch_id', 'city', 'state'])\n",
    "\n",
    "# Print combined\n",
    "print(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining by Index\n",
    "The DataFrames revenue and managers are displayed in the IPython Shell. Here, they are indexed by 'branch_id'.\n",
    "\n",
    "Choose the function call below that will join the DataFrames on their indexes and return 5 rows with index labels [10, 20, 30, 31, 47]. Explore each of them in the IPython Shell to get a better understanding of their functionality.\n",
    "\n",
    "                  city  revenue state\n",
    "branch_id                            \n",
    "10              Austin      100    TX\n",
    "20              Denver       83    CO\n",
    "30         Springfield        4    IL\n",
    "47           Mendocino      200    CA\n",
    "\n",
    "                branch   manager state\n",
    "branch_id                             \n",
    "10              Austin  Charlers    TX\n",
    "20              Denver      Joel    CO\n",
    "47           Mendocino     Brett    CA\n",
    "31         Springfield     Sally    MO\n",
    "\n",
    "\n",
    "INSTRUCTIONS\n",
    "\n",
    "Possible Answers\n",
    "pd.merge(revenue, managers, on='branch_id').\n",
    "press 1\n",
    "pd.merge(managers, revenue, how='left').\n",
    "press 2\n",
    "revenue.join(managers, lsuffix='_rev', rsuffix='_mng', how='outer'). Answer.\n",
    "press 3\n",
    "managers.join(revenue, lsuffix='_mgn', rsuffix='_rev', how='left').\n",
    "press 4\n",
    "\n",
    "**Remember, the DataFrame .join() method joins on the Index while the pd.merge() function can merge on arbitrary DataFrame columns.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a joining strategy\n",
    "Suppose you have two DataFrames: students (with columns 'StudentID', 'LastName', 'FirstName', and 'Major') and midterm_results (with columns 'StudentID', 'Q1', 'Q2', and 'Q3' for their scores on midterm questions).\n",
    "\n",
    "You want to combine the DataFrames into a single DataFrame grades, and be able to easily spot which students wrote the midterm and which didn't (their midterm question scores 'Q1', 'Q2', & 'Q3' should be filled with NaN values).\n",
    "\n",
    "You also want to drop rows from midterm_results in which the StudentID is not found in students.\n",
    "\n",
    "Which of the following strategies gives the desired result?\n",
    "\n",
    "INSTRUCTIONS\n",
    "\n",
    "Possible Answers\n",
    "A left join: grades = pd.merge(students, midterm_results, how='left'). Answer\n",
    "press 1\n",
    "A right join: grades = pd.merge(students, midterm_results, how='right').\n",
    "press 2\n",
    "An inner join: grades = pd.merge(students, midterm_results, how='inner').\n",
    "press 3\n",
    "An outer join: grades = pd.merge(students, midterm_results, how='outer').\n",
    "press 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left & right merging on multiple columns\n",
    "You now have, in addition to the revenue and managers DataFrames from prior exercises, a DataFrame sales that summarizes units sold from specific branches (identified by city and state but not branch_id).\n",
    "\n",
    "Once again, the managers DataFrame uses the label branch in place of city as in the other two DataFrames. Your task here is to employ left and right merges to preserve data and identify where data is missing.\n",
    "\n",
    "By merging revenue and sales with a right merge, you can identify the missing revenue values. Here, you don't need to specify left_on or right_on because the columns to merge on have matching labels.\n",
    "\n",
    "By merging sales and managers with a left merge, you can identify the missing manager. Here, the columns to merge on have conflicting labels, so you must specify left_on and right_on. In both cases, you're looking to figure out how to connect the fields in rows containing Springfield.\n",
    "\n",
    "pandas has been imported as pd and the three DataFrames revenue, managers, and sales have been pre-loaded. They have been printed for you to explore in the IPython Shell.\n",
    "\n",
    "INSTRUCTIONS\n",
    "\n",
    "Execute a right merge using pd.merge() with revenue and sales to yield a new DataFrame revenue_and_sales.\n",
    "Use how='right' and on=['city', 'state'].\n",
    "Print the new DataFrame revenue_and_sales. This has been done for you.\n",
    "Execute a left merge with sales and managers to yield a new DataFrame sales_and_managers.\n",
    "Use how='left', left_on=['city', 'state'], and right_on=['branch', 'state'].\n",
    "Print the new DataFrame sales_and_managers. This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "revenue\n",
      "   branch_id         city  revenue state\n",
      "0         10       Austin      100    TX\n",
      "1         20       Denver       83    CO\n",
      "2         30  Springfield        4    IL\n",
      "3         47    Mendocino      200    CA\n",
      "managers\n",
      "        branch  branch_id  manager state\n",
      "0       Austin         10  Charles    TX\n",
      "1       Denver         20     Joel    CO\n",
      "2    Mendocino         47    Brett    CA\n",
      "3  Springfield         31    Sally    MO\n",
      "sales\n",
      "          city state  units\n",
      "0    Mendocino    CA      1\n",
      "1       Denver    CO      4\n",
      "2       Austin    TX      2\n",
      "3  Springfield    MO      5\n",
      "4  Springfield    IL      1\n",
      "   branch_id         city  revenue state  units\n",
      "0       10.0       Austin    100.0    TX      2\n",
      "1       20.0       Denver     83.0    CO      4\n",
      "2       30.0  Springfield      4.0    IL      1\n",
      "3       47.0    Mendocino    200.0    CA      1\n",
      "4        NaN  Springfield      NaN    MO      5\n",
      "          city state  units       branch  branch_id  manager\n",
      "0    Mendocino    CA      1    Mendocino       47.0    Brett\n",
      "1       Denver    CO      4       Denver       20.0     Joel\n",
      "2       Austin    TX      2       Austin       10.0  Charles\n",
      "3  Springfield    MO      5  Springfield       31.0    Sally\n",
      "4  Springfield    IL      1          NaN        NaN      NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "revenue = pd.DataFrame({'branch_id':[10,20,30,47],'city':['Austin','Denver','Springfield','Mendocino'],\n",
    "                       'revenue':[100,83,4,200], 'state':['TX','CO','IL','CA']})\n",
    "print('revenue')\n",
    "print(revenue)\n",
    "\n",
    "managers = pd.DataFrame({'branch':['Austin','Denver','Mendocino','Springfield'],'branch_id':[10,20,47,31],\n",
    "                       'manager':['Charles','Joel','Brett','Sally'], 'state':['TX','CO','CA','MO']})\n",
    "print('managers')\n",
    "print(managers)\n",
    "\n",
    "sales = pd.DataFrame({'city':['Mendocino','Denver','Austin','Springfield','Springfield'],\n",
    "                      'state':['CA','CO','TX','MO','IL'], 'units':[1,4,2,5,1]})\n",
    "print('sales')\n",
    "print(sales )\n",
    "\n",
    "#my code above \n",
    "\n",
    "# Merge revenue and sales: revenue_and_sales\n",
    "revenue_and_sales = pd.merge(revenue, sales, on=['city','state'], how='right')\n",
    "\n",
    "# Print revenue_and_sales\n",
    "print(revenue_and_sales)\n",
    "\n",
    "# Merge sales and managers: sales_and_managers\n",
    "sales_and_managers = pd.merge(sales, managers, left_on=['city','state'], right_on=['branch','state'], how='left')\n",
    "\n",
    "# Print sales_and_managers\n",
    "print(sales_and_managers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging DataFrames with outer join\n",
    "This exercise picks up where the previous one left off. The DataFrames revenue, managers, and sales are pre-loaded into your namespace (and, of course, pandas is imported as pd). Moreover, the merged DataFrames revenue_and_sales and sales_and_managers have been pre-computed exactly as you did in the previous exercise.\n",
    "\n",
    "The merged DataFrames contain enough information to construct a DataFrame with 5 rows with all known information correctly aligned and each branch listed only once. You will try to merge the merged DataFrames on all matching keys (which computes an inner join by default). You can compare the result to an outer join and also to an outer join with restricted subset of columns as keys.\n",
    "\n",
    "INSTRUCTIONS\n",
    "\n",
    "Merge sales_and_managers with revenue_and_sales. Store the result as merge_default.\n",
    "Print merge_default. This has been done for you.\n",
    "Merge sales_and_managers with revenue_and_sales using how='outer'. Store the result as merge_outer.\n",
    "Print merge_outer. This has been done for you.\n",
    "Merge sales_and_managers with revenue_and_sales only on ['city','state'] using an outer join. Store the result as merge_outer_on and hit 'Submit Answer' to see what the merged DataFrames look like!\n",
    "\n",
    "**It seems: outer here means full outer join but not include other outer joins such as left and right outer joins. default is inner join**\n",
    "\n",
    "**Also note, all outer joins, including full outer join can be used with ON conditions. Usually this will reduce the number of rows**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        city state  units     branch  branch_id  manager  revenue\n",
      "0  Mendocino    CA      1  Mendocino       47.0    Brett    200.0\n",
      "1     Denver    CO      4     Denver       20.0     Joel     83.0\n",
      "2     Austin    TX      2     Austin       10.0  Charles    100.0\n",
      "          city state  units       branch  branch_id  manager  revenue\n",
      "0    Mendocino    CA      1    Mendocino       47.0    Brett    200.0\n",
      "1       Denver    CO      4       Denver       20.0     Joel     83.0\n",
      "2       Austin    TX      2       Austin       10.0  Charles    100.0\n",
      "3  Springfield    MO      5  Springfield       31.0    Sally      NaN\n",
      "4  Springfield    IL      1          NaN        NaN      NaN      NaN\n",
      "5  Springfield    IL      1          NaN       30.0      NaN      4.0\n",
      "6  Springfield    MO      5          NaN        NaN      NaN      NaN\n",
      "          city state  units_x       branch  branch_id_x  manager  branch_id_y  \\\n",
      "0    Mendocino    CA        1    Mendocino         47.0    Brett         47.0   \n",
      "1       Denver    CO        4       Denver         20.0     Joel         20.0   \n",
      "2       Austin    TX        2       Austin         10.0  Charles         10.0   \n",
      "3  Springfield    MO        5  Springfield         31.0    Sally          NaN   \n",
      "4  Springfield    IL        1          NaN          NaN      NaN         30.0   \n",
      "\n",
      "   revenue  units_y  \n",
      "0    200.0        1  \n",
      "1     83.0        4  \n",
      "2    100.0        2  \n",
      "3      NaN        5  \n",
      "4      4.0        1  \n"
     ]
    }
   ],
   "source": [
    "#Need data from previous cell \n",
    "# Perform the first merge: merge_default\n",
    "merge_default = pd.merge(sales_and_managers, revenue_and_sales)\n",
    "\n",
    "# Print merge_default\n",
    "print(merge_default)\n",
    "\n",
    "# Perform the second merge: merge_outer\n",
    "merge_outer = pd.merge(sales_and_managers, revenue_and_sales, how='outer')\n",
    "\n",
    "# Print merge_outer\n",
    "print(merge_outer)\n",
    "\n",
    "# Perform the third merge: merge_outer_on\n",
    "merge_outer_on = pd.merge(sales_and_managers, revenue_and_sales, on=['city','state'], how='outer')\n",
    "\n",
    "# Print merge_outer_on\n",
    "print(merge_outer_on)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using merge_ordered()\n",
    "This exercise uses pre-loaded DataFrames austin and houston that contain weather data from the cities Austin and Houston respectively. They have been printed in the IPython Shell for you to examine.\n",
    "\n",
    "Weather conditions were recorded on separate days and you need to merge these two DataFrames together such that the dates are ordered. To do this, you'll use pd.merge_ordered(). After you're done, note the order of the rows before and after merging.\n",
    "\n",
    "INSTRUCTIONS\n",
    "\n",
    "Perform an ordered merge on austin and houston using pd.merge_ordered(). Store the result as tx_weather.\n",
    "Print tx_weather. You should notice that the rows are sorted by the date but it is not possible to tell which observation came from which city.\n",
    "Perform another ordered merge on austin and houston.\n",
    "This time, specify the keyword arguments on='date' and suffixes=['_aus','_hus'] so that the rows can be distinguished. Store the result as tx_weather_suff.\n",
    "Print tx_weather_suff to examine its contents. This has been done for you.\n",
    "Perform a third ordered merge on austin and houston.\n",
    "This time, in addition to the on and suffixes parameters, specify the keyword argument fill_method='ffill' to use forward-filling to replace NaN entries with the most recent non-null entry, and hit 'Submit Answer' to examine the contents of the merged DataFrames!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        date ratings\n",
      "0 2016-01-01  Cloudy\n",
      "1 2016-01-04   Rainy\n",
      "2 2016-01-17   Sunny\n",
      "3 2016-02-08  Cloudy\n",
      "4 2016-03-01   Sunny\n",
      "    date_aus ratings   date_hus\n",
      "0 2016-01-01  Cloudy 2016-01-01\n",
      "1 2016-02-08  Cloudy 2016-01-01\n",
      "2        NaT   Rainy 2016-01-04\n",
      "3 2016-01-17   Sunny 2016-03-01\n",
      "        date ratings_aus ratings_hus\n",
      "0 2016-01-01      Cloudy      Cloudy\n",
      "1 2016-01-04      Cloudy       Rainy\n",
      "2 2016-01-17       Sunny       Rainy\n",
      "3 2016-02-08      Cloudy       Rainy\n",
      "4 2016-03-01      Cloudy       Sunny\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "austin = pd.DataFrame({'date':['2016-01-01','2016-02-08','2016-01-17'], 'ratings':['Cloudy','Cloudy','Sunny']})\n",
    "austin['date'] = pd.to_datetime(austin['date'])\n",
    "houston = pd.DataFrame({'date':['2016-01-04','2016-01-01','2016-03-01'], 'ratings':['Rainy','Cloudy','Sunny']})\n",
    "houston['date'] = pd.to_datetime(houston['date'])\n",
    "\n",
    "#my code above\n",
    "\n",
    "# Perform the first ordered merge: tx_weather\n",
    "tx_weather = pd.merge_ordered(austin, houston)\n",
    "\n",
    "# Print tx_weather\n",
    "print(tx_weather)\n",
    "\n",
    "# Perform the second ordered merge: tx_weather_suff\n",
    "tx_weather_suff = pd.merge_ordered(austin, houston, on='date', suffixes=['_aus', '_hus'])\n",
    "\n",
    "# Print tx_weather_suff\n",
    "print(tx_weather_suff)\n",
    "\n",
    "# Perform the third ordered merge: tx_weather_ffill\n",
    "tx_weather_ffill = pd.merge_ordered(austin, houston, on='date', fill_method='ffill', suffixes=['_aus', '_hus'])\n",
    "\n",
    "# Print tx_weather_ffill\n",
    "print(tx_weather_ffill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can also order by other columns**  \n",
    "**However, note the merger_ordered() here has nothing to do with the order by in sql. In pandas, I think sort is related to order by**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using merge_asof()\n",
    "Similar to pd.merge_ordered(), the pd.merge_asof() function will also merge values in order using the on column, but for each row in the left DataFrame, only rows from the right DataFrame whose 'on' column values are less than the left value will be kept.\n",
    "\n",
    "**This function can be used to align disparate datetime frequencies without having to first resample.**\n",
    "\n",
    "Here, you'll merge monthly oil prices (US dollars) into a full automobile fuel efficiency dataset. The oil and automobile DataFrames have been pre-loaded as oil and auto. The first 5 rows of each have been printed in the IPython Shell for you to explore.\n",
    "\n",
    "These datasets will align such that the first price of the year will be broadcast into the rows of the automobiles DataFrame. This is considered correct since by the start of any given year, most automobiles for that year will have already been manufactured.\n",
    "\n",
    "You'll then inspect the merged DataFrame, resample by year and compute the mean 'Price' and 'mpg'. You should be able to see a trend in these two columns, that you can confirm by computing the Pearson correlation between resampled 'Price' and 'mpg'.\n",
    "\n",
    "INSTRUCTIONS\n",
    "\n",
    "Merge auto and oil using pd.merge_asof() with left_on='yr' and right_on='Date'. Store the result as merged.\n",
    "Print the tail of merged. This has been done for you.\n",
    "Resample merged using 'A' (annual frequency), and on='Date'. Select [['mpg','Price']] and aggregate the mean. Store the result as yearly.\n",
    "Hit Submit Answer to examine the contents of yearly and yearly.corr(), which shows the Pearson correlation between the resampled 'Price' and 'mpg'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    mpg  cyl  displ   hp  weight  accel         yr origin  \\\n",
      "0  18.0    8  307.0  130    3504   12.0 1970-01-01     US   \n",
      "1  15.0    8  350.0  165    3693   11.5 1970-01-01     US   \n",
      "2  18.0    8  318.0  150    3436   11.0 1970-01-01     US   \n",
      "3  16.0    8  304.0  150    3433   12.0 1970-01-01     US   \n",
      "4  17.0    8  302.0  140    3449   10.5 1970-01-01     US   \n",
      "\n",
      "                        name  \n",
      "0  chevrolet chevelle malibu  \n",
      "1          buick skylark 320  \n",
      "2         plymouth satellite  \n",
      "3              amc rebel sst  \n",
      "4                ford torino  \n",
      "-------------------\n",
      "        Date  Price\n",
      "0 1970-01-01   3.35\n",
      "1 1970-02-01   3.35\n",
      "2 1970-03-01   3.35\n",
      "3 1970-04-01   3.35\n",
      "4 1970-05-01   3.35\n",
      "-------------------\n",
      "      mpg  cyl  displ  hp  weight  accel         yr  origin             name  \\\n",
      "387  27.0    4  140.0  86    2790   15.6 1982-01-01      US  ford mustang gl   \n",
      "388  44.0    4   97.0  52    2130   24.6 1982-01-01  Europe        vw pickup   \n",
      "389  32.0    4  135.0  84    2295   11.6 1982-01-01      US    dodge rampage   \n",
      "390  28.0    4  120.0  79    2625   18.6 1982-01-01      US      ford ranger   \n",
      "391  31.0    4  119.0  82    2720   19.4 1982-01-01      US       chevy s-10   \n",
      "\n",
      "          Date  Price  \n",
      "387 1982-01-01  33.85  \n",
      "388 1982-01-01  33.85  \n",
      "389 1982-01-01  33.85  \n",
      "390 1982-01-01  33.85  \n",
      "391 1982-01-01  33.85  \n",
      "-------------------\n",
      "                  mpg  Price\n",
      "Date                        \n",
      "1970-12-31  17.689655   3.35\n",
      "1971-12-31  21.111111   3.56\n",
      "1972-12-31  18.714286   3.56\n",
      "1973-12-31  17.100000   3.56\n",
      "1974-12-31  22.769231  10.11\n",
      "1975-12-31  20.266667  11.16\n",
      "1976-12-31  21.573529  11.16\n",
      "1977-12-31  23.375000  13.90\n",
      "1978-12-31  24.061111  14.85\n",
      "1979-12-31  25.093103  14.85\n",
      "1980-12-31  33.803704  32.50\n",
      "1981-12-31  30.185714  38.00\n",
      "1982-12-31  32.000000  33.85\n",
      "-------------------\n",
      "            mpg     Price\n",
      "mpg    1.000000  0.948677\n",
      "Price  0.948677  1.000000\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "auto = pd.read_csv(\"automobiles.csv\")\n",
    "auto['yr'] = pd.to_datetime(auto['yr'])\n",
    "oil = pd.read_csv(\"oil_price.csv\")\n",
    "oil['Date'] = pd.to_datetime(oil['Date'])\n",
    "print(auto.head())\n",
    "print('-------------------')\n",
    "print(oil.head())\n",
    "print('-------------------')\n",
    "#above my code\n",
    "\n",
    "# Merge auto and oil: merged\n",
    "merged = pd.merge_asof(auto, oil, left_on='yr', right_on='Date')\n",
    "\n",
    "# Print the tail of merged\n",
    "print(merged.tail())\n",
    "print('-------------------')\n",
    "\n",
    "# Resample merged: yearly\n",
    "yearly = merged.resample('A',on='Date')[['mpg','Price']].mean()\n",
    "\n",
    "# Print yearly\n",
    "print(yearly)\n",
    "print('-------------------')\n",
    "\n",
    "# Print yearly.corr()\n",
    "print(yearly.corr())\n",
    "print('-------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study - Summer Olympics\n",
    "\n",
    "To cement your new skills, you'll apply them by working on an in-depth study involving Olympic medal data. The analysis involves integrating your multi-DataFrame skills from this course and also skills you've gained in previous pandas courses. This is a rich dataset that will allow you to fully leverage your pandas data manipulation skills. Enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Olympic edition DataFrame\n",
    "In this chapter, you'll be using The Guardian's Olympic medal dataset.\n",
    "\n",
    "Your first task here is to prepare a DataFrame editions from a tab-separated values (TSV) file.\n",
    "\n",
    "Initially, editions has 26 rows (one for each Olympic edition, i.e., a year in which the Olympics was held) and 7 columns: 'Edition', 'Bronze', 'Gold', 'Silver', 'Grand Total', 'City', and 'Country'.\n",
    "\n",
    "For the analysis that follows, you won't need the overall medal counts, so you want to keep only the useful columns from editions: 'Edition', 'Grand Total', City, and Country.\n",
    "\n",
    "INSTRUCTIONS\n",
    "\n",
    "Read file_path into a DataFrame called editions. The identifier file_path has been pre-defined with the filename 'Summer Olympic medallists 1896 to 2008 - EDITIONS.tsv'. You'll have to use the option sep='\\t' because the file uses tabs to delimit fields (pd.read_csv() expects commas by default).\n",
    "Select only the columns 'Edition', 'Grand Total', 'City', and 'Country' from editions.\n",
    "Print the final DataFrame editions in entirety (there are only 26 rows). This has been done for you, so hit 'Submit Answer' to see the result!\n",
    "\n",
    "**Note .tsv is just tab separated value file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Edition  Grand Total         City                     Country\n",
      "0      1896          151       Athens                      Greece\n",
      "1      1900          512        Paris                      France\n",
      "2      1904          470    St. Louis               United States\n",
      "3      1908          804       London              United Kingdom\n",
      "4      1912          885    Stockholm                      Sweden\n",
      "5      1920         1298      Antwerp                     Belgium\n",
      "6      1924          884        Paris                      France\n",
      "7      1928          710    Amsterdam                 Netherlands\n",
      "8      1932          615  Los Angeles               United States\n",
      "9      1936          875       Berlin                     Germany\n",
      "10     1948          814       London              United Kingdom\n",
      "11     1952          889     Helsinki                     Finland\n",
      "12     1956          885    Melbourne                   Australia\n",
      "13     1960          882         Rome                       Italy\n",
      "14     1964         1010        Tokyo                       Japan\n",
      "15     1968         1031  Mexico City                      Mexico\n",
      "16     1972         1185       Munich  West Germany (now Germany)\n",
      "17     1976         1305     Montreal                      Canada\n",
      "18     1980         1387       Moscow       U.S.S.R. (now Russia)\n",
      "19     1984         1459  Los Angeles               United States\n",
      "20     1988         1546        Seoul                 South Korea\n",
      "21     1992         1705    Barcelona                       Spain\n",
      "22     1996         1859      Atlanta               United States\n",
      "23     2000         2015       Sydney                   Australia\n",
      "24     2004         1998       Athens                      Greece\n",
      "25     2008         2042      Beijing                       China\n"
     ]
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load DataFrame from file_path: editions\n",
    "editions = pd.read_csv('Summer Olympic medalists 1896 to 2008 - EDITIONS.tsv', sep='\\t')\n",
    "\n",
    "# Extract the relevant columns: editions\n",
    "editions = editions[['Edition', 'Grand Total', 'City', 'Country']]\n",
    "\n",
    "# Print editions DataFrame\n",
    "print(editions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading IOC codes DataFrame\n",
    "Your task here is to prepare a DataFrame ioc_codes from a comma-separated values (CSV) file.\n",
    "\n",
    "Initially, ioc_codes has 200 rows (one for each country) and 3 columns: 'Country', 'NOC', & 'ISO code'.\n",
    "\n",
    "For the analysis that follows, you want to keep only the useful columns from ioc_codes: 'Country' and 'NOC' (the column 'NOC' contains three-letter codes representing each country).\n",
    "\n",
    "INSTRUCTIONS\n",
    "\n",
    "Read file_path into a DataFrame called ioc_codes. The identifier file_path has been pre-defined with the filename 'Summer Olympic medallists 1896 to 2008 - IOC COUNTRY CODES.csv'.\n",
    "Select only the columns 'Country' and 'NOC' from ioc_codes.\n",
    "Print the leading 5 and trailing 5 rows of the DataFrame ioc_codes (there are 200 rows in total). This has been done for you, so hit 'Submit Answer' to see the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Country  NOC\n",
      "0      Afghanistan  AFG\n",
      "1          Albania  ALB\n",
      "2          Algeria  ALG\n",
      "3  American Samoa*  ASA\n",
      "4          Andorra  AND\n",
      "             Country  NOC\n",
      "196          Vietnam  VIE\n",
      "197  Virgin Islands*  ISV\n",
      "198            Yemen  YEM\n",
      "199           Zambia  ZAM\n",
      "200         Zimbabwe  ZIM\n"
     ]
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load DataFrame from file_path: ioc_codes\n",
    "ioc_codes = pd.read_csv('Summer Olympic medalists 1896 to 2008 - IOC COUNTRY CODES.csv')\n",
    "\n",
    "# Extract the relevant columns: ioc_codes\n",
    "ioc_codes = ioc_codes[['Country', 'NOC']]\n",
    "\n",
    "# Print first and last 5 rows of ioc_codes\n",
    "print(ioc_codes.head())\n",
    "print(ioc_codes.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building medals DataFrame\n",
    "Here, you'll start with the DataFrame editions from the previous exercise.\n",
    "\n",
    "You have a sequence of files summer_1896.csv, summer_1900.csv, ..., summer_2008.csv, one for each Olympic edition (year).\n",
    "\n",
    "You will build up a dictionary medals_dict with the Olympic editions (years) as keys and DataFrames as values.\n",
    "\n",
    "The dictionary is built up inside a loop over the year of each Olympic edition (from the Index of editions).\n",
    "\n",
    "Once the dictionary of DataFrames is built up, you will combine the DataFrames using pd.concat().\n",
    "\n",
    "INSTRUCTIONS\n",
    "\n",
    "Within the for loop:\n",
    "Create the file path. This has been done for you.\n",
    "Read file_path into a DataFrame. Assign the result to the year key of medals_dict.\n",
    "Select only the columns 'Athlete', 'NOC', and 'Medal' from medals_dict[year].\n",
    "Create a new column called 'Edition' in the DataFrame medals_dict[year] whose entries are all year.\n",
    "Concatenate the dictionary of DataFrames medals_dict into a DataFame called medals. Specify the keyword argument ignore_index=True to prevent repeated integer indices.\n",
    "Print the first and last 5 rows of medals. This has been done for you, so hit 'Submit Answer' to see the result!\n",
    "\n",
    "**Note for .tsv file, I need sep = '\\t' option**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Athlete  NOC   Medal  Edition\n",
      "0       HAJOS, Alfred  HUN    Gold     1896\n",
      "1    HERSCHMANN, Otto  AUT  Silver     1896\n",
      "2   DRIVAS, Dimitrios  GRE  Bronze     1896\n",
      "3  MALOKINIS, Ioannis  GRE    Gold     1896\n",
      "4  CHASAPIS, Spiridon  GRE  Silver     1896\n",
      "                    Athlete  NOC   Medal  Edition\n",
      "29211        ENGLICH, Mirko  GER  Silver     2008\n",
      "29212  MIZGAITIS, Mindaugas  LTU  Bronze     2008\n",
      "29213       PATRIKEEV, Yuri  ARM  Bronze     2008\n",
      "29214         LOPEZ, Mijain  CUB    Gold     2008\n",
      "29215        BAROEV, Khasan  RUS  Silver     2008\n"
     ]
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "#****# Need run the following code to create files if not existed. \n",
    "\n",
    "# editions = pd.read_csv('Summer Olympic medalists 1896 to 2008 - EDITIONS.tsv', sep='\\t')\n",
    "# # Extract the relevant columns: editions\n",
    "# editions = editions[['Edition', 'Grand Total', 'City', 'Country']]\n",
    "\n",
    "# df = pd.read_csv(\"Summer Olympic medalists 1896 to 2008.tsv\", sep='\\t')\n",
    "\n",
    "# for year in editions['Edition']:\n",
    "#     df_temp = df[df['Edition']== year]\n",
    "#     file_path = 'summer_{:d}.csv'.format(year)\n",
    "#     df_temp.to_csv(file_path)\n",
    "\n",
    "#my code above\n",
    "\n",
    "\n",
    "# Create empty dictionary: medals_dict\n",
    "medals_dict = {}\n",
    "\n",
    "for year in editions['Edition']:\n",
    "\n",
    "    # Create the file path: file_path\n",
    "    file_path = 'summer_{:d}.csv'.format(year)\n",
    "    \n",
    "    # Load file_path into a DataFrame: medals_dict[year]\n",
    "    medals_dict[year] = pd.read_csv(file_path)\n",
    "    \n",
    "    # Extract relevant columns: medals_dict[year]\n",
    "    medals_dict[year] = medals_dict[year][['Athlete', 'NOC', 'Medal']]\n",
    "    \n",
    "    # Assign year to column 'Edition' of medals_dict\n",
    "    medals_dict[year]['Edition'] = year\n",
    "    \n",
    "# Concatenate medals_dict: medals\n",
    "medals = pd.concat(medals_dict, ignore_index=True)\n",
    "\n",
    "# Print first and last 5 rows of medals\n",
    "print(medals.head())\n",
    "print(medals.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting medals by country/edition in a pivot table\n",
    "Here, you'll start with the concatenated DataFrame medals from the previous exercise.\n",
    "\n",
    "You can construct a pivot table to see the number of medals each country won in each year. The result is a new DataFrame with the Olympic edition on the Index and with 138 country NOC codes as columns. If you want a refresher on pivot tables, it may be useful to refer back to the relevant exercises in Manipulating DataFrames with pandas.\n",
    "\n",
    "INSTRUCTIONS\n",
    "\n",
    "Construct a pivot table from the DataFrame medals, aggregating by count (by specifying the aggfunc parameter). Use 'Edition' as the index, 'Athlete' for the values, and 'NOC' for the columns.\n",
    "Print the first & last 5 rows of medal_counts. This has been done for you, so hit 'Submit Answer' to see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOC      AFG  AHO  ALG   ANZ  ARG  ARM  AUS   AUT  AZE  BAH  ...   URS  URU  \\\n",
      "Edition                                                      ...              \n",
      "1896     NaN  NaN  NaN   NaN  NaN  NaN  2.0   5.0  NaN  NaN  ...   NaN  NaN   \n",
      "1900     NaN  NaN  NaN   NaN  NaN  NaN  5.0   6.0  NaN  NaN  ...   NaN  NaN   \n",
      "1904     NaN  NaN  NaN   NaN  NaN  NaN  NaN   1.0  NaN  NaN  ...   NaN  NaN   \n",
      "1908     NaN  NaN  NaN  19.0  NaN  NaN  NaN   1.0  NaN  NaN  ...   NaN  NaN   \n",
      "1912     NaN  NaN  NaN  10.0  NaN  NaN  NaN  14.0  NaN  NaN  ...   NaN  NaN   \n",
      "\n",
      "NOC        USA  UZB  VEN  VIE  YUG  ZAM  ZIM   ZZX  \n",
      "Edition                                             \n",
      "1896      20.0  NaN  NaN  NaN  NaN  NaN  NaN   6.0  \n",
      "1900      55.0  NaN  NaN  NaN  NaN  NaN  NaN  34.0  \n",
      "1904     394.0  NaN  NaN  NaN  NaN  NaN  NaN   8.0  \n",
      "1908      63.0  NaN  NaN  NaN  NaN  NaN  NaN   NaN  \n",
      "1912     101.0  NaN  NaN  NaN  NaN  NaN  NaN   NaN  \n",
      "\n",
      "[5 rows x 138 columns]\n",
      "NOC      AFG  AHO  ALG  ANZ   ARG  ARM    AUS  AUT  AZE  BAH ...   URS  URU  \\\n",
      "Edition                                                      ...              \n",
      "1992     NaN  NaN  2.0  NaN   2.0  NaN   57.0  6.0  NaN  1.0 ...   NaN  NaN   \n",
      "1996     NaN  NaN  3.0  NaN  20.0  2.0  132.0  3.0  1.0  5.0 ...   NaN  NaN   \n",
      "2000     NaN  NaN  5.0  NaN  20.0  1.0  183.0  4.0  3.0  6.0 ...   NaN  1.0   \n",
      "2004     NaN  NaN  NaN  NaN  47.0  NaN  157.0  8.0  5.0  2.0 ...   NaN  NaN   \n",
      "2008     1.0  NaN  2.0  NaN  51.0  6.0  149.0  3.0  7.0  5.0 ...   NaN  NaN   \n",
      "\n",
      "NOC        USA  UZB  VEN  VIE   YUG  ZAM  ZIM  ZZX  \n",
      "Edition                                             \n",
      "1992     224.0  NaN  NaN  NaN   NaN  NaN  NaN  NaN  \n",
      "1996     260.0  2.0  NaN  NaN  26.0  1.0  NaN  NaN  \n",
      "2000     248.0  4.0  NaN  1.0  26.0  NaN  NaN  NaN  \n",
      "2004     264.0  5.0  2.0  NaN   NaN  NaN  3.0  NaN  \n",
      "2008     315.0  6.0  1.0  1.0   NaN  NaN  4.0  NaN  \n",
      "\n",
      "[5 rows x 138 columns]\n"
     ]
    }
   ],
   "source": [
    "# Construct the pivot_table: medal_counts\n",
    "medal_counts = medals.pivot_table(index='Edition', values='Athlete', columns='NOC', aggfunc='count')\n",
    "\n",
    "# Print the first & last 5 rows of medal_counts\n",
    "print(medal_counts.head())\n",
    "print(medal_counts.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing fraction of medals per Olympic edition\n",
    "In this exercise, you'll start with the DataFrames editions, medals, & medal_counts from prior exercises.\n",
    "\n",
    "You can extract a Series with the total number of medals awarded in each Olympic edition.\n",
    "\n",
    "The DataFrame medal_counts can be divided row-wise by the total number of medals awarded each edition; the method .divide() performs the broadcast as you require.\n",
    "\n",
    "This gives you a normalized indication of each country's performance in each edition.\n",
    "\n",
    "INSTRUCTIONS\n",
    "\n",
    "Set the index of the DataFrame editions to be 'Edition' (using the method .set_index()). Save the result as totals.\n",
    "Extract the 'Grand Total' column from totals and assign the result back to totals.\n",
    "Divide the DataFrame medal_counts by totals along each row. You will have to use the .divide() method with the option axis='rows'. Assign the result to fractions.\n",
    "Print first & last 5 rows of the DataFrame fractions. This has been done for you, so hit 'Submit Answer' to see the results!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOC      AFG  AHO  ALG       ANZ  ARG  ARM       AUS       AUT  AZE  BAH  \\\n",
      "Edition                                                                    \n",
      "1896     NaN  NaN  NaN       NaN  NaN  NaN  0.013245  0.033113  NaN  NaN   \n",
      "1900     NaN  NaN  NaN       NaN  NaN  NaN  0.009766  0.011719  NaN  NaN   \n",
      "1904     NaN  NaN  NaN       NaN  NaN  NaN       NaN  0.002128  NaN  NaN   \n",
      "1908     NaN  NaN  NaN  0.023632  NaN  NaN       NaN  0.001244  NaN  NaN   \n",
      "1912     NaN  NaN  NaN  0.011299  NaN  NaN       NaN  0.015819  NaN  NaN   \n",
      "\n",
      "NOC        ...     URS  URU       USA  UZB  VEN  VIE  YUG  ZAM  ZIM       ZZX  \n",
      "Edition    ...                                                                 \n",
      "1896       ...     NaN  NaN  0.132450  NaN  NaN  NaN  NaN  NaN  NaN  0.039735  \n",
      "1900       ...     NaN  NaN  0.107422  NaN  NaN  NaN  NaN  NaN  NaN  0.066406  \n",
      "1904       ...     NaN  NaN  0.838298  NaN  NaN  NaN  NaN  NaN  NaN  0.017021  \n",
      "1908       ...     NaN  NaN  0.078358  NaN  NaN  NaN  NaN  NaN  NaN       NaN  \n",
      "1912       ...     NaN  NaN  0.114124  NaN  NaN  NaN  NaN  NaN  NaN       NaN  \n",
      "\n",
      "[5 rows x 138 columns]\n",
      "NOC          AFG  AHO       ALG  ANZ       ARG       ARM       AUS       AUT  \\\n",
      "Edition                                                                        \n",
      "1992         NaN  NaN  0.001173  NaN  0.001173       NaN  0.033431  0.003519   \n",
      "1996         NaN  NaN  0.001614  NaN  0.010758  0.001076  0.071006  0.001614   \n",
      "2000         NaN  NaN  0.002481  NaN  0.009926  0.000496  0.090819  0.001985   \n",
      "2004         NaN  NaN       NaN  NaN  0.023524       NaN  0.078579  0.004004   \n",
      "2008     0.00049  NaN  0.000979  NaN  0.024976  0.002938  0.072968  0.001469   \n",
      "\n",
      "NOC           AZE       BAH ...   URS       URU       USA       UZB       VEN  \\\n",
      "Edition                     ...                                                 \n",
      "1992          NaN  0.000587 ...   NaN       NaN  0.131378       NaN       NaN   \n",
      "1996     0.000538  0.002690 ...   NaN       NaN  0.139860  0.001076       NaN   \n",
      "2000     0.001489  0.002978 ...   NaN  0.000496  0.123077  0.001985       NaN   \n",
      "2004     0.002503  0.001001 ...   NaN       NaN  0.132132  0.002503  0.001001   \n",
      "2008     0.003428  0.002449 ...   NaN       NaN  0.154261  0.002938  0.000490   \n",
      "\n",
      "NOC           VIE       YUG       ZAM       ZIM  ZZX  \n",
      "Edition                                               \n",
      "1992          NaN       NaN       NaN       NaN  NaN  \n",
      "1996          NaN  0.013986  0.000538       NaN  NaN  \n",
      "2000     0.000496  0.012903       NaN       NaN  NaN  \n",
      "2004          NaN       NaN       NaN  0.001502  NaN  \n",
      "2008     0.000490       NaN       NaN  0.001959  NaN  \n",
      "\n",
      "[5 rows x 138 columns]\n"
     ]
    }
   ],
   "source": [
    "# Set Index of editions: totals\n",
    "totals = editions.set_index('Edition')\n",
    "\n",
    "# Reassign totals['Grand Total']: totals\n",
    "totals = totals['Grand Total']\n",
    "\n",
    "# Divide medal_counts by totals: fractions\n",
    "fractions = medal_counts.divide(totals, axis='rows')\n",
    "\n",
    "# Print first & last 5 rows of fractions\n",
    "print(fractions.head())\n",
    "print(fractions.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We have used .multiply() before and here we have .divide()**\n",
    "\n",
    "### Computing percentage change in fraction of medals won\n",
    "Here, you'll start with the DataFrames editions, medals, medal_counts, & fractions from prior exercises.\n",
    "\n",
    "To see if there is a host country advantage, you first want to see how the fraction of medals won changes from edition to edition.\n",
    "\n",
    "The expanding mean provides a way to see this down each column. It is the value of the mean with all the data available up to that point in time. If you are interested in learning more about pandas' expanding transformations, this section of the pandas documentation has additional information.\n",
    "\n",
    "INSTRUCTIONS\n",
    "\n",
    "Create mean_fractions by chaining the methods .expanding().mean() to fractions.\n",
    "Compute the percentage change in mean_fractions down each column by applying .pct_change() and multiplying by 100. Assign the result to fractions_change.\n",
    "Reset the index of fractions_change using the .reset_index() method. This will make 'Edition' an ordinary column.\n",
    "Print the first and last 5 rows of the DataFrame fractions_change. This has been done for you, so hit 'Submit Answer' to see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOC  Edition  AFG  AHO  ALG        ANZ  ARG  ARM        AUS        AUT  AZE  \\\n",
      "0       1896  NaN  NaN  NaN        NaN  NaN  NaN        NaN        NaN  NaN   \n",
      "1       1900  NaN  NaN  NaN        NaN  NaN  NaN -13.134766 -32.304688  NaN   \n",
      "2       1904  NaN  NaN  NaN        NaN  NaN  NaN   0.000000 -30.169386  NaN   \n",
      "3       1908  NaN  NaN  NaN        NaN  NaN  NaN   0.000000 -23.013510  NaN   \n",
      "4       1912  NaN  NaN  NaN -26.092774  NaN  NaN   0.000000   6.254438  NaN   \n",
      "\n",
      "NOC    ...      URS  URU         USA  UZB  VEN  VIE  YUG  ZAM  ZIM        ZZX  \n",
      "0      ...      NaN  NaN         NaN  NaN  NaN  NaN  NaN  NaN  NaN        NaN  \n",
      "1      ...      NaN  NaN   -9.448242  NaN  NaN  NaN  NaN  NaN  NaN  33.561198  \n",
      "2      ...      NaN  NaN  199.651245  NaN  NaN  NaN  NaN  NaN  NaN -22.642384  \n",
      "3      ...      NaN  NaN  -19.549222  NaN  NaN  NaN  NaN  NaN  NaN   0.000000  \n",
      "4      ...      NaN  NaN  -12.105733  NaN  NaN  NaN  NaN  NaN  NaN   0.000000  \n",
      "\n",
      "[5 rows x 139 columns]\n",
      "NOC  Edition  AFG  AHO        ALG  ANZ       ARG        ARM        AUS  \\\n",
      "21      1992  NaN  0.0  -7.214076  0.0 -6.767308        NaN   2.754114   \n",
      "22      1996  NaN  0.0   8.959211  0.0  1.306696        NaN  10.743275   \n",
      "23      2000  NaN  0.0  19.762488  0.0  0.515190 -26.935484  12.554986   \n",
      "24      2004  NaN  0.0   0.000000  0.0  9.625365   0.000000   8.161162   \n",
      "25      2008  NaN  0.0  -8.197807  0.0  8.588555  91.266408   6.086870   \n",
      "\n",
      "NOC       AUT        AZE ...   URS        URU       USA        UZB       VEN  \\\n",
      "21  -3.034840        NaN ...   0.0   0.000000 -1.329330        NaN  0.000000   \n",
      "22  -3.876773        NaN ...   0.0   0.000000 -1.010378        NaN  0.000000   \n",
      "23  -3.464221  88.387097 ...   0.0 -12.025323 -1.341842  42.258065  0.000000   \n",
      "24  -2.186922  48.982144 ...   0.0   0.000000 -1.031922  21.170339 -1.615969   \n",
      "25  -3.389836  31.764436 ...   0.0   0.000000 -0.450031  14.610625 -6.987342   \n",
      "\n",
      "NOC       VIE       YUG        ZAM        ZIM  ZZX  \n",
      "21        NaN  0.000000   0.000000   0.000000  0.0  \n",
      "22        NaN -2.667732 -10.758472   0.000000  0.0  \n",
      "23        NaN -2.696445   0.000000   0.000000  0.0  \n",
      "24   0.000000  0.000000   0.000000 -43.491929  0.0  \n",
      "25  -0.661117  0.000000   0.000000 -23.316533  0.0  \n",
      "\n",
      "[5 rows x 139 columns]\n"
     ]
    }
   ],
   "source": [
    "# Apply the expanding mean: mean_fractions\n",
    "mean_fractions = fractions.expanding().mean()\n",
    "\n",
    "# Compute the percentage change: fractions_change\n",
    "fractions_change = mean_fractions.pct_change()*100\n",
    "\n",
    "# Reset the index of fractions_change: fractions_change\n",
    "fractions_change = fractions_change.reset_index()\n",
    "\n",
    "# Print first & last 5 rows of fractions_change\n",
    "print(fractions_change.head())\n",
    "print(fractions_change.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building hosts DataFrame\n",
    "Your task here is to prepare a DataFrame hosts by left joining editions and ioc_codes.\n",
    "\n",
    "Once created, you will subset the Edition and NOC columns and set Edition as the Index.\n",
    "\n",
    "There are some missing NOC values; you will set those explicitly.\n",
    "\n",
    "Finally, you'll reset the Index & print the final DataFrame.\n",
    "\n",
    "INSTRUCTIONS\n",
    "\n",
    "Create the DataFrame hosts by doing a left join on DataFrames editions and ioc_codes (using pd.merge()).\n",
    "Clean up hosts by subsetting and setting the Index.\n",
    "Extract the columns 'Edition' and 'NOC'.\n",
    "Set 'Edition' column as the Index.\n",
    "Use the .loc[] accessor to find and assign the missing values to the 'NOC' column in hosts. This has been done for you.\n",
    "Reset the index of hosts using .reset_index(), which returns a new DataFrame.\n",
    "Hit 'Submit Answer' to see what hosts looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         NOC\n",
      "Edition     \n",
      "1972     NaN\n",
      "1980     NaN\n",
      "1988     NaN\n",
      "    Edition  NOC\n",
      "0      1896  GRE\n",
      "1      1900  FRA\n",
      "2      1904  USA\n",
      "3      1908  GBR\n",
      "4      1912  SWE\n",
      "5      1920  BEL\n",
      "6      1924  FRA\n",
      "7      1928  NED\n",
      "8      1932  USA\n",
      "9      1936  GER\n",
      "10     1948  GBR\n",
      "11     1952  FIN\n",
      "12     1956  AUS\n",
      "13     1960  ITA\n",
      "14     1964  JPN\n",
      "15     1968  MEX\n",
      "16     1972  FRG\n",
      "17     1976  CAN\n",
      "18     1980  URS\n",
      "19     1984  USA\n",
      "20     1988  KOR\n",
      "21     1992  ESP\n",
      "22     1996  USA\n",
      "23     2000  AUS\n",
      "24     2004  GRE\n",
      "25     2008  CHN\n"
     ]
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Load DataFrame from file_path: ioc_codes\n",
    "ioc_codes = pd.read_csv('Summer Olympic medalists 1896 to 2008 - IOC COUNTRY CODES.csv')\n",
    "\n",
    "# Extract the relevant columns: ioc_codes\n",
    "ioc_codes = ioc_codes[['Country', 'NOC']]\n",
    "\n",
    "# my code above \n",
    "\n",
    "# Left join editions and ioc_codes: hosts\n",
    "hosts = pd.merge(editions, ioc_codes, how='left')\n",
    "\n",
    "# Extract relevant columns and set index: hosts\n",
    "hosts = hosts[['Edition','NOC']].set_index('Edition')\n",
    "\n",
    "# Fix missing 'NOC' values of hosts\n",
    "print(hosts.loc[hosts.NOC.isnull()])\n",
    "hosts.loc[1972, 'NOC'] = 'FRG'\n",
    "hosts.loc[1980, 'NOC'] = 'URS'\n",
    "hosts.loc[1988, 'NOC'] = 'KOR'\n",
    "\n",
    "# Reset Index of hosts: hosts\n",
    "hosts = hosts.reset_index()\n",
    "\n",
    "# Print hosts\n",
    "print(hosts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping for analysis\n",
    "This exercise starts off with fractions_change and hosts already loaded.\n",
    "\n",
    "Your task here is to reshape the fractions_change DataFrame for later analysis.\n",
    "\n",
    "Initially, fractions_change is a wide DataFrame of 26 rows (one for each Olympic edition) and 139 columns (one for the edition and 138 for the competing countries).\n",
    "\n",
    "On reshaping with pd.melt(), as you will see, the result is a tall DataFrame with 3588 rows and 3 columns that summarizes the fractional change in the expanding mean of the percentage of medals won for each country in blocks.\n",
    "\n",
    "INSTRUCTIONS\n",
    "100 XP\n",
    "Create a DataFrame reshaped by reshaping the DataFrame fractions_change with pd.melt().\n",
    "You'll need to use the keyword argument id_vars='Edition' to set the identifier variable.\n",
    "You'll also need to use the keyword argument value_name='Change' to set the measured variables.\n",
    "Print the shape of the DataFrames reshaped and fractions_change. This has been done for you.\n",
    "Create a DataFrame chn by extracting all the rows from reshaped in which the three letter code for each country ('NOC') is 'CHN'.\n",
    "Print the last 5 rows of the DataFrame chn using the .tail() method. This has been done for you, so hit 'Submit Answer' to see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3588, 3) (26, 139)\n",
      "     Edition  NOC     Change\n",
      "567     1992  CHN   4.240630\n",
      "568     1996  CHN   7.860247\n",
      "569     2000  CHN  -3.851278\n",
      "570     2004  CHN   0.128863\n",
      "571     2008  CHN  13.251332\n"
     ]
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Reshape fractions_change: reshaped\n",
    "reshaped = pd.melt(fractions_change, id_vars='Edition', value_name='Change')\n",
    "\n",
    "# Print reshaped.shape and fractions_change.shape\n",
    "print(reshaped.shape, fractions_change.shape)\n",
    "\n",
    "# Extract rows from reshaped where 'NOC' == 'CHN': chn\n",
    "chn = reshaped.loc[reshaped.NOC == 'CHN']\n",
    "\n",
    "# Print last 5 rows of chn\n",
    "print(chn.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On looking at the hosting countries from the last 5 Olympic editions and the fractional change of medals won by China the last 5 editions, you can see that China fared significantly better in 2008 (i.e., when China was the host country)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging to compute influence\n",
    "This exercise starts off with the DataFrames reshaped and hosts in the namespace.\n",
    "\n",
    "Your task is to merge the two DataFrames and tidy the result.\n",
    "\n",
    "The end result is a DataFrame summarizing the fractional change in the expanding mean of the percentage of medals won for the host country in each Olympic edition.\n",
    "\n",
    "INSTRUCTIONS\n",
    "\n",
    "Merge reshaped and hosts using an inner join. Remember, how='inner' is the default behavior for pd.merge().\n",
    "Print the first 5 rows of the DataFrame merged. This has been done for you. You should see that the rows are jumbled chronologically.\n",
    "Set the index of merged to be 'Edition' and sort the index.\n",
    "Print the first 5 rows of the DataFrame influence. This has been done for you, so hit 'Submit Answer' to see the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Edition  NOC     Change\n",
      "0     1956  AUS  54.615063\n",
      "1     2000  AUS  12.554986\n",
      "2     1920  BEL  54.757887\n",
      "3     1976  CAN  -2.143977\n",
      "4     2008  CHN  13.251332\n",
      "         NOC      Change\n",
      "Edition                 \n",
      "1896     GRE         NaN\n",
      "1900     FRA  198.002486\n",
      "1904     USA  199.651245\n",
      "1908     GBR  134.489218\n",
      "1912     SWE   71.896226\n"
     ]
    }
   ],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Merge reshaped and hosts: merged\n",
    "merged = pd.merge(reshaped, hosts)\n",
    "\n",
    "# Print first 5 rows of merged\n",
    "print(merged.head())\n",
    "\n",
    "# Set Index of merged and sort it: influence\n",
    "influence = merged.set_index('Edition').sort_index()\n",
    "\n",
    "# Print first 5 rows of influence\n",
    "print(influence.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting influence of host country\n",
    "This final exercise starts off with the DataFrames influence and editions in the namespace. Your job is to plot the influence of being a host country.\n",
    "\n",
    "INSTRUCTIONS\n",
    "\n",
    "Create a Series called change by extracting the 'Change' column from influence.\n",
    "Create a bar plot of change using the .plot() method with kind='bar'. Save the result as ax to permit further customization.\n",
    "Customize the bar plot of change to improve readability:\n",
    "Apply the method .set_ylabel(\"% Change of Host Country Medal Count\") toax.\n",
    "Apply the method .set_title(\"Is there a Host Country Advantage?\") to ax.\n",
    "Apply the method .set_xticklabels(editions['City']) to ax.\n",
    "Reveal the final plot using plt.show()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x255c0017ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract influence['Change']: change\n",
    "change = influence['Change']\n",
    "\n",
    "# Make bar plot of change: ax\n",
    "ax = change.plot(kind='bar')\n",
    "\n",
    "# Customize the plot to improve readability\n",
    "ax.set_ylabel(\"% Change of Host Country Medal Count\")\n",
    "ax.set_title(\"Is there a Host Country Advantage?\")\n",
    "ax.set_xticklabels(editions['City'])\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
