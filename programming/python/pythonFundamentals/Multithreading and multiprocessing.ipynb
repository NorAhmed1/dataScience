{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I  Fundamentals \n",
    "https://www.quantstart.com/articles/Parallelising-Python-with-Threading-and-Multiprocessing   \n",
    "https://www.ploggingdev.com/2017/01/multiprocessing-and-multithreading-in-python-3/    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminology \n",
    "* Concurrency is when two or more tasks can start, run, and complete in overlapping time periods. It does not necessarily mean they will ever both be running at the same instant. E.g. multitasking on a single-core machine, in which it is impossible to run in the same instant. However, we can still have concurrency and multi-threading in this single-core case, and achieve performance gain for some special cases (network or IO bound tasks)\n",
    "\n",
    "* Parallelism is when two or more tasks are executed simultaneously.\n",
    "\n",
    "* A thread is a sequence of instructions **within a process**. It can be thought of as a lightweight process. Threads **share the same memory space**. Therefore, locking mechanisms such as critical sections (C++) are necessary to protect data from simultaneous accesses. \n",
    "\n",
    "* A process is an instance of a program running in a computer which can contain one or more threads. A process has its **independent memory space**. When different processes access same data, mutex is necessary to prevent from simultaneous modification of the data in C++. How about python? \n",
    "\n",
    "* Concurrent access to the same data by multiple threads or multiple processes is one of key difficulties in multi-threading / multi-processing calculations. Some problems, e.g. Monte Carlo simulations, usually do not need share states and thus particularly suitable for multi-tasking. The following examples belong to this category. For concurrent access of same data by multiple threads see C++ code and notes elsewhere. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why sometimes a python program runs slower with multiple threads?\n",
    "Because the **Python interpreter is not thread safe**, the internals of the main Python interpreter, CPython, negate the possibility of true multi-threading due to a process known as the Global Interpreter Lock (GIL). At any one time only a single thread can acquire a lock for a Python object or C API. The interpreter will reacquire this lock for every 100 bytecodes of Python instructions and around (potentially) blocking I/O operations. Because of this lock CPU-bound code will see no gain in performance when using the Threading library, but it will likely gain performance increases if the Multiprocessing library is used.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When is Python threading library useful?\n",
    "Now that Python on the CPython interpreter does not support true multi-core execution via multithreading, what is the usage of its threading library? It is useful in many non-CPU-bound problems such as network programming or data input/output (I/O). In GUI applications to keep the UI thread responsive. Threads should not be used for CPU bound tasks. Using threads for CPU bound tasks will actually result in worse performance compared to using a single thread. Below are some typical IO-bound problems:  \n",
    "\n",
    "* webscraping\n",
    "* reading and writing to files\n",
    "* sharing data between programs\n",
    "* network communications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-processing in python\n",
    "* In C/C++, either multi-threading or multi-processing can achieve performance gain from multi-core CPUs. In python, we can only gain from multi-processing. This works in a fundamentally different way to the Threading library, even though the syntax of the two is extremely similar. \n",
    "\n",
    "* The Multiprocessing library actually spawns multiple operating system processes for each parallel task. This nicely side-steps the GIL, by giving each process its own Python interpreter and thus own GIL. Hence each process can be fed to a separate processor core and then regrouped at the end once all processes have finished.\n",
    "\n",
    "* There are some drawbacks, however. Spawning extra processes introduces I/O overhead as data is having to be shuffled around between processors. This can add to the overall run-time. However, assuming the data is restricted to each process, it is possible to gain significant speedup. Of course, one must always be aware of Amdahl's Law, which indicates the gain must have a limit due to the portion of un-parallelized code. \n",
    "\n",
    "* Typical CPU-bound processes that are suitable for multi-processing\n",
    "    * computations\n",
    "    * text formatting\n",
    "    * image rescaling\n",
    "    * data analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running multi-threading and multiprocessing python code\n",
    "* Multi-threading python code can be run directly within Jupyter notebook.  \n",
    "\n",
    "* Multi-processing python code cannot be run directly within Jupyter notebook (at least for current version). It needs further steps to run multi-processing code in Jupyter (check web for details). A simple way out here is to run the code in VS code or command line. Or use Jupyter magic functions to simulating the command line running, as shown in the following examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "In numerical calculation, the vectorization can sometimes gain much more than either multi-threading or multi-processing. The gain can be a few hundreds times due to specific CPU structures. If we employ both vectorization and multi-tasking, then the over gain should be improved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples -- Part I "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multithreading Example: Webscraping\n",
    "\n",
    "Historically, the programming knowledge required to set up multithreading involved a good understanding of Python's Global Interpreter Lock (the GIL prevents multiple threads from running the same Python code at once). Also, you had to set up special classes that behave like Producers to divvy up the work, Consumers (aka \"workers\") to perform the work, and a Queue to hold tasks and provide communications. And that was just the beginning.\n",
    "\n",
    "Fortunately, with the `map()` function and two standard libraries, *multiprocessing* and *multiprocessing.dummy*, setting up parallel processes and threads can become fairly straightforward.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a classic multithreading example provided by [IBM](http://www.ibm.com/developerworks/aix/library/au-threadingpython/) and adapted by [Chris Kiehl](http://chriskiehl.com/article/parallelism-in-one-line/) where you divide the task of retrieving web pages across multiple threads:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import threading \n",
    "import Queue \n",
    "import urllib2 \n",
    "\n",
    "class Consumer(threading.Thread): \n",
    "    def __init__(self, queue): \n",
    "    threading.Thread.__init__(self)\n",
    "    self._queue = queue \n",
    "\n",
    "    def run(self):\n",
    "    while True: \n",
    "        content = self._queue.get() \n",
    "        if isinstance(content, str) and content == 'quit':\n",
    "        break\n",
    "        response = urllib2.urlopen(content)\n",
    "    print 'Thanks!'\n",
    "\n",
    "\n",
    "def Producer():\n",
    "    urls = [\n",
    "    'http://www.python.org', 'http://www.yahoo.com'\n",
    "    'http://www.scala.org', 'http://www.google.com'\n",
    "    # etc.. \n",
    "    ]\n",
    "  \n",
    "  queue = Queue.Queue()\n",
    "  \n",
    "  worker_threads = build_worker_pool(queue, 4)\n",
    "  start_time = time.time()\n",
    "\n",
    "  # Add the urls to process\n",
    "  for url in urls: \n",
    "    queue.put(url)  \n",
    "  # Add the poison pill\n",
    "  for worker in worker_threads:\n",
    "    queue.put('quit')\n",
    "  for worker in worker_threads:\n",
    "    worker.join()\n",
    "\n",
    "  print 'Done! Time taken: {}'.format(time.time() - start_time)\n",
    "\n",
    "def build_worker_pool(queue, size):\n",
    "  workers = []\n",
    "  for _ in range(size):\n",
    "    worker = Consumer(queue)\n",
    "    worker.start() \n",
    "    workers.append(worker)\n",
    "  return workers\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  Producer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the multithreading library provided by the *multiprocessing.dummy* module and `map()` all of this becomes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "#This is from multiprocessing.dummy, but it is a multithreading example, as shown in the title? \n",
    "\n",
    "pool = ThreadPool(4) # choose a number of workers\n",
    "\n",
    "urls = [\n",
    "'http://www.python.org', 'http://www.yahoo.com'\n",
    "'http://www.scala.org', 'http://www.google.com'\n",
    "# etc.. \n",
    "]\n",
    "\n",
    "results = pool.map(urllib2.urlopen, urls)\n",
    "pool.close() \n",
    "pool.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, the *multiprocessing.dummy* module provides the parallel threads, and `map(urllib2.urlopen, urls)` assigns the labor!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing Example: Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carle Method and Estimating Pi\n",
    "\n",
    "For a given number of points *n*, we have $$π = \\frac{4 \\cdot points\\ inside\\ circle}{total\\ points\\ n}$$\n",
    "\n",
    "To set up our multiprocessing program, we first derive a function for finding Pi that we can pass to `map()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random  # perform this import outside the function\n",
    "\n",
    "def find_pi(n):\n",
    "    \"\"\"\n",
    "    Function to estimate the value of Pi\n",
    "    \"\"\"\n",
    "    inside=0\n",
    "\n",
    "    for i in range(0,n):\n",
    "        x=random()\n",
    "        y=random()\n",
    "        if (x*x+y*y)**(0.5)<=1:  # if i falls inside the circle\n",
    "            inside+=1\n",
    "\n",
    "    pi=4*inside/n\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test `find_pi` on 5,000 points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1168"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_pi(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ran very quickly, but the results are not very accurate!\n",
    "\n",
    "Next we'll write a script that sets up a pool of workers, and lets us time the results against varying sized pools. We'll set up two arguments to represent *processes* and *total_iterations*. Inside the script, we'll break *total_iterations* down into the number of iterations passed to each process, by making a processes-sized list.<br>For example:\n",
    "\n",
    "    total_iterations = 1000\n",
    "    processes = 5\n",
    "    iterations = [total_iterations//processes]*processes\n",
    "    iterations\n",
    "    # Output: [200, 200, 200, 200, 200]\n",
    "    \n",
    "This list will be passed to our `map()` function along with `find_pi()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.py\n",
    "from random import random\n",
    "from multiprocessing import Pool\n",
    "import timeit\n",
    "\n",
    "def find_pi(n):\n",
    "    \"\"\"\n",
    "    Function to estimate the value of Pi\n",
    "    \"\"\"\n",
    "    inside=0\n",
    "\n",
    "    for i in range(0,n):\n",
    "        x=random()\n",
    "        y=random()\n",
    "        if (x*x+y*y)**(0.5)<=1:  # if i falls inside the circle\n",
    "            inside+=1\n",
    "\n",
    "    pi=4*inside/n\n",
    "    return pi\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    N = 10**5  # total iterations\n",
    "    P = 5      # number of processes\n",
    "    \n",
    "    p = Pool(P)\n",
    "    print(timeit.timeit(lambda: print(f'{sum(p.map(find_pi, [N//P]*P))/P :0.7f}'), number=10)) \n",
    "    # Take the average of the pi calculated and then print out. \n",
    "    \n",
    "    p.close()\n",
    "    p.join()\n",
    "    print(f'{N} total iterations with {P} processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1367600\n",
      "3.1442800\n",
      "3.1329200\n",
      "3.1370800\n",
      "3.1380800\n",
      "3.1417600\n",
      "3.1467200\n",
      "3.1326000\n",
      "3.1434000\n",
      "3.1481200\n",
      "0.2432337451411549\n",
      "100000 total iterations with 5 processes\n"
     ]
    }
   ],
   "source": [
    "! python test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know our script works, let's increase the number of iterations, and compare two different pools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.py\n",
    "from random import random\n",
    "from multiprocessing import Pool\n",
    "import timeit\n",
    "\n",
    "def find_pi(n):\n",
    "    \"\"\"\n",
    "    Function to estimate the value of Pi\n",
    "    \"\"\"\n",
    "    inside=0\n",
    "\n",
    "    for i in range(0,n):\n",
    "        x=random()\n",
    "        y=random()\n",
    "        if (x*x+y*y)**(0.5)<=1:  # if i falls inside the circle\n",
    "            inside+=1\n",
    "\n",
    "    pi=4*inside/n\n",
    "    return pi\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    N = 10**7  # total iterations\n",
    "    \n",
    "    P = 1      # number of processes\n",
    "    p = Pool(P)\n",
    "    print(timeit.timeit(lambda: print(f'{sum(p.map(find_pi, [N//P]*P))/P:0.7f}'), number=10))\n",
    "    p.close()\n",
    "    p.join()\n",
    "    print(f'{N} total iterations with {P} processes')\n",
    "    \n",
    "    P = 5      # number of processes\n",
    "    p = Pool(P)\n",
    "    print(timeit.timeit(lambda: print(f'{sum(p.map(find_pi, [N//P]*P))/P:0.7f}'), number=10))\n",
    "    p.close()\n",
    "    p.join()\n",
    "    print(f'{N} total iterations with {P} processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1415920\n",
      "3.1412692\n",
      "3.1409872\n",
      "3.1418424\n",
      "3.1413748\n",
      "3.1413968\n",
      "3.1419348\n",
      "3.1407936\n",
      "3.1410084\n",
      "3.1413128\n",
      "38.84810802688602\n",
      "10000000 total iterations with 1 processes\n",
      "3.1410436\n",
      "3.1414548\n",
      "3.1416328\n",
      "3.1414500\n",
      "3.1417748\n",
      "3.1412176\n",
      "3.1412932\n",
      "3.1414372\n",
      "3.1415492\n",
      "3.1418072\n",
      "12.2747860630176\n",
      "10000000 total iterations with 5 processes\n"
     ]
    }
   ],
   "source": [
    "! python test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More is Better ...to a point.\n",
    "\n",
    "The gain in speed as you add more parallel processes tends to flatten out at some point. In any collection of tasks, there are going to be one or two that take longer than average, and no amount of added processing can speed them up. This is best described in [Amdahl's Law](https://en.wikipedia.org/wiki/Amdahl%27s_law).\n",
    "\n",
    "**The speed-up depends on the number of CPU and the portion of code that cannot be parallelized.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Script\n",
    "\n",
    "In the example below, we'll add a context manager to shrink these three lines\n",
    "\n",
    "    p = Pool(P)\n",
    "    ...\n",
    "    p.close()\n",
    "    p.join()\n",
    "    \n",
    "to one line:\n",
    "\n",
    "    with Pool(P) as p:\n",
    "    \n",
    "And we'll accept command line arguments using the *sys* module.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test2.py\n",
    "from random import random\n",
    "from multiprocessing import Pool\n",
    "import timeit\n",
    "import sys\n",
    "\n",
    "N = int(sys.argv[1])  # these arguments are passed in from the command line\n",
    "P = int(sys.argv[2])\n",
    "\n",
    "def find_pi(n):\n",
    "    \"\"\"\n",
    "    Function to estimate the value of Pi\n",
    "    \"\"\"\n",
    "    inside=0\n",
    "\n",
    "    for i in range(0,n):\n",
    "        x=random()\n",
    "        y=random()\n",
    "        if (x*x+y*y)**(0.5)<=1:  # if i falls inside the circle\n",
    "            inside+=1\n",
    "\n",
    "    pi=4*inside/n\n",
    "    return pi\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    with Pool(P) as p:\n",
    "        print(timeit.timeit(lambda: print(f'{sum(p.map(find_pi, [N//P]*P))/P:0.5f}'), number=10))\n",
    "    print(f'{N} total iterations with {P} processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python test2.py 10000000 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples -- Part II  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The use of threads for filesystem IO\n",
    "A queue is used to store the files that need to be processed. A dictionary is used to store the input and output file names. The process_queue() function is used to retrieve items from the queue and perform the copy operation. The copy operation is done in the copy_op function using the shutil module.\n",
    "\n",
    "Note : The v1.mp4 and v2.mp4 were 250MB each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from queue import Queue\n",
    "import time\n",
    "import shutil\n",
    "\n",
    "print_lock = threading.Lock()\n",
    "\n",
    "def copy_op(file_data):\n",
    "    with print_lock:\n",
    "        print(\"Starting thread : {}\".format(threading.current_thread().name))\n",
    "\n",
    "    mydata = threading.local()\n",
    "    mydata.ip, mydata.op = next(iter(file_data.items()))\n",
    "\n",
    "    shutil.copy(mydata.ip, mydata.op)\n",
    "\n",
    "    with print_lock:\n",
    "        print(\"Finished thread : {}\".format(threading.current_thread().name))\n",
    "\n",
    "def process_queue():\n",
    "    while True:\n",
    "        file_data = compress_queue.get()\n",
    "        copy_op(file_data)\n",
    "        compress_queue.task_done()\n",
    "\n",
    "compress_queue = Queue()\n",
    "\n",
    "output_names = [{'v1.mp4' : 'v11.mp4'},{'v2.mp4' : 'v22.mp4'}]\n",
    "\n",
    "for i in range(2):\n",
    "    t = threading.Thread(target=process_queue)\n",
    "    t.daemon = True\n",
    "    t.start()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for file_data in output_names:\n",
    "    compress_queue.put(file_data)\n",
    "\n",
    "compress_queue.join()\n",
    "\n",
    "print(\"Execution time = {0:.5f}\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : The v1.mp4 and v2.mp4 were 250MB each.  \n",
    "7 to 10 seconds was the time taken when using one thread  \n",
    "4.5 to 5.5 seconds was the time taken when using two threads  \n",
    "So it’s clear that threads can be used for parallel filesystem IO.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The use of threads for network IO\n",
    "The following example demonstrates the use of threads for network IO using the requests library. This is a toy example use case of threads for networking IO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from queue import Queue\n",
    "import requests\n",
    "import bs4\n",
    "import time\n",
    "\n",
    "print_lock = threading.Lock()\n",
    "\n",
    "def get_url(current_url):\n",
    "\n",
    "    with print_lock:\n",
    "        print(\"\\nStarting thread {}\".format(threading.current_thread().name))\n",
    "    res = requests.get(current_url)\n",
    "    res.raise_for_status()\n",
    "\n",
    "    current_page = bs4.BeautifulSoup(res.text,\"html.parser\")\n",
    "    current_title = current_page.select('title')[0].getText()\n",
    "\n",
    "    with print_lock:\n",
    "        print(\"{}\\n\".format(threading.current_thread().name))\n",
    "        print(\"{}\\n\".format(current_url))\n",
    "        print(\"{}\\n\".format(current_title))\n",
    "        print(\"\\nFinished fetching : {}\".format(current_url))\n",
    "\n",
    "def process_queue():\n",
    "    while True:\n",
    "        current_url = url_queue.get()\n",
    "        get_url(current_url)\n",
    "        url_queue.task_done()\n",
    "\n",
    "url_queue = Queue()\n",
    "\n",
    "url_list = [\"https://www.google.com\"]*5\n",
    "\n",
    "for i in range(5):\n",
    "    t = threading.Thread(target=process_queue)\n",
    "    t.daemon = True\n",
    "    t.start()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for current_url in url_list:\n",
    "    url_queue.put(current_url)\n",
    "\n",
    "url_queue.join()\n",
    "\n",
    "print(threading.enumerate())\n",
    "\n",
    "print(\"Execution time = {0:.5f}\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single thread : 4 seconds  \n",
    "Two threads : 3 seconds  \n",
    "Five threads : 2 seconds  \n",
    "In network IO, most of the time is spent waiting for the response from the URL, so this is another use case where using threads improves performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The use of thread in CPU bound tasks\n",
    "In the following program a queue holds numbers. The task is to find the sum of prime number less than or equal to the given number. This is clearly a CPU bound task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from queue import Queue\n",
    "import time\n",
    "\n",
    "list_lock = threading.Lock()\n",
    "\n",
    "def find_rand(num):\n",
    "    sum_of_primes = 0\n",
    "\n",
    "    ix = 2\n",
    "\n",
    "    while ix <= num:\n",
    "        if is_prime(ix):\n",
    "            sum_of_primes += ix\n",
    "        ix += 1\n",
    "\n",
    "    sum_primes_list.append(sum_of_primes)\n",
    "\n",
    "def is_prime(num):\n",
    "    if num <= 1:\n",
    "        return False\n",
    "    elif num <= 3:\n",
    "        return True\n",
    "    elif num%2 == 0 or num%3 == 0:\n",
    "        return False\n",
    "    i = 5\n",
    "    while i*i <= num:\n",
    "        if num%i == 0 or num%(i+2) == 0:\n",
    "            return False\n",
    "        i += 6\n",
    "    return True\n",
    "\n",
    "def process_queue():\n",
    "    while True:\n",
    "        rand_num = min_nums.get()\n",
    "        find_rand(rand_num)\n",
    "        min_nums.task_done()\n",
    "\n",
    "min_nums = Queue()\n",
    "\n",
    "rand_list = [1000000, 2000000, 3000000]\n",
    "sum_primes_list = list()\n",
    "\n",
    "for i in range(2):\n",
    "    t = threading.Thread(target=process_queue)\n",
    "    t.daemon = True\n",
    "    t.start()\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for rand_num in rand_list:\n",
    "    min_nums.put(rand_num)\n",
    "\n",
    "min_nums.join()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "sum_primes_list.sort()\n",
    "print(sum_primes_list)\n",
    "\n",
    "print(\"Execution time = {0:.5f}\".format(end_time - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single thread : 25.5 seconds\n",
    "Two threads : 28 seconds\n",
    "The results are very clear : don't use threads to improve performance of CPU bound tasks. You will always end up with worse performance.  **Note these statements are only true for python. For other languages such as C/C++, threads can also improve performance for CPU bound tasks.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing for parallel execution of tasks\n",
    "In the following example we take the same task used above and process the inputs in parallel using the multiprocessing module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile test1.py\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "def sum_prime(num):\n",
    "    \n",
    "    sum_of_primes = 0\n",
    "\n",
    "    ix = 2\n",
    "\n",
    "    while ix <= num:\n",
    "        if is_prime(ix):\n",
    "            sum_of_primes += ix\n",
    "        ix += 1\n",
    "\n",
    "    return sum_of_primes\n",
    "\n",
    "def is_prime(num):\n",
    "    if num <= 1:\n",
    "        return False\n",
    "    elif num <= 3:\n",
    "        return True\n",
    "    elif num%2 == 0 or num%3 == 0:\n",
    "        return False\n",
    "    i = 5\n",
    "    while i*i <= num:\n",
    "        if num%i == 0 or num%(i+2) == 0:\n",
    "            return False\n",
    "        i += 6\n",
    "    return True\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    start = time.time()\n",
    "    with Pool(4) as p:\n",
    "        print(p.map(sum_prime, [1000000, 2000000, 3000000])) #original sentence\n",
    "    print(\"Time taken = {0:.5f}\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python test1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a single process : 20.29 seconds  \n",
    "Using four processes : 11.59 seconds  \n",
    "\n",
    "We see a huge improvement from using a single process to using four multiple processes (quad cores CPU). So using the multiprocessing module results in the full utilization of the CPU. However, using more than four processes will not improve performance anymore. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter-process communication  \n",
    "Inter process communication can be achieved using queues or pipes.  \n",
    "\n",
    "* The Queue in the multiprocessing module works similar to the queue module used to demonstrate how the threading module works earlier. \n",
    "\n",
    "* Another useful communication mechanism between processes is a pipe. A pipe is a duplex (two way) communication channel. Note : Reading or writing to the same end of the pipe simultaneously can result in data corruption. The following is a basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test2.py\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "\n",
    "def info(conn):\n",
    "    conn.send(\"Hello from {}\\nppid = {}\\npid={}\".format(mp.current_process().name, os.getppid(), os.getpid()))\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parent_conn, child_conn = mp.Pipe()\n",
    "    p = mp.Process(target=info, args=(child_conn,))\n",
    "    p.daemon = True\n",
    "    p.start()\n",
    "    print(parent_conn.recv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello from Process-1\n",
      "ppid = 11044\n",
      "pid=15072\n"
     ]
    }
   ],
   "source": [
    "!python test2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
